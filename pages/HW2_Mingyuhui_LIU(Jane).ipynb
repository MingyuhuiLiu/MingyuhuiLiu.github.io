{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import itertools\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 938.23786125]\n",
      "Mean squared error: 2548.07\n",
      "Variance score: 0.47\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfwZ6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e5315593c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Question 1): a). Execute the original example. \n",
    "\n",
    "Step 1:\n",
    "The original examole only used one feature\n",
    "\n",
    "\"\"\"  \n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2] # Taking the 3rd column (feature) and reshape it into 442 by 1, using np.newaxis. \n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:] # Setting test data as the last 20 instances. \n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features' data: \n",
      "(442, 10)\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "20\n",
      "Coefficients: \n",
      " [  3.03499549e-01  -2.37639315e+02   5.10530605e+02   3.27736980e+02\n",
      "  -8.14131709e+02   4.92814588e+02   1.02848452e+02   1.84606489e+02\n",
      "   7.43519617e+02   7.60951722e+01]\n",
      "Mean squared error: 2004.57\n",
      "\n",
      "Sum of squared error, aka residual sum of squares.: 40091.35\n",
      "\n",
      "Variance score: 0.59\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 1): b). Perform regression for all the features.  \n",
    "\n",
    "Step 1: Inspect the data to determine how many features. \n",
    "\n",
    "\"\"\"\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "print(\"Shape of features' data: \\n{}\".format(diabetes.data.shape))\n",
    "\n",
    "# Write the data into panda dataframe for easier furthur process.\n",
    "df = pd.DataFrame(diabetes.data)\n",
    "\n",
    "# Get the columns names for further manipulation. \n",
    "col = list(df.columns.values)\n",
    "print(col)\n",
    "print()\n",
    "\n",
    "# Independent variable dataframe X.\n",
    "X = df.as_matrix(columns = col)\n",
    "\n",
    "# Target.\n",
    "y_train = diabetes.target[:-20]\n",
    "y_test = diabetes.target[-20:]\n",
    "\n",
    "length = len(y_test)\n",
    "print(length)\n",
    "# Split the data into training/testing sets\n",
    "X_train = X[:-20][:]\n",
    "X_test = X[-20:][:]\n",
    "\"\"\"#print(X_train)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\"\"\"\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "print()\n",
    "\n",
    "# Sum of squared error, aka residual sum of squares. \n",
    "print(\"Sum of squared error, aka residual sum of squares.: %.2f\"\n",
    "      % (mean_squared_error(y_test, y_pred)*length))\n",
    "print()\n",
    "\n",
    "# Explained variance score: a result of 1 denotes a perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction power in terms of variance coverage increased by 23.81% based on R-squared comparison.\n",
      "Prediction power in terms of accuracy increased by 0.21% based on MSE comparison.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 1): Prediction power comparison.  \n",
    "\n",
    "Step 2: Compare the prediction power with more features.\n",
    "\n",
    "According to the variance score, when the model is fed with more features, the variance coverage increases by 23.81%. \n",
    "And according to the MSE, the prediction accuracy increases by 0.21%.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# R-squared comparison.\n",
    "addition = np.absolute((r2_score(diabetes_y_test, diabetes_y_pred))-(r2_score(y_test, y_pred)))\n",
    "diff = addition/r2_score(diabetes_y_test, diabetes_y_pred)*100\n",
    "print(\"Prediction power in terms of variance coverage increased by {}% based on R-squared comparison.\".format(np.round(diff,2)))\n",
    "\n",
    "# MSE comparison. \n",
    "decrease = np.absolute((mean_squared_error(y_test, y_pred))-(mean_squared_error(diabetes_y_test, diabetes_y_pred)))\n",
    "diff_mse = decrease/(mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "print(\"Prediction power in terms of accuracy increased by {}% based on MSE comparison.\".format(np.round(diff_mse,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Question 1): c). 10-folds Cross validation. METHOD ONE: Manually do the 10-folds cross validation.\n",
    "\n",
    "Step 3: \n",
    " i. Since I have 442 samples, in order to divide the dataset into 10 equal-size folds, I need to randomly drop 2 instance. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# Set the number of samples to be removed.\n",
    "remove_n = 2\n",
    "\n",
    "# Randomly choose remove_n number of index from dataframe.\n",
    "drop = np.random.choice(df.index, remove_n, replace=False)\n",
    "\n",
    "# Drop rows that are randomly chosen based on indexes, for fearure data.\n",
    "df = pd.DataFrame(diabetes.data)\n",
    "df_new = df.drop(drop)\n",
    "\n",
    "# Drop the same rows in target data.\n",
    "# Write target into panda dataframe for easier furthur process.\n",
    "df_target = pd.DataFrame(diabetes.target)\n",
    "df_target_new = df_target.drop(drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Question 1): c). 10-folds Cross validation. METHOD ONE: Manually do the 10-folds cross validation.\n",
    "\n",
    "Step 3: \n",
    " 2. Split the index into 10 folds, so that I can devide the data with indexes.  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split df_new with np.vsplit, thus I need to convert the dataframe into np array first.\n",
    "n = len(df_target_new)\n",
    "array = np.arange(n).reshape((n, 1))\n",
    "#print(array)\n",
    "\n",
    "split = np.vsplit(array, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************Fold 0 as the test dataset*******************\n",
      "Coefficients: \n",
      " [[  -7.57217804 -227.31830298  530.81213222  322.25242088 -628.38191361\n",
      "   395.25489411   -9.09589389  125.15221739  642.3265809    81.82766809]]\n",
      "Mean squared error: 2564.58\n",
      "Variance score: 0.54\n",
      "\n",
      "****************Fold 1 as the test dataset*******************\n",
      "Coefficients: \n",
      " [[ -21.76523725 -219.19528148  531.88792813  328.95641169 -908.92227095\n",
      "   584.05987594  140.85798014  178.26157321  787.77249995   80.9882284 ]]\n",
      "Mean squared error: 2917.47\n",
      "Variance score: 0.28\n",
      "\n",
      "****************Fold 2 as the test dataset*******************\n",
      "Coefficients: \n",
      " [[  42.35644654 -225.81344762  532.86776186  331.54605834 -984.55276633\n",
      "   595.56711433  238.44463302  277.75499874  817.56581657   31.2531031 ]]\n",
      "Mean squared error: 3460.48\n",
      "Variance score: 0.38\n",
      "\n",
      "****************Fold 3 as the test dataset*******************\n",
      "Coefficients: \n",
      " [[ -24.92275568 -246.62450583  508.90645648  330.80881808 -494.52086592\n",
      "   274.17581092  -62.31970798   81.75316201  664.51253808   37.64230214]]\n",
      "Mean squared error: 2763.70\n",
      "Variance score: 0.63\n",
      "\n",
      "****************Fold 4 as the test dataset*******************\n",
      "Coefficients: \n",
      " [[ -28.2394058  -243.7391521   514.10967017  347.92312721 -823.7812307\n",
      "   517.18250955   92.15333148  151.73397661  791.89471985   50.87643708]]\n",
      "Mean squared error: 3545.95\n",
      "Variance score: 0.28\n",
      "\n",
      "****************Fold 5 as the test dataset*******************\n",
      "Coefficients: \n",
      " [[  24.44011991 -274.70318684  488.60744944  342.35855006 -887.47811469\n",
      "   561.67767303  148.66171446  214.46333059  766.12557371   45.83277063]]\n",
      "Mean squared error: 2893.84\n",
      "Variance score: 0.62\n",
      "\n",
      "****************Fold 6 as the test dataset*******************\n",
      "Coefficients: \n",
      " [[ -62.83876909 -208.53116727  533.66117891  300.43094671 -909.75083706\n",
      "   511.34120263  172.71265651  211.77455755  790.59347197  101.36654606]]\n",
      "Mean squared error: 3693.38\n",
      "Variance score: 0.42\n",
      "\n",
      "****************Fold 7 as the test dataset*******************\n",
      "Coefficients: \n",
      " [[ -11.79987696 -253.56868765  543.50102976  314.62570512 -656.10104804\n",
      "   349.58791284   75.17504038  193.53386781  724.3609954    78.69686663]]\n",
      "Mean squared error: 2280.62\n",
      "Variance score: 0.44\n",
      "\n",
      "****************Fold 8 as the test dataset*******************\n",
      "Coefficients: \n",
      " [[ -14.98297623 -244.93722469  524.50887152  321.10258177 -659.00942301\n",
      "   377.38559677   31.42398166  131.63023819  733.31636254   52.06033594]]\n",
      "Mean squared error: 4123.08\n",
      "Variance score: 0.43\n",
      "\n",
      "****************Fold 9 as the test dataset*******************\n",
      "Coefficients: \n",
      " [[ -9.24135552e-02  -2.36204646e+02   5.24667537e+02   3.03079889e+02\n",
      "   -7.36376686e+02   4.19168497e+02   9.09929586e+01   2.12053869e+02\n",
      "    6.81947219e+02   9.52785524e+01]]\n",
      "Mean squared error: 1769.94\n",
      "Variance score: 0.69\n",
      "\n",
      "[0.53661899341967234, 0.27871274057858597, 0.38411431213690539, 0.62532914666318318, 0.27859569415438179, 0.61905447968034466, 0.41861628226269088, 0.43556240057869822, 0.43435036266074334, 0.68563952099471603]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 1): c). 10-folds Cross validation. METHOD ONE: Manually do the 10-folds cross validation.\n",
    "\n",
    "Step 3: \n",
    " 3. Perform the Cross-Validation. \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "Original_X = df_new.as_matrix()\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "R2_Manually = []\n",
    "for i in range(10):\n",
    "    print(\"****************Fold {} as the test dataset*******************\".format(i))\n",
    "\n",
    "    X_train = np.delete(Original_X, (split[i].tolist()), axis=0)\n",
    "    X_step_1 = Original_X[split[i].tolist(),]\n",
    "    \n",
    "    X_test = np.squeeze(X_step_1)\n",
    "\n",
    "    y_train = np.delete(np.array(df_target_new), (split[i].tolist()), axis=0)\n",
    "    y_step_1 = np.array(df_target_new)[split[i].tolist(),]\n",
    "    \n",
    "    y_test = np.squeeze(y_step_1)\n",
    "    \n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Explained variance score: a result of 1 denotes a perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "    R2_Manually.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "print(R2_Manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted into 10 folds\n",
      "Coefficients: \n",
      " [[  -9.35207221 -231.25935869  522.28500451  322.9065529  -633.34781888\n",
      "   401.62150256   -8.76480774  115.25791809  650.39226132   88.66074286]]\n",
      "Mean squared error: 2533.85\n",
      "\n",
      "Variance score: 0.56\n",
      "Coefficients: \n",
      " [[ -21.11846535 -215.88640593  529.35821161  323.83044314 -948.59584272\n",
      "   607.30739517  162.55714013  189.10624282  803.30840477   86.70881806]]\n",
      "Mean squared error: 2870.77\n",
      "\n",
      "Variance score: 0.23\n",
      "Coefficients: \n",
      " [[  44.59538444 -230.20810407  533.18081477  335.16932957 -989.99143477\n",
      "   602.00982274  239.60713172  279.80132481  820.70907004   25.69869348]]\n",
      "Mean squared error: 3512.72\n",
      "\n",
      "Variance score: 0.35\n",
      "Coefficients: \n",
      " [[ -24.3926342  -247.60708344  503.88724153  330.70556115 -509.97724606\n",
      "   283.36701109  -55.56622118   83.0034667   672.09717696   40.25679542]]\n",
      "Mean squared error: 2759.23\n",
      "\n",
      "Variance score: 0.62\n",
      "Coefficients: \n",
      " [[ -27.31113259 -246.61849223  509.69607756  348.08394708 -843.83610246\n",
      "   532.00664814   99.61898966  150.54628179  803.54748166   53.83014503]]\n",
      "Mean squared error: 3555.68\n",
      "\n",
      "Variance score: 0.27\n",
      "Coefficients: \n",
      " [[  25.5869905  -277.01566155  483.91885153  342.5453913  -906.86023823\n",
      "   575.28345173  157.03594326  215.42442696  776.40621418   48.251528  ]]\n",
      "Mean squared error: 2900.38\n",
      "\n",
      "Variance score: 0.62\n",
      "Coefficients: \n",
      " [[ -61.85182138 -210.62357144  529.08198762  300.24467676 -930.54717832\n",
      "   525.54325927  182.06121384  213.51694568  801.46329912  103.88214648]]\n",
      "Mean squared error: 3696.28\n",
      "\n",
      "Variance score: 0.42\n",
      "Coefficients: \n",
      " [[ -10.75870331 -255.39045814  538.9663028   314.49572067 -675.45533634\n",
      "   362.53106544   84.26245554  196.0790707   734.03449916   81.02506452]]\n",
      "Mean squared error: 2282.28\n",
      "\n",
      "Variance score: 0.44\n",
      "Coefficients: \n",
      " [[ -13.93804147 -247.24238179  519.48225703  321.1957307  -679.29267869\n",
      "   391.31067881   39.97784908  133.14462254  743.6753583    54.75924354]]\n",
      "Mean squared error: 4122.94\n",
      "\n",
      "Variance score: 0.43\n",
      "Coefficients: \n",
      " [[   0.92891559 -238.50198325  519.89110807  303.04397319 -756.32531098\n",
      "   433.01009122   99.81896609  213.40218236  692.49862595   98.00129273]]\n",
      "Mean squared error: 1769.68\n",
      "\n",
      "Variance score: 0.69\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 1): c). 10 folds Cross-validation. Method 2: KFold function in SKlearn.\n",
    "\n",
    "Step 3: \n",
    "\n",
    "Reference: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Read in features and target as dataframe.\n",
    "df = pd.DataFrame(diabetes.data)\n",
    "df_target = pd.DataFrame(diabetes.target)\n",
    "\n",
    "# Inditiate KFold.\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "# Split data into 10 folds.\n",
    "s = kf.split(df)\n",
    "n = kf.get_n_splits(df)\n",
    "print(\"Splitted into {} folds\".format(n))\n",
    "\n",
    "# Initiate regression.\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "R2_KFold = []\n",
    "# Cross-validation.\n",
    "for train_index, test_index in kf.split(df, df_target):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = df.iloc[train_index], df.iloc[test_index]\n",
    "    y_train, y_test = df_target.iloc[train_index], df_target.iloc[test_index]\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train, y_train)\n",
    "    # Predict.\n",
    "    y_pred = regr.predict(X_test)\n",
    "    # The coefficients\n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\" \n",
    "          % mean_squared_error(y_test, y_pred))\n",
    "    print()\n",
    "    # Explained variance score: a result of 1 denotes a perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "    #Create a list of variance scores for each fold to answer question d.\n",
    "    R2_KFold.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "#print()\n",
    "#print(R2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of variance scores for the 10-folds : 0.46965939331299217\n",
      "Range of variance scores for the 10-folds : [0.2785956941543818,0.685639520994716]\n",
      "Standard Deviation of variance scores for the 10-folds : 0.1352706995337737\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 1): d). 10 folds Cross-validation variance scores.  \n",
    "\n",
    "Step 4: \n",
    "i. Calculate the results from the maually method.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "R2 = R2_Manually\n",
    "\n",
    "# Properties for 10-folds cross validation variance scores. \n",
    "average = np.mean(R2)\n",
    "maximum = np.max(R2)\n",
    "minimum = np.min(R2)\n",
    "std = np.std(R2)\n",
    "print(\"Average of variance scores for the 10-folds : {}\".format(average))\n",
    "print(\"Range of variance scores for the 10-folds : [{},{}]\".format(minimum, maximum))\n",
    "print(\"Standard Deviation of variance scores for the 10-folds : {}\".format(std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of variance scores for the 10-folds : 0.4619623619583371\n",
      "Range of variance scores for the 10-folds : [0.23056091914863153,0.6856851417427137]\n",
      "Standard Deviation of variance scores for the 10-folds : 0.14698789185375874\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 1): d). 10 folds Cross-validation variance scores.  \n",
    "\n",
    "Step 4: \n",
    "ii. Calculate the results from the KFold method.\n",
    "\n",
    "Noted that the Kfold and Mannually methods are different, this is the main reasons:\n",
    "- I took out the 3 extra data points randomly from the original dataset, while the KFold is handling the problem automatically. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "R2 = R2_KFold\n",
    "\n",
    "# Properties for 10-folds cross validation variance scores. \n",
    "average = np.mean(R2)\n",
    "maximum = np.max(R2)\n",
    "minimum = np.min(R2)\n",
    "std = np.std(R2)\n",
    "print(\"Average of variance scores for the 10-folds : {}\".format(average))\n",
    "print(\"Range of variance scores for the 10-folds : [{},{}]\".format(minimum, maximum))\n",
    "print(\"Standard Deviation of variance scores for the 10-folds : {}\".format(std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[ -10.01219782 -239.81908937  519.83978679  324.39042769 -792.18416163\n",
      "   476.74583782  101.04457032  177.06417623  751.27932109   67.62538639]]\n",
      "Mean squared error: 2859.69\n",
      "\n",
      "Variance score: 0.52\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 1): d). Fit a regression model to all the data.  \n",
    "\n",
    "Step 5: Fit a regression model to all the data. \n",
    "\n",
    "Noted that even this is a statistical method other than a Machine Learning method, I still fitted the data with LinearRegression().\n",
    "But instead of divide the whole dataset into test and train, I fitted the model with the whole dataset, \n",
    "and use all the data to predict, and calculate the MSE and R2 based on the dredicted data and origial whole target data. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Fit all the data for a regression.\n",
    "# Initiate regression.\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "regr.fit(df, df_target)\n",
    "\n",
    "X = df\n",
    "y = df_target\n",
    "\n",
    "y_pred = regr.predict(X)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y, y_pred))\n",
    "print()\n",
    "# Explained variance score: a result of 1 denotes a perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y, y_pred))\n",
    "#Create a list of variance scores for each fold to answer question d.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy0AAAFLCAYAAADbImNoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noQUAAACA1QgtAAAAAKxGaAEAAABgtf8H31R5yGL0WSkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e531739be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Question 2):\n",
    "\n",
    "Step 1: Execute the sample codes and comment the functions for each line of code.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the true function, which is a cosine function. \n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "# Use random.seed to make sure everytime we randomly chose the same set of data in order to better compare the results. \n",
    "np.random.seed(0)\n",
    "\n",
    "# Set up number of samples and degrees.\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15] # In polynomial function, if degree = 1, then it is linear.\n",
    "\n",
    "# Genertae 30 random samples, and order them by using np.sort. \n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "# Generate y, with true_fun(), which is the true function, PLUS noise, using np.random.randn().\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    # Set the figure position: this is a 1 by 3 figure. \n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=()) # Set up the figure property: no x or y ticks. \n",
    "    \n",
    "    # Iterate through different degrees: 1, 4, 15, and generate polynomial features. \n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    # Initiate LinearRegression().\n",
    "    linear_regression = LinearRegression()\n",
    "    # Use preprocessing.pipeline to transform and fit data in a more comparable way. \n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    # Fit the the model with the order in the pipeline: first generate polynomial features and then perform linear regression\n",
    "    pipeline.fit(X[:, np.newaxis], y) # And use np.newaxis to reshape the X. \n",
    "    \n",
    "    # Evaluate the models using crossvalidation: model_selection.cross_val_score()\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100) # Set the test data set within range [0,1], with even steps and a total step of 100. \n",
    "    \n",
    "    # Plot predicted regression line:\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    # Plot true line:\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    # Plot the data with noises:\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    # Set lables and axis limits:\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    # Set legend\n",
    "    plt.legend(loc=\"best\")\n",
    "    # Set title\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))\n",
    "\n",
    "plt.show() # Print out the figures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC for all coefficients with degrees of 0 in PolynomialFeature is:\n",
      "-41.59329815466777 \n",
      "BIC for all coefficients with degrees of 0 in PolynomialFeature is:\n",
      "-38.79090339134346 \n",
      "*******************************************\n",
      "\n",
      "AIC for all coefficients with degrees of 1 in PolynomialFeature is:\n",
      "-190.3629780276827 \n",
      "BIC for all coefficients with degrees of 1 in PolynomialFeature is:\n",
      "-187.56058326435837 \n",
      "*******************************************\n",
      "\n",
      "AIC for all coefficients with degrees of 2 in PolynomialFeature is:\n",
      "-142.86740758813883 \n",
      "BIC for all coefficients with degrees of 2 in PolynomialFeature is:\n",
      "-140.06501282481452 \n",
      "*******************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 2):\n",
    "\n",
    "Step 2: Calculate AIC and BIC.\n",
    "\n",
    "Since these are statistical measurment, I fitted the model with all data to calculate the AIC and BIC. \n",
    "\n",
    "\"\"\"\n",
    "np.random.seed(0)\n",
    "\n",
    "# K should be the number of features plus one. \n",
    "k = 1+1\n",
    "n = n_samples\n",
    "\n",
    "# Set the test data set within range [0,1], with even steps and a total step of 100. \n",
    "\n",
    "# Genertae 30 random samples, and order them by using np.sort. \n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "# Generate y, with true_fun(), which is the true function, PLUS noise, using np.random.randn().\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "for i in range(len(degrees)):\n",
    "    # Iterate through different degrees: 1, 4, 15, and generate polynomial features. \n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    # Initiate LinearRegression().\n",
    "    linear_regression = LinearRegression()\n",
    "    # Use preprocessing.pipeline to transform and fit data in a more comparable way. \n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    # Fit the the model with the order in the pipeline: first generate polynomial features and then perform linear regression\n",
    "    pipeline.fit(X[:, np.newaxis], y) # And use np.newaxis to reshape the X. \n",
    "    \n",
    "    # Predict with all data.     \n",
    "    y_pred = pipeline.predict(X[:, np.newaxis])\n",
    "    mean_sq_er_total = mean_squared_error(true_fun(X), y_pred)\n",
    "\n",
    "    # AIC and BIC for All coefficients\n",
    "    aic_total = (2*k) + n * np.log(mean_sq_er_total)\n",
    "    print(\"AIC for all coefficients with degrees of {} in PolynomialFeature is:\\n{} \".format(i, aic_total))\n",
    "    bic_total = k * np.log(n) + n * np.log(mean_sq_er_total)\n",
    "    print(\"BIC for all coefficients with degrees of {} in PolynomialFeature is:\\n{} \".format(i, bic_total))\n",
    "    print(\"*******************************************\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "************************************************QUESTION 3: MPG AUTO************************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>accerleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>429.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>4341.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>454.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>4354.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4312.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement horsepower  weight  accerleration  \\\n",
       "0  18.0          8         307.0      130.0  3504.0           12.0   \n",
       "1  15.0          8         350.0      165.0  3693.0           11.5   \n",
       "2  18.0          8         318.0      150.0  3436.0           11.0   \n",
       "3  16.0          8         304.0      150.0  3433.0           12.0   \n",
       "4  17.0          8         302.0      140.0  3449.0           10.5   \n",
       "5  15.0          8         429.0      198.0  4341.0           10.0   \n",
       "6  14.0          8         454.0      220.0  4354.0            9.0   \n",
       "7  14.0          8         440.0      215.0  4312.0            8.5   \n",
       "\n",
       "   model year  origin  \n",
       "0          70       1  \n",
       "1          70       1  \n",
       "2          70       1  \n",
       "3          70       1  \n",
       "4          70       1  \n",
       "5          70       1  \n",
       "6          70       1  \n",
       "7          70       1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Question 3: a. Clean the data, removing samples with empty entries and scaling each feature to have zero mean and unit variance\n",
    "\n",
    "Step 1: read in data file.\n",
    "\n",
    "Columns should be named as the following: \n",
    "\n",
    "1. mpg: continuous \n",
    "2. cylinders: multi-valued discrete \n",
    "3. displacement: continuous \n",
    "4. horsepower: continuous \n",
    "5. weight: continuous \n",
    "6. acceleration: continuous \n",
    "7. model year: multi-valued discrete \n",
    "8. origin: multi-valued discrete \n",
    "9. car name: string (unique for each instance)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "read=pd.read_csv('auto_mpg.data', delim_whitespace = True,\n",
    "               names = [\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"accerleration\", \"model year\", \"origin\", \"car name\"]) \n",
    "                 \n",
    "data = read.drop(['car name'], axis=1, inplace = False)\n",
    "print(data.shape)\n",
    "data.head(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg              False\n",
      "cylinders        False\n",
      "displacement     False\n",
      "horsepower       False\n",
      "weight           False\n",
      "accerleration    False\n",
      "model year       False\n",
      "origin           False\n",
      "dtype: bool\n",
      "\n",
      "Columns names: \n",
      "['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'accerleration', 'model year', 'origin']\n",
      "\n",
      "Type for column mpg: <class 'numpy.float64'>\n",
      "Type for column cylinders: <class 'numpy.int64'>\n",
      "Type for column displacement: <class 'numpy.float64'>\n",
      "Type for column horsepower: <class 'str'>\n",
      "Type for column weight: <class 'numpy.float64'>\n",
      "Type for column accerleration: <class 'numpy.float64'>\n",
      "Type for column model year: <class 'numpy.int64'>\n",
      "Type for column origin: <class 'str'>\n",
      "\n",
      "Index that containing special characters:\n",
      "[32, 126, 330, 336, 354, 374]\n",
      "\n",
      "(398, 8)\n",
      "(392, 8)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 3: a. Clean the data, removing samples with empty entries and scaling each feature to have zero mean and unit variance\n",
    "\n",
    "Step 2: Clean the data\n",
    "\n",
    "1). Noted that in the original dataset, categorical column \"origin\" is stored as numerical, I need to convert them into string. \n",
    "2). Check datatype.\n",
    "3). Drop NANs and other data that does not make sense. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Convert 'origin' column into string:\n",
    "data['origin'] = data['origin'].astype(str)\n",
    "\n",
    "# Clean data: check first if there is any NANs.\n",
    "print(data.isnull().any()) # No NAN. If there is any, comment out the following. \n",
    "#df = data.dropna(how='all')\n",
    "print()\n",
    "\n",
    "col = list(data.columns.values)\n",
    "print(\"Columns names: \\n{}\".format(col))\n",
    "print()\n",
    "\n",
    "# Check each column's type: noted that horsepower is str. \n",
    "for column in col:\n",
    "    print(\"Type for column {}: {}\".format(column, type(data[column][0])))\n",
    "    \n",
    "print()\n",
    "    \n",
    "# Check the string: I found '?' in there, which cannot be converted to numeirc. \n",
    "index_list = data['horsepower'].index[data['horsepower'] == '?'].tolist()\n",
    "print(\"Index that containing special characters:\\n{}\".format(index_list))\n",
    "print()\n",
    "print(data.shape)\n",
    "# Drop the rows with index_list generated above. \n",
    "df=data.drop(np.array(index_list))\n",
    "\n",
    "# Need to reset the index, otherwise the index are gone after the drop. \n",
    "df = df.reset_index(drop=True)\n",
    "print(df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>accerleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.697747</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.075915</td>\n",
       "      <td>130</td>\n",
       "      <td>0.619748</td>\n",
       "      <td>-1.283618</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.082115</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.486832</td>\n",
       "      <td>165</td>\n",
       "      <td>0.842258</td>\n",
       "      <td>-1.464852</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.697747</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.181033</td>\n",
       "      <td>150</td>\n",
       "      <td>0.539692</td>\n",
       "      <td>-1.646086</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.953992</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.047246</td>\n",
       "      <td>150</td>\n",
       "      <td>0.536160</td>\n",
       "      <td>-1.283618</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.825870</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.028134</td>\n",
       "      <td>140</td>\n",
       "      <td>0.554997</td>\n",
       "      <td>-1.827320</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.082115</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>2.241772</td>\n",
       "      <td>198</td>\n",
       "      <td>1.605147</td>\n",
       "      <td>-2.008554</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.210238</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>2.480677</td>\n",
       "      <td>220</td>\n",
       "      <td>1.620452</td>\n",
       "      <td>-2.371022</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.210238</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>2.346890</td>\n",
       "      <td>215</td>\n",
       "      <td>1.571005</td>\n",
       "      <td>-2.552256</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mpg  cylinders  displacement horsepower    weight  accerleration  \\\n",
       "0 -0.697747   1.482053      1.075915        130  0.619748      -1.283618   \n",
       "1 -1.082115   1.482053      1.486832        165  0.842258      -1.464852   \n",
       "2 -0.697747   1.482053      1.181033        150  0.539692      -1.646086   \n",
       "3 -0.953992   1.482053      1.047246        150  0.536160      -1.283618   \n",
       "4 -0.825870   1.482053      1.028134        140  0.554997      -1.827320   \n",
       "5 -1.082115   1.482053      2.241772        198  1.605147      -2.008554   \n",
       "6 -1.210238   1.482053      2.480677        220  1.620452      -2.371022   \n",
       "7 -1.210238   1.482053      2.346890        215  1.571005      -2.552256   \n",
       "\n",
       "   model year origin  \n",
       "0   -1.623241      1  \n",
       "1   -1.623241      1  \n",
       "2   -1.623241      1  \n",
       "3   -1.623241      1  \n",
       "4   -1.623241      1  \n",
       "5   -1.623241      1  \n",
       "6   -1.623241      1  \n",
       "7   -1.623241      1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Question 3: a. Clean the data, removing samples with empty entries and scaling each feature to have zero mean and unit variance\n",
    "\n",
    "Step 3:\n",
    "\n",
    "1. Rewrite columns into their normalized form;\n",
    "2. Convert string column to numeric column.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for column in col:\n",
    "    if column != 'horsepower' and column != 'origin':\n",
    "        mean = df[column].mean()\n",
    "        std = df[column].std()\n",
    "        df[column] = (df[column]-mean)/std\n",
    "    if column == 'horsepower':\n",
    "        for ele in range(0,len(df['horsepower'])):\n",
    "            df[column][ele] = (float(df[column][ele]))\n",
    "\n",
    "\n",
    "df.head(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>accerleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.697747</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.075915</td>\n",
       "      <td>0.663285</td>\n",
       "      <td>0.619748</td>\n",
       "      <td>-1.283618</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.082115</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.486832</td>\n",
       "      <td>1.57258</td>\n",
       "      <td>0.842258</td>\n",
       "      <td>-1.464852</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.697747</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.181033</td>\n",
       "      <td>1.18288</td>\n",
       "      <td>0.539692</td>\n",
       "      <td>-1.646086</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.953992</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.047246</td>\n",
       "      <td>1.18288</td>\n",
       "      <td>0.536160</td>\n",
       "      <td>-1.283618</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.825870</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.028134</td>\n",
       "      <td>0.923085</td>\n",
       "      <td>0.554997</td>\n",
       "      <td>-1.827320</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.082115</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>2.241772</td>\n",
       "      <td>2.42992</td>\n",
       "      <td>1.605147</td>\n",
       "      <td>-2.008554</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.210238</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>2.480677</td>\n",
       "      <td>3.00148</td>\n",
       "      <td>1.620452</td>\n",
       "      <td>-2.371022</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.210238</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>2.346890</td>\n",
       "      <td>2.87158</td>\n",
       "      <td>1.571005</td>\n",
       "      <td>-2.552256</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mpg  cylinders  displacement horsepower    weight  accerleration  \\\n",
       "0 -0.697747   1.482053      1.075915   0.663285  0.619748      -1.283618   \n",
       "1 -1.082115   1.482053      1.486832    1.57258  0.842258      -1.464852   \n",
       "2 -0.697747   1.482053      1.181033    1.18288  0.539692      -1.646086   \n",
       "3 -0.953992   1.482053      1.047246    1.18288  0.536160      -1.283618   \n",
       "4 -0.825870   1.482053      1.028134   0.923085  0.554997      -1.827320   \n",
       "5 -1.082115   1.482053      2.241772    2.42992  1.605147      -2.008554   \n",
       "6 -1.210238   1.482053      2.480677    3.00148  1.620452      -2.371022   \n",
       "7 -1.210238   1.482053      2.346890    2.87158  1.571005      -2.552256   \n",
       "\n",
       "   model year origin  \n",
       "0   -1.623241      1  \n",
       "1   -1.623241      1  \n",
       "2   -1.623241      1  \n",
       "3   -1.623241      1  \n",
       "4   -1.623241      1  \n",
       "5   -1.623241      1  \n",
       "6   -1.623241      1  \n",
       "7   -1.623241      1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Question 3: a. Clean the data, removing samples with empty entries and scaling each feature to have zero mean and unit variance\n",
    "\n",
    "Step 4: Rewrite horsepower into its standardized form.\n",
    "\n",
    "Centralize the data by subtracting the mean, then devide the data with standard deviation. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "mean = df['horsepower'].mean()\n",
    "std = df['horsepower'].std()\n",
    "df['horsepower'] = (df['horsepower']-mean)/std\n",
    "\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 0.046677   -0.08986849  0.30446999 -0.06381986  0.35855809 -0.22516793\n",
      "  0.09657048  0.12859745 -0.7553884 ]\n",
      "Mean squared error: 0.17\n",
      "\n",
      "Sum of squared error, aka residual sum of squares.: 6.82\n",
      "\n",
      "Variance score: 0.82\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 3: b. Find the best fit model and print the coeficcient. \n",
    "\n",
    "Step 5: \n",
    "i.  DictVectorization for categorical features, aka 'origins'. \n",
    "ii. Fit the model with LiearRegression() to get the best model for all features. \n",
    "\n",
    "Noted that I used the train_test_split here to split up the dataset, the test size is set to be 10% of the total data.\n",
    "In such way I expected to make it more comparable with the 10-fold CV. \n",
    "\n",
    "\"\"\"\n",
    "np.random.seed(0)\n",
    "\n",
    "# Feature columns:\n",
    "col = ['cylinders', 'displacement', 'horsepower', 'weight', 'accerleration', 'model year', 'origin']\n",
    "\n",
    "# Create original X and y:\n",
    "Original_X = df[col]\n",
    "Original_y = df['mpg']\n",
    "\n",
    "# Dict_Vectorize the X: \n",
    "dict_data = Original_X.T.to_dict().values()\n",
    "\n",
    "# Initiate vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform X and y: y does not need to transform. \n",
    "X = vectorizer.fit_transform(dict_data)\n",
    "y = Original_y\n",
    "\n",
    "# Use tran_test_split from sklearn to split the test and train. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "print()\n",
    "\n",
    "# Sum of squared error, aka residual sum of squares. \n",
    "print(\"Sum of squared error, aka residual sum of squares.: %.2f\"\n",
    "      % ((mean_squared_error(y_test, y_pred))*len(y_test)))\n",
    "print()\n",
    "\n",
    "#print(\"Residual Sum of squares: %.2f\" % np.mean((y_pred - y_test) ** 2))\n",
    "\n",
    "# Explained variance score: a result of 1 denotes a perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC for all coefficients is:\n",
      "-666.4561468340506 \n",
      "BIC for all coefficients is:\n",
      "-634.686052115727 \n",
      "*******************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 3: c). Calculate the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for two models\n",
    " one with all coefficients  \n",
    "\n",
    "Step 6: AIC and BIC for all features. Should also be with all data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Fit all data:\n",
    "# Create original X and y:\n",
    "Original_X = df[col]\n",
    "Original_y = df['mpg']\n",
    "\n",
    "# Dict_Vectorize the X: \n",
    "dict_data = Original_X.T.to_dict().values()\n",
    "\n",
    "# Initiate vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform X and y: y does not need to transform. \n",
    "X = vectorizer.fit_transform(dict_data)\n",
    "y = Original_y\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X, y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X)\n",
    "\n",
    "\n",
    "k = 7+1\n",
    "n = len(y)\n",
    "\n",
    "mean_sq_er_total = mean_squared_error(y, y_pred)\n",
    "\n",
    "# AIC and BIC for All coefficients\n",
    "aic_total = (2*k) + n * np.log(mean_sq_er_total)\n",
    "print(\"AIC for all coefficients is:\\n{} \".format(aic_total))\n",
    "bic_total = k * np.log(n) + n * np.log(mean_sq_er_total)\n",
    "print(\"BIC for all coefficients is:\\n{} \".format(bic_total))\n",
    "print(\"*******************************************\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[ 0.42332854]]\n",
      "Mean squared error: 0.82\n",
      "\n",
      "Sum of squared error, aka residual sum of squares.: 32.75\n",
      "\n",
      "Variance score: 0.18\n",
      "\n",
      "AIC for all coefficients is:\n",
      "-74.41516012147483 \n",
      "BIC for all coefficients is:\n",
      "-66.4726364418939 \n",
      "*******************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 3: e). Calculate the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for two models\n",
    " one with one coefficient  \n",
    "\n",
    "Step 7: AIC and BIC for one feature. These are statistical methods, so I fitted the model with all samples. \n",
    "\n",
    "From above I know that \"weight\" has the most significant magnitute in coefficient.\n",
    "Then X will be weight in this question. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# Independent variable dataframe X.\n",
    "# X = df['accerleration'][:, np.newaxis]\n",
    "X = df['accerleration'][:, np.newaxis]\n",
    "\n",
    "y = df['mpg'][:, np.newaxis]\n",
    "\n",
    "# Use tran_test_split from sklearn to split the test and train. \n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Fill the model using all samples\n",
    "regr.fit(X, y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y, y_pred))\n",
    "print()\n",
    "\n",
    "# Sum of squared error, aka residual sum of squares. \n",
    "print(\"Sum of squared error, aka residual sum of squares.: %.2f\"\n",
    "      % ((mean_squared_error(y, y_pred))*len(y_test)))\n",
    "print()\n",
    "\n",
    "# Explained variance score: a result of 1 denotes a perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y, y_pred))\n",
    "print()\n",
    "\n",
    "# BIC and AIC: \n",
    "k = 1+1\n",
    "n = len(y)\n",
    "\n",
    "mean_sq_er_total = mean_squared_error(y, y_pred)\n",
    "\n",
    "# AIC and BIC for All coefficients\n",
    "aic_total = (2*k) + n * np.log(mean_sq_er_total)\n",
    "print(\"AIC for all coefficients is:\\n{} \".format(aic_total))\n",
    "bic_total = k * np.log(n) + n * np.log(mean_sq_er_total)\n",
    "print(\"BIC for all coefficients is:\\n{} \".format(bic_total))\n",
    "print(\"*******************************************\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Question 3:\n",
    "\n",
    "Step 8: Find a better model based on AIC and BIC. \n",
    "\n",
    "1. Define a function to iterate through given numbers of independent variable,\n",
    "and return variable combinations. \n",
    "   a). I set 'weight' as the main independent variable, since it has the highest coefficient.\n",
    "   b). To simplify the problem, I only use 2 and 6 features, which means to select 1 and 5 variables from list, since there is already a 'weight'.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def models(size):\n",
    "    # Define other independent variables for me to choose from. \n",
    "    Ind = [\"cylinders\", \"displacement\", \"horsepower\", \"accerleration\", \"model year\", \"origin\"]\n",
    "    # Create the combinations for chosen number of variables. \n",
    "    lst_select = list(itertools.combinations(Ind,size))\n",
    "    print(\"Model Selected:\")\n",
    "    print()\n",
    "\n",
    "    # Create the final model selection list. \n",
    "    lst_models = []\n",
    "    for ele in lst_select:\n",
    "        lst_main = [\"weight\"]\n",
    "        lst_main.extend(ele)\n",
    "        lst_models.append(lst_main)\n",
    "\n",
    "\n",
    "    print()\n",
    "    return lst_models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Question 3:\n",
    "\n",
    "Step 8: Find a better model based on AIC and BIC. \n",
    "\n",
    "Fit the model with all instances. \n",
    "\n",
    "2. Define a function to perform linear regression and caculate the AIC and BIC.\n",
    "   a). I set 'weight' as the main independent variable, since it has the highest coefficient.\n",
    "   b). To simplify the problem, I only use 2 and 5 features, which means to select 1 and 4 variables from list, since there is already a 'weight'.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def Linear_IC(model):\n",
    "    for m in model:\n",
    "        Original_X = df[m]\n",
    "        Original_y = df['mpg']\n",
    "        \n",
    "        # Dict_Vectorize the X: \n",
    "        dict_data = Original_X.T.to_dict().values()\n",
    "\n",
    "        # Initiate vectorizer\n",
    "        vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "        # Transform X and y: y does not need to transform. \n",
    "        X = vectorizer.fit_transform(dict_data)\n",
    "        # y does not need to be vectorized, since this is a numerical column.\n",
    "        y = Original_y\n",
    "        # Use tran_test_split from sklearn to split the test and train. \n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)\n",
    "\n",
    "        # Create linear regression object\n",
    "        regr = linear_model.LinearRegression()\n",
    "\n",
    "        # Train the model using the training sets\n",
    "        regr.fit(X, y)\n",
    "\n",
    "        # Make predictions using the testing set\n",
    "        y_pred = regr.predict(X)\n",
    "        \n",
    "        k = len(model)+1\n",
    "        n = len(y)\n",
    "\n",
    "        mean_sq_er_total = mean_squared_error(y, y_pred)\n",
    "\n",
    "        # AIC and BIC for All coefficients\n",
    "        aic_total = (2*k) + n * np.log(mean_sq_er_total)\n",
    "        print(\"AIC for {} is:\\n{} \".format(m, aic_total))\n",
    "        bic_total = k * np.log(n) + n * np.log(mean_sq_er_total)\n",
    "        print(\"BIC for {} is:\\n{} \".format(m, bic_total))\n",
    "        print(\"*******************************************\")\n",
    "        print()    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Selected:\n",
      "\n",
      "\n",
      "AIC for ['weight', 'cylinders', 'displacement', 'horsepower', 'accerleration', 'model year'] is:\n",
      "-636.4744577072913 \n",
      "BIC for ['weight', 'cylinders', 'displacement', 'horsepower', 'accerleration', 'model year'] is:\n",
      "-608.6756248287581 \n",
      "*******************************************\n",
      "\n",
      "AIC for ['weight', 'cylinders', 'displacement', 'horsepower', 'accerleration', 'origin'] is:\n",
      "-487.19808444062426 \n",
      "BIC for ['weight', 'cylinders', 'displacement', 'horsepower', 'accerleration', 'origin'] is:\n",
      "-459.399251562091 \n",
      "*******************************************\n",
      "\n",
      "AIC for ['weight', 'cylinders', 'displacement', 'horsepower', 'model year', 'origin'] is:\n",
      "-667.7928322750048 \n",
      "BIC for ['weight', 'cylinders', 'displacement', 'horsepower', 'model year', 'origin'] is:\n",
      "-639.9939993964715 \n",
      "*******************************************\n",
      "\n",
      "AIC for ['weight', 'cylinders', 'displacement', 'accerleration', 'model year', 'origin'] is:\n",
      "-666.6595162219766 \n",
      "BIC for ['weight', 'cylinders', 'displacement', 'accerleration', 'model year', 'origin'] is:\n",
      "-638.8606833434433 \n",
      "*******************************************\n",
      "\n",
      "AIC for ['weight', 'cylinders', 'horsepower', 'accerleration', 'model year', 'origin'] is:\n",
      "-658.5356017446155 \n",
      "BIC for ['weight', 'cylinders', 'horsepower', 'accerleration', 'model year', 'origin'] is:\n",
      "-630.7367688660822 \n",
      "*******************************************\n",
      "\n",
      "AIC for ['weight', 'displacement', 'horsepower', 'accerleration', 'model year', 'origin'] is:\n",
      "-666.0846891462209 \n",
      "BIC for ['weight', 'displacement', 'horsepower', 'accerleration', 'model year', 'origin'] is:\n",
      "-638.2858562676877 \n",
      "*******************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Linear_IC(models(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Selected:\n",
      "\n",
      "\n",
      "AIC for ['weight', 'cylinders'] is:\n",
      "-455.6581896347281 \n",
      "BIC for ['weight', 'cylinders'] is:\n",
      "-427.85935675619487 \n",
      "*******************************************\n",
      "\n",
      "AIC for ['weight', 'displacement'] is:\n",
      "-457.610871221308 \n",
      "BIC for ['weight', 'displacement'] is:\n",
      "-429.81203834277477 \n",
      "*******************************************\n",
      "\n",
      "AIC for ['weight', 'horsepower'] is:\n",
      "-467.3787628362023 \n",
      "BIC for ['weight', 'horsepower'] is:\n",
      "-439.5799299576691 \n",
      "*******************************************\n",
      "\n",
      "AIC for ['weight', 'accerleration'] is:\n",
      "-458.56411137934197 \n",
      "BIC for ['weight', 'accerleration'] is:\n",
      "-430.76527850080873 \n",
      "*******************************************\n",
      "\n",
      "AIC for ['weight', 'model year'] is:\n",
      "-634.2714586733665 \n",
      "BIC for ['weight', 'model year'] is:\n",
      "-606.4726257948332 \n",
      "*******************************************\n",
      "\n",
      "AIC for ['weight', 'origin'] is:\n",
      "-461.6338867584645 \n",
      "BIC for ['weight', 'origin'] is:\n",
      "-433.83505387993125 \n",
      "*******************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 Features:\n",
    "\n",
    "Linear_IC(models(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Question 3: g). Use 10-fold cross validation and MSE as a metric to select among these three models. \n",
    "\n",
    "Manually Method.\n",
    "\n",
    "Step 9. 10-fold cross validation\n",
    "\n",
    "Define a class CrossValidation(), and inside the class, define the function k_fold(k, model_list):\n",
    "k - number of folds;\n",
    "model_list: the list containing the column names for the selected features. \n",
    "\n",
    "Noted that this class should only be used after the dataframe is defined, cleaned and named as 'df'.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class CrossValidation_Manually():\n",
    "    \n",
    "    def __init__(self, model_list): \n",
    "        self.inst_attr = \"Selected Features for Cross Validation.\" \n",
    "        self.model_list = model_list\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return(print(\"Selected features: \\n{}\".format(self.model_list)))\n",
    "\n",
    "    \n",
    "    def K_fold(self):\n",
    "        \n",
    "        self.R2_Manually = []\n",
    "        self.MSE_Manually = []\n",
    "        \n",
    "        for i in range(10):\n",
    "            print(\"****************Fold {} as the test dataset*******************\".format(i))\n",
    "\n",
    "            # Noted that before execute the function, I need the data after the extra datapoints taken out. \n",
    "            # Namely, I have df_new and df_target_new. \n",
    "            n = len(df_target_new)\n",
    "            array = np.arange(n).reshape((n, 1))\n",
    "            #print(array)\n",
    "\n",
    "            split = np.vsplit(array, 10)\n",
    "\n",
    "            # Create linear regression object\n",
    "            regr = linear_model.LinearRegression()\n",
    "        \n",
    "            X_train = np.delete(np.array(df_new), (split[i].tolist()), axis=0)\n",
    "            \n",
    "            X_step_1 = np.array(df_new)[split[i].tolist(),]\n",
    "            X_test = np.squeeze(X_step_1)\n",
    "\n",
    "            y_train = np.delete(np.array(df_target_new), (split[i].tolist()), axis=0)\n",
    "        \n",
    "            y_step_1 = np.array(df_target_new)[split[i].tolist(),]  \n",
    "            y_test = np.squeeze(y_step_1)\n",
    "    \n",
    "            regr.fit(X_train, y_train)\n",
    "    \n",
    "            y_pred = regr.predict(X_test)\n",
    "    \n",
    "            print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "            # The mean squared error\n",
    "            print(\"Mean squared error: %.2f\"\n",
    "                  % mean_squared_error(y_test, y_pred))\n",
    "            self.MSE_Manually.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "            # Explained variance score: a result of 1 denotes a perfect prediction\n",
    "            print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "            self.R2_Manually.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "            print()\n",
    "\n",
    "    # For further questions in logistic regression.\n",
    "    def K_fold_logreg(self):\n",
    "        \n",
    "        #self.R2_logreg = []\n",
    "        self.MSE_logreg = []\n",
    "        self.score_logreg = []\n",
    "        self.accuracy_logreg = []\n",
    "        \n",
    "        for i in range(10):\n",
    "            print(\"****************Fold {} as the test dataset*******************\".format(i))\n",
    "\n",
    "            # Noted that before execute the function, I need the data after the extra datapoints taken out. \n",
    "            # Namely, I have df_new and df_target_new. \n",
    "            n = len(df_target_new)\n",
    "            array = np.arange(n).reshape((n, 1))\n",
    "            #print(array)\n",
    "\n",
    "            split = np.vsplit(array, 10)\n",
    "            \n",
    "            # Train and test data:\n",
    "            X_train = np.delete(np.array(df_new), (split[i].tolist()), axis=0)\n",
    "            \n",
    "            X_step_1 = np.array(df_new)[split[i].tolist(),]\n",
    "            X_test = np.squeeze(X_step_1)\n",
    "\n",
    "            y_train = np.delete(np.array(df_target_new), (split[i].tolist()), axis=0)\n",
    "        \n",
    "            y_step_1 = np.array(df_target_new)[split[i].tolist(),]  \n",
    "            y_test = np.squeeze(y_step_1)\n",
    "    \n",
    "            # Initiate logistic regression model:\n",
    "            logreg = linear_model.LogisticRegression(C=1e5)\n",
    "            \n",
    "            logreg.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = logreg.predict(X_test)\n",
    "\n",
    "            #print(\"Prediction: {}\".format(y_pred))\n",
    "\n",
    "            print(\"Logistic Regression Score with LogisticRegression.scoore(): {}\".format(logreg.score(X_test, y_test)))\n",
    "            self.score_logreg.append(logreg.score(X_test, y_test))\n",
    "            print()\n",
    "            \n",
    "            print(\"Logistic Regression Score with Metrics.accuracy_score(): {}\".format(accuracy_score(y_test, y_pred)))\n",
    "            self.accuracy_logreg.append(accuracy_score(y_test, y_pred))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how many data points to be droped, I need the shape of vectorized data:\n",
      "(392, 8)\n",
      "The shape of new data:\n",
      "(390, 8)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 3: g). Use 10-fold cross validation and MSE as a metric to select among these three  models. Manually Method.\n",
    "\n",
    "Model 1: ['weight', 'cylinders', 'displacement', 'horsepower', 'model year', 'origin'] since the AIC and BIC are the smallest, suggesting a good fit:\n",
    "\n",
    "Step 1: Since I am going to use 10-folds manually, I need to drop extra data points with the Victorized matrix.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Original_X = df[['weight', 'cylinders', 'displacement', 'horsepower', 'model year', 'origin']]\n",
    "Original_y = df['mpg']\n",
    "# Dict_Vectorize the X: \n",
    "dict_data = Original_X.T.to_dict().values()\n",
    "\n",
    "# Initiate vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform X and y: y does not need to transform. \n",
    "X = pd.DataFrame(vectorizer.fit_transform(dict_data))\n",
    "y = Original_y\n",
    "\n",
    "print(\"To determine how many data points to be droped, I need the shape of vectorized data:\\n{}\".format(X.shape))\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# Set the number of samples to be removed.\n",
    "remove_n = 2\n",
    "\n",
    "# Randomly choose remove_n number of index from dataframe.\n",
    "drop = np.random.choice(df.index, remove_n, replace=False)\n",
    "\n",
    "# Drop rows that are randomly chosen based on indexes, for fearure data.\n",
    "df_new = X.drop(drop)\n",
    "\n",
    "# Drop the same rows in target data.\n",
    "# Write target into panda dataframe for easier furthur process.\n",
    "df_target_new = y.drop(drop)\n",
    "\n",
    "print(\"The shape of new data:\\n{}\".format(df_new.shape))\n",
    "# print(len(df_target_new))\n",
    "# print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************Fold 0 as the test dataset*******************\n",
      "Coefficients: \n",
      " [-0.10787796  0.33716095 -0.185793    0.40050197 -0.24389737  0.10558049\n",
      "  0.13831689 -0.68280928]\n",
      "Mean squared error: 0.16\n",
      "Variance score: 0.65\n",
      "\n",
      "****************Fold 1 as the test dataset*******************\n",
      "Coefficients: \n",
      " [-0.13991014  0.31472152 -0.10714193  0.38044561 -0.2249957   0.10093149\n",
      "  0.12406422 -0.71582277]\n",
      "Mean squared error: 0.15\n",
      "Variance score: 0.77\n",
      "\n",
      "****************Fold 2 as the test dataset*******************\n",
      "Coefficients: \n",
      " [-0.10415255  0.29695365 -0.14149993  0.34373634 -0.22552212  0.07906398\n",
      "  0.14645814 -0.71160246]\n",
      "Mean squared error: 0.16\n",
      "Variance score: 0.66\n",
      "\n",
      "****************Fold 3 as the test dataset*******************\n",
      "Coefficients: \n",
      " [-0.14413236  0.38894516 -0.14398435  0.35870711 -0.25047262  0.12718146\n",
      "  0.12329117 -0.72871381]\n",
      "Mean squared error: 0.15\n",
      "Variance score: 0.76\n",
      "\n",
      "****************Fold 4 as the test dataset*******************\n",
      "Coefficients: \n",
      " [-0.118021    0.32459559 -0.11580741  0.36307947 -0.23838452  0.11662124\n",
      "  0.12176328 -0.72148744]\n",
      "Mean squared error: 0.09\n",
      "Variance score: 0.76\n",
      "\n",
      "****************Fold 5 as the test dataset*******************\n",
      "Coefficients: \n",
      " [-0.10493051  0.29780577 -0.12721867  0.36964405 -0.25060539  0.1106865\n",
      "  0.13991889 -0.67240815]\n",
      "Mean squared error: 0.11\n",
      "Variance score: 0.85\n",
      "\n",
      "****************Fold 6 as the test dataset*******************\n",
      "Coefficients: \n",
      " [-0.09575303  0.25392961 -0.07891874  0.38348683 -0.25451196  0.10558648\n",
      "  0.14892548 -0.67183384]\n",
      "Mean squared error: 0.28\n",
      "Variance score: 0.61\n",
      "\n",
      "****************Fold 7 as the test dataset*******************\n",
      "Coefficients: \n",
      " [-0.09256317  0.28609515 -0.1238086   0.36637712 -0.25630373  0.10728439\n",
      "  0.14901934 -0.66172614]\n",
      "Mean squared error: 0.20\n",
      "Variance score: 0.77\n",
      "\n",
      "****************Fold 8 as the test dataset*******************\n",
      "Coefficients: \n",
      " [-0.1517708   0.27142845 -0.07203008  0.33298426 -0.16388255  0.0611369\n",
      "  0.10274565 -0.68119881]\n",
      "Mean squared error: 0.48\n",
      "Variance score: 0.19\n",
      "\n",
      "****************Fold 9 as the test dataset*******************\n",
      "Coefficients: \n",
      " [-0.09570753  0.22678887 -0.08688184  0.35489225 -0.22094543  0.07021975\n",
      "  0.15072568 -0.66579548]\n",
      "Mean squared error: 0.24\n",
      "Variance score: 0.56\n",
      "\n",
      "Mean R2 for Manually generated 10-fold:\n",
      "0.6585628893615294\n",
      "\n",
      "Mean MSE for Manually generated 10-fold:\n",
      "0.201592421521993\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 3: g). Use 10-fold cross validation and MSE as a metric to select among these three  models. Manually Method.\n",
    "\n",
    "Model 1: ['weight', 'cylinders', 'displacement', 'horsepower', 'model year', 'origin'] since the AIC and BIC are the smallest, suggesting a good fit:\n",
    "\n",
    "Step 2: Use the Class CrossValidation_Manually for ['weight', 'cylinders', 'displacement', 'horsepower', 'model year', 'origin'].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "CV = CrossValidation_Manually(['weight', 'cylinders', 'displacement', 'horsepower', 'model year', 'origin'])\n",
    "CV.K_fold()\n",
    "#print(CV.R2_Manually)\n",
    "print(\"Mean R2 for Manually generated 10-fold:\\n{}\".format(np.mean(CV.R2_Manually)))\n",
    "print()\n",
    "#print(CV.MSE_Manually)\n",
    "print(\"Mean MSE for Manually generated 10-fold:\\n{}\".format(np.mean(CV.MSE_Manually)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how many data points to be droped, I need the shape of vectorized data:\n",
      "(392, 9)\n",
      "The shape of new data:\n",
      "(390, 9)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 3: g). Use 10-fold cross validation and MSE as a metric to select among these three  models. Manually Method.\n",
    "\n",
    "Model 2: All features.\n",
    "\n",
    "Step 1: Since I am going to use 10-folds manually, I need to drop extra data points with the Victorized matrix.\n",
    "\n",
    "\"\"\"\n",
    "col = ['cylinders', 'displacement', 'horsepower', 'weight', 'accerleration', 'model year', 'origin']\n",
    "\n",
    "Original_X = df[col]\n",
    "Original_y = df['mpg']\n",
    "# Dict_Vectorize the X: \n",
    "dict_data = Original_X.T.to_dict().values()\n",
    "\n",
    "# Initiate vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform X and y: y does not need to transform. \n",
    "X = pd.DataFrame(vectorizer.fit_transform(dict_data))\n",
    "y = Original_y\n",
    "\n",
    "print(\"To determine how many data points to be droped, I need the shape of vectorized data:\\n{}\".format(X.shape))\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# Set the number of samples to be removed.\n",
    "remove_n = 2\n",
    "\n",
    "# Randomly choose remove_n number of index from dataframe.\n",
    "drop = np.random.choice(df.index, remove_n, replace=False)\n",
    "\n",
    "# Drop rows that are randomly chosen based on indexes, for fearure data.\n",
    "df_new = X.drop(drop)\n",
    "\n",
    "# Drop the same rows in target data.\n",
    "# Write target into panda dataframe for easier furthur process.\n",
    "df_target_new = y.drop(drop)\n",
    "\n",
    "print(\"The shape of new data:\\n{}\".format(df_new.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************Fold 0 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.01596907 -0.10669702  0.34018156 -0.16225495  0.40165517 -0.24340513\n",
      "  0.10583235  0.13757278 -0.69956065]\n",
      "Mean squared error: 0.15\n",
      "Variance score: 0.66\n",
      "\n",
      "****************Fold 1 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.04050435 -0.13461984  0.32673068 -0.05877689  0.38188957 -0.22373483\n",
      "  0.10129901  0.12243582 -0.75878748]\n",
      "Mean squared error: 0.15\n",
      "Variance score: 0.77\n",
      "\n",
      "****************Fold 2 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.03755443 -0.1003782   0.30826507 -0.09496732  0.34540591 -0.22469373\n",
      "  0.07915171  0.14554201 -0.7505717 ]\n",
      "Mean squared error: 0.17\n",
      "Variance score: 0.65\n",
      "\n",
      "****************Fold 3 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.02613028 -0.1409901   0.39776534 -0.11327585  0.36017411 -0.24997741\n",
      "  0.12572446  0.12425294 -0.75524174]\n",
      "Mean squared error: 0.15\n",
      "Variance score: 0.76\n",
      "\n",
      "****************Fold 4 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.02783779 -0.11594274  0.33355421 -0.08314498  0.36426613 -0.23648211\n",
      "  0.11508289  0.12139921 -0.7491035 ]\n",
      "Mean squared error: 0.09\n",
      "Variance score: 0.76\n",
      "\n",
      "****************Fold 5 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.03083625 -0.10163837  0.30727714 -0.09119295  0.37123856 -0.25065031\n",
      "  0.11003736  0.14061295 -0.70164481]\n",
      "Mean squared error: 0.11\n",
      "Variance score: 0.84\n",
      "\n",
      "****************Fold 6 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.00824945 -0.0948306   0.25649074 -0.06920086  0.38392939 -0.25440216\n",
      "  0.1055542   0.14884797 -0.67987444]\n",
      "Mean squared error: 0.28\n",
      "Variance score: 0.62\n",
      "\n",
      "****************Fold 7 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.06097604 -0.08438513  0.30747772 -0.05638666  0.36983602 -0.25958085\n",
      "  0.10857438  0.15100647 -0.71754051]\n",
      "Mean squared error: 0.21\n",
      "Variance score: 0.75\n",
      "\n",
      "****************Fold 8 as the test dataset*******************\n",
      "Coefficients: \n",
      " [-0.01079202 -0.15310295  0.26785557 -0.08402936  0.33256696 -0.16374277\n",
      "  0.0607711   0.10297167 -0.67104221]\n",
      "Mean squared error: 0.49\n",
      "Variance score: 0.19\n",
      "\n",
      "****************Fold 9 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.00629729 -0.09490096  0.22871792 -0.07952143  0.3553559  -0.22087828\n",
      "  0.07048828  0.15039    -0.6719569 ]\n",
      "Mean squared error: 0.24\n",
      "Variance score: 0.57\n",
      "\n",
      "Mean R2 for Manually generated 10-fold:\n",
      "0.6553998350903572\n",
      "\n",
      "Mean MSE for Manually generated 10-fold:\n",
      "0.20402131057568335\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 3: g). Use 10-fold cross validation and MSE as a metric to select among these three  models. Manually Method.\n",
    "\n",
    "Model 1: All features\n",
    "\n",
    "Step 2: Use the Class CrossValidation_Manually for ['weight', 'horsepower'].\n",
    "\n",
    "\"\"\"\n",
    "CV = CrossValidation_Manually(col)\n",
    "CV.K_fold()\n",
    "#print(CV.R2_Manually)\n",
    "print(\"Mean R2 for Manually generated 10-fold:\\n{}\".format(np.mean(CV.R2_Manually)))\n",
    "print()\n",
    "#print(CV.MSE_Manually)\n",
    "print(\"Mean MSE for Manually generated 10-fold:\\n{}\".format(np.mean(CV.MSE_Manually)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how many data points to be droped, I need the shape of data:\n",
      "(392,)\n",
      "The shape of new data:\n",
      "(390,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 3: g). Use 10-fold cross validation and MSE as a metric to select among these three  models. Manually Method.\n",
    "\n",
    "Model 2: One feature.\n",
    "\n",
    "Step 1: Since I am going to use 10-folds manually, I need to drop extra data points with the Victorized matrix.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Original_X = df['accerleration']\n",
    "Original_y = df['mpg']\n",
    "\n",
    "# Dict_Vectorize the X: \n",
    "X = Original_X\n",
    "y = Original_y\n",
    "\n",
    "print(\"To determine how many data points to be droped, I need the shape of data:\\n{}\".format(X.shape))\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# Set the number of samples to be removed.\n",
    "remove_n = 2\n",
    "\n",
    "# Randomly choose remove_n number of index from dataframe.\n",
    "drop = np.random.choice(df.index, remove_n, replace=False)\n",
    "\n",
    "# Drop rows that are randomly chosen based on indexes, for fearure data.\n",
    "df_new = X.drop(drop)\n",
    "\n",
    "# Drop the same rows in target data.\n",
    "# Write target into panda dataframe for easier furthur process.\n",
    "df_target_new = y.drop(drop)\n",
    "\n",
    "print(\"The shape of new data:\\n{}\".format(df_new.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************Fold 0 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.4066289]\n",
      "Mean squared error: 0.51\n",
      "\n",
      "****************Fold 1 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.40228822]\n",
      "Mean squared error: 0.66\n",
      "\n",
      "****************Fold 2 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.39503672]\n",
      "Mean squared error: 0.65\n",
      "\n",
      "****************Fold 3 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.43295572]\n",
      "Mean squared error: 0.68\n",
      "\n",
      "****************Fold 4 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.44965339]\n",
      "Mean squared error: 0.51\n",
      "\n",
      "****************Fold 5 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.44175761]\n",
      "Mean squared error: 0.81\n",
      "\n",
      "****************Fold 6 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.41894505]\n",
      "Mean squared error: 0.61\n",
      "\n",
      "****************Fold 7 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.45938171]\n",
      "Mean squared error: 1.17\n",
      "\n",
      "****************Fold 8 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.39047438]\n",
      "Mean squared error: 2.06\n",
      "\n",
      "****************Fold 9 as the test dataset*******************\n",
      "Coefficients: \n",
      " [ 0.41592265]\n",
      "Mean squared error: 1.31\n",
      "\n",
      "Mean for MES: 0.8976721507470353\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 3: g). Use 10-fold cross validation and MSE as a metric to select among these three  models. Manually Method.\n",
    "\n",
    "Model 2: One feature.\n",
    "\n",
    "Step 1: Since I am going to use 10-folds manually, I need to drop extra data points with the Victorized matrix.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "n = len(df_target_new)\n",
    "array = np.arange(n).reshape((n, 1))\n",
    "#print(array)\n",
    "\n",
    "split = np.vsplit(array, 10)\n",
    "\n",
    "MSE = []\n",
    "for i in range(10):\n",
    "    print(\"****************Fold {} as the test dataset*******************\".format(i))\n",
    "\n",
    "    X_train = np.delete(np.array(df_new), (split[i].tolist()), axis=0)\n",
    "    X_step_1 = np.array(df_new)[split[i].tolist(),]\n",
    "    \n",
    "    X_test = np.squeeze(X_step_1)\n",
    "\n",
    "    y_train = np.delete(np.array(df_target_new), (split[i].tolist()), axis=0)\n",
    "    y_step_1 = np.array(df_target_new)[split[i].tolist(),]\n",
    "    \n",
    "    y_test = np.squeeze(y_step_1)\n",
    "    \n",
    "    regr.fit(X_train.reshape(-1, 1), y_train)\n",
    "    \n",
    "    y_pred = regr.predict(X_test.reshape(-1, 1))\n",
    "    \n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_test, y_pred))\n",
    "    MSE.append(mean_squared_error(y_test, y_pred))\n",
    "    print()\n",
    "\n",
    "print(\"Mean for MES: {}\".format(np.mean(MSE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Question 3: g). Use 10-fold cross validation and MSE as a metric to select among these three  models: KFold Method.\n",
    "\n",
    "Step 9. 10-fold cross validation\n",
    "\n",
    "Define a class CrossValidation(), and inside the class, define the function k_fold(k, model_list):\n",
    "k - number of folds;\n",
    "model_list: the list containing the column names for the selected features. \n",
    "\n",
    "Noted that this class should only be used after the dataframe is defined, cleaned and named as 'df'.\n",
    "\n",
    "\n",
    "class CrossValidation_KFold():\n",
    "    \n",
    "    def __init__(self, model_list): \n",
    "        self.inst_attr = \"Selected Features for Cross Validation.\" \n",
    "        self.model_list = model_list\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return(print(\"Selected features: \\n{}\".format(self.model_list)))\n",
    "\n",
    "    \n",
    "    def K_fold(self, k):\n",
    "    \n",
    "        np.random.seed(0)\n",
    "    \n",
    "        X = df.as_matrix(columns = self.model_list)\n",
    "        y = df['mpg'][:, np.newaxis]\n",
    "\n",
    "        # Inditiate KFold.\n",
    "        kf = KFold(n_splits=k)\n",
    "\n",
    "        # Split data into 10 folds.\n",
    "        s = kf.split(df)\n",
    "        n = kf.get_n_splits(df)\n",
    "        print(\"Splitted into {} folds\".format(n))\n",
    "\n",
    "        # Initiate regression.\n",
    "        # Create linear regression object\n",
    "        regr = linear_model.LinearRegression()\n",
    "\n",
    "        self.R2 = []\n",
    "        self.MSE = []\n",
    "        # Cross-validation.\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            print(\"**********************************\")\n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            # Train the model using the training sets\n",
    "            regr.fit(X_train, y_train)\n",
    "            # Predict.\n",
    "            y_pred = regr.predict(X_test)\n",
    "        \n",
    "            # The coefficients\n",
    "            print('Coefficients: \\n', regr.coef_)\n",
    "        \n",
    "            # The mean squared error\n",
    "            print(\"Mean squared error: %.2f\" \n",
    "                  % mean_squared_error(y_test, y_pred))\n",
    "            self.MSE.append(mean_squared_error(y_test, y_pred))\n",
    "            print()\n",
    "        \n",
    "            # Explained variance score: a result of 1 denotes a perfect prediction\n",
    "            print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "            #Create a list of variance scores for each fold to answer question d.\n",
    "            self.R2.append(r2_score(y_test, y_pred))\n",
    "\n",
    " \"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Question 4:\n",
    "\n",
    "Allow for interactions. \n",
    "\n",
    "Step 1: Created the interaction columns. \n",
    "\n",
    "Noted that the creation for interaction requires comprehension for each feature.\n",
    "And since I am not an expert for cars, I just picked two features to show the thought process for this question. \n",
    "Lets assume there is a possible interaction between 'horsepower' and 'acceleration'. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create interaction column, based on the existing df.\n",
    "# df.head(8)\n",
    "df['inter'] = df['horsepower']*df['accerleration']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>accerleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "      <th>inter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.697747</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.075915</td>\n",
       "      <td>0.663285</td>\n",
       "      <td>0.619748</td>\n",
       "      <td>-1.283618</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.13069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.082115</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.486832</td>\n",
       "      <td>1.57258</td>\n",
       "      <td>0.842258</td>\n",
       "      <td>-1.464852</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.28817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.697747</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.181033</td>\n",
       "      <td>1.18288</td>\n",
       "      <td>0.539692</td>\n",
       "      <td>-1.646086</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.00404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.953992</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.047246</td>\n",
       "      <td>1.18288</td>\n",
       "      <td>0.536160</td>\n",
       "      <td>-1.283618</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.662297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.825870</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.028134</td>\n",
       "      <td>0.923085</td>\n",
       "      <td>0.554997</td>\n",
       "      <td>-1.827320</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.79652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.082115</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>2.241772</td>\n",
       "      <td>2.42992</td>\n",
       "      <td>1.605147</td>\n",
       "      <td>-2.008554</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.34219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.210238</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>2.480677</td>\n",
       "      <td>3.00148</td>\n",
       "      <td>1.620452</td>\n",
       "      <td>-2.371022</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.12436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.210238</td>\n",
       "      <td>1.482053</td>\n",
       "      <td>2.346890</td>\n",
       "      <td>2.87158</td>\n",
       "      <td>1.571005</td>\n",
       "      <td>-2.552256</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.29368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mpg  cylinders  displacement horsepower    weight  accerleration  \\\n",
       "0 -0.697747   1.482053      1.075915   0.663285  0.619748      -1.283618   \n",
       "1 -1.082115   1.482053      1.486832    1.57258  0.842258      -1.464852   \n",
       "2 -0.697747   1.482053      1.181033    1.18288  0.539692      -1.646086   \n",
       "3 -0.953992   1.482053      1.047246    1.18288  0.536160      -1.283618   \n",
       "4 -0.825870   1.482053      1.028134   0.923085  0.554997      -1.827320   \n",
       "5 -1.082115   1.482053      2.241772    2.42992  1.605147      -2.008554   \n",
       "6 -1.210238   1.482053      2.480677    3.00148  1.620452      -2.371022   \n",
       "7 -1.210238   1.482053      2.346890    2.87158  1.571005      -2.552256   \n",
       "\n",
       "   model year origin     inter  \n",
       "0   -1.623241      1  -0.13069  \n",
       "1   -1.623241      1  -1.28817  \n",
       "2   -1.623241      1  -1.00404  \n",
       "3   -1.623241      1 -0.662297  \n",
       "4   -1.623241      1  -0.79652  \n",
       "5   -1.623241      1  -3.34219  \n",
       "6   -1.623241      1  -5.12436  \n",
       "7   -1.623241      1  -5.29368  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Question 4:\n",
    "\n",
    "Step 2: Standardize the interaction column.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#df.head(8)\n",
    "mean = np.mean(df['inter'])\n",
    "std = df['inter'].std()\n",
    "df['inter'] = (df['inter']-mean)/std\n",
    "#print(np.mean(df['inter']))\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[-0.47732836 -0.64630976 -0.21057006 -0.28074663]]\n",
      "Mean squared error: 0.21\n",
      "\n",
      "Sum of squared error, aka residual sum of squares.: 12.30\n",
      "\n",
      "Variance score: 0.79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 4:\n",
    "\n",
    "Step 3: Regression\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "col = ['displacement','horsepower', 'accerleration', 'inter']\n",
    "\n",
    "# Independent variable dataframe X.\n",
    "X = df.as_matrix(columns = col)\n",
    "\n",
    "y = df['mpg'][:, np.newaxis]\n",
    "\n",
    "# Use tran_test_split from sklearn to split the test and train. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "print()\n",
    "\n",
    "# Sum of squared error, aka residual sum of squares. \n",
    "print(\"Sum of squared error, aka residual sum of squares.: %.2f\"\n",
    "      % ((mean_squared_error(y_test, y_pred))*len(y_test)))\n",
    "print()\n",
    "\n",
    "# Explained variance score: a result of 1 denotes a perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*****************************************************QUESTION 5*************************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>30.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>25.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>63.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77.9583</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>39.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived     sex    age  sibsp  parch      fare embarked\n",
       "0       1         1  female  29.00      0      0  211.3375        S\n",
       "1       1         1    male   0.92      1      2  151.5500        S\n",
       "2       1         0  female   2.00      1      2  151.5500        S\n",
       "3       1         0    male  30.00      1      2  151.5500        S\n",
       "4       1         0  female  25.00      1      2  151.5500        S\n",
       "5       1         1    male  48.00      0      0   26.5500        S\n",
       "6       1         1  female  63.00      1      0   77.9583        S\n",
       "7       1         0    male  39.00      0      0    0.0000        S"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Question 5: Titanic Dataset, a)\n",
    "\n",
    "Step 1. pclass, survived (the target variable), sex, age, sibsp, parch, fare, embarked\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Read in the csv.\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "# Keep only columns of interest. \n",
    "drop = ['name', 'ticket','cabin','boat','body','home.dest']\n",
    "df = df.drop(drop, axis = 1)\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1309, 8)\n",
      "(1043, 8)\n",
      "Type for column pclass: <class 'numpy.int64'>\n",
      "Type for column sex: <class 'str'>\n",
      "Type for column age: <class 'numpy.float64'>\n",
      "Type for column sibsp: <class 'numpy.int64'>\n",
      "Type for column parch: <class 'numpy.int64'>\n",
      "Type for column fare: <class 'numpy.float64'>\n",
      "Type for column embarked: <class 'str'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 5: Titanic Dataset, a)\n",
    "\n",
    "Step 2. Clean the data:\n",
    "\n",
    "i. I want to drop the NaNs, and check the type for each column. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(df.shape)\n",
    "df.dropna(how = 'any', inplace = True)\n",
    "print(df.shape)\n",
    "\n",
    "# Check type for each column:\n",
    "col = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
    "\n",
    "for column in col:\n",
    "    print(\"Type for column {}: {}\".format(column, type(df[column][0])))\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type for column pclass: <class 'str'>\n",
      "Type for column sex: <class 'str'>\n",
      "Type for column age: <class 'numpy.float64'>\n",
      "Type for column sibsp: <class 'numpy.int64'>\n",
      "Type for column parch: <class 'numpy.int64'>\n",
      "Type for column fare: <class 'numpy.float64'>\n",
      "Type for column embarked: <class 'str'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 5: Titanic Dataset, a)\n",
    "\n",
    "Step 2. Clean the data:\n",
    "\n",
    "ii. Since I wanted to treat 'pclass' as categorical, I need to convert it into string for further vectorization.\n",
    "\n",
    "\"\"\"\n",
    "# Convert 'pclass' column into string:\n",
    "df['pclass'] = df['pclass'].astype(str)\n",
    "for column in col:\n",
    "    print(\"Type for column {}: {}\".format(column, type(df[column][0])))\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived or Not: 0\n",
      "     pclass  survived     sex   age  sibsp  parch      fare embarked\n",
      "2         1         0  female   2.0      1      2  151.5500        S\n",
      "3         1         0    male  30.0      1      2  151.5500        S\n",
      "4         1         0  female  25.0      1      2  151.5500        S\n",
      "7         1         0    male  39.0      0      0    0.0000        S\n",
      "9         1         0    male  71.0      0      0   49.5042        C\n",
      "10        1         0    male  47.0      1      0  227.5250        C\n",
      "16        1         0    male  24.0      0      1  247.5208        C\n",
      "19        1         0    male  36.0      0      0   75.2417        C\n",
      "25        1         0    male  25.0      0      0   26.0000        C\n",
      "30        1         0    male  45.0      0      0   35.5000        S\n",
      "34        1         0    male  42.0      0      0   26.5500        S\n",
      "38        1         0    male  41.0      0      0   30.5000        S\n",
      "39        1         0    male  48.0      0      0   50.4958        C\n",
      "45        1         0    male  45.0      0      0   26.5500        S\n",
      "51        1         0    male  33.0      0      0    5.0000        S\n",
      "52        1         0    male  28.0      0      0   47.1000        S\n",
      "53        1         0    male  17.0      0      0   47.1000        S\n",
      "58        1         0    male  49.0      0      0   26.0000        S\n",
      "60        1         0    male  36.0      1      0   78.8500        S\n",
      "62        1         0    male  46.0      1      0   61.1750        S\n",
      "71        1         0    male  27.0      1      0  136.7792        C\n",
      "75        1         0    male  47.0      0      0   25.5875        S\n",
      "77        1         0    male  37.0      1      1   83.1583        C\n",
      "81        1         0    male  70.0      1      1   71.0000        S\n",
      "84        1         0    male  39.0      1      0   71.2833        C\n",
      "89        1         0    male  31.0      1      0   52.0000        S\n",
      "96        1         0    male  50.0      1      0  106.4250        C\n",
      "101       1         0    male  39.0      0      0   29.7000        C\n",
      "105       1         0  female  36.0      0      0   31.6792        C\n",
      "110       1         0    male  30.0      0      0   27.7500        C\n",
      "...     ...       ...     ...   ...    ...    ...       ...      ...\n",
      "1264      3         0    male  40.5      0      2   14.5000        S\n",
      "1265      3         0  female  10.0      0      2   24.1500        S\n",
      "1266      3         0    male  36.0      1      1   24.1500        S\n",
      "1267      3         0  female  30.0      1      1   24.1500        S\n",
      "1269      3         0    male  33.0      0      0    9.5000        S\n",
      "1270      3         0    male  28.0      0      0    9.5000        S\n",
      "1271      3         0    male  28.0      0      0    9.5000        S\n",
      "1272      3         0    male  47.0      0      0    9.0000        S\n",
      "1273      3         0  female  18.0      2      0   18.0000        S\n",
      "1274      3         0    male  31.0      3      0   18.0000        S\n",
      "1275      3         0    male  16.0      2      0   18.0000        S\n",
      "1276      3         0  female  31.0      1      0   18.0000        S\n",
      "1278      3         0    male  20.0      0      0    7.8542        S\n",
      "1279      3         0  female  14.0      0      0    7.8542        S\n",
      "1280      3         0    male  22.0      0      0    7.8958        S\n",
      "1281      3         0    male  22.0      0      0    9.0000        S\n",
      "1285      3         0    male  32.5      0      0    9.5000        S\n",
      "1287      3         0    male  51.0      0      0    7.7500        S\n",
      "1288      3         0    male  18.0      1      0    6.4958        S\n",
      "1289      3         0    male  21.0      1      0    6.4958        S\n",
      "1294      3         0    male  28.5      0      0   16.1000        S\n",
      "1295      3         0    male  21.0      0      0    7.2500        S\n",
      "1296      3         0    male  27.0      0      0    8.6625        S\n",
      "1298      3         0    male  36.0      0      0    9.5000        S\n",
      "1299      3         0    male  27.0      1      0   14.4542        C\n",
      "1301      3         0    male  45.5      0      0    7.2250        C\n",
      "1304      3         0  female  14.5      1      0   14.4542        C\n",
      "1306      3         0    male  26.5      0      0    7.2250        C\n",
      "1307      3         0    male  27.0      0      0    7.2250        C\n",
      "1308      3         0    male  29.0      0      0    7.8750        S\n",
      "\n",
      "[618 rows x 8 columns]\n",
      "2        2.0\n",
      "3       30.0\n",
      "4       25.0\n",
      "7       39.0\n",
      "9       71.0\n",
      "10      47.0\n",
      "16      24.0\n",
      "19      36.0\n",
      "25      25.0\n",
      "30      45.0\n",
      "34      42.0\n",
      "38      41.0\n",
      "39      48.0\n",
      "45      45.0\n",
      "51      33.0\n",
      "52      28.0\n",
      "53      17.0\n",
      "58      49.0\n",
      "60      36.0\n",
      "62      46.0\n",
      "71      27.0\n",
      "75      47.0\n",
      "77      37.0\n",
      "81      70.0\n",
      "84      39.0\n",
      "89      31.0\n",
      "96      50.0\n",
      "101     39.0\n",
      "105     36.0\n",
      "110     30.0\n",
      "        ... \n",
      "1264    40.5\n",
      "1265    10.0\n",
      "1266    36.0\n",
      "1267    30.0\n",
      "1269    33.0\n",
      "1270    28.0\n",
      "1271    28.0\n",
      "1272    47.0\n",
      "1273    18.0\n",
      "1274    31.0\n",
      "1275    16.0\n",
      "1276    31.0\n",
      "1278    20.0\n",
      "1279    14.0\n",
      "1280    22.0\n",
      "1281    22.0\n",
      "1285    32.5\n",
      "1287    51.0\n",
      "1288    18.0\n",
      "1289    21.0\n",
      "1294    28.5\n",
      "1295    21.0\n",
      "1296    27.0\n",
      "1298    36.0\n",
      "1299    27.0\n",
      "1301    45.5\n",
      "1304    14.5\n",
      "1306    26.5\n",
      "1307    27.0\n",
      "1308    29.0\n",
      "Name: age, Length: 618, dtype: float64\n",
      "Survived or Not: 1\n",
      "     pclass  survived     sex    age  sibsp  parch      fare embarked\n",
      "0         1         1  female  29.00      0      0  211.3375        S\n",
      "1         1         1    male   0.92      1      2  151.5500        S\n",
      "5         1         1    male  48.00      0      0   26.5500        S\n",
      "6         1         1  female  63.00      1      0   77.9583        S\n",
      "8         1         1  female  53.00      2      0   51.4792        S\n",
      "11        1         1  female  18.00      1      0  227.5250        C\n",
      "12        1         1  female  24.00      0      0   69.3000        C\n",
      "13        1         1  female  26.00      0      0   78.8500        S\n",
      "14        1         1    male  80.00      0      0   30.0000        S\n",
      "17        1         1  female  50.00      0      1  247.5208        C\n",
      "18        1         1  female  32.00      0      0   76.2917        C\n",
      "20        1         1    male  37.00      1      1   52.5542        S\n",
      "21        1         1  female  47.00      1      1   52.5542        S\n",
      "22        1         1    male  26.00      0      0   30.0000        C\n",
      "23        1         1  female  42.00      0      0  227.5250        C\n",
      "24        1         1  female  29.00      0      0  221.7792        S\n",
      "26        1         1    male  25.00      1      0   91.0792        C\n",
      "27        1         1  female  19.00      1      0   91.0792        C\n",
      "28        1         1  female  35.00      0      0  135.6333        S\n",
      "29        1         1    male  28.00      0      0   26.5500        S\n",
      "31        1         1    male  40.00      0      0   31.0000        C\n",
      "32        1         1  female  30.00      0      0  164.8667        S\n",
      "33        1         1  female  58.00      0      0   26.5500        S\n",
      "35        1         1  female  45.00      0      0  262.3750        C\n",
      "36        1         1  female  22.00      0      1   55.0000        S\n",
      "41        1         1  female  44.00      0      0   27.7208        C\n",
      "42        1         1  female  59.00      2      0   51.4792        S\n",
      "43        1         1  female  60.00      0      0   76.2917        C\n",
      "44        1         1  female  41.00      0      0  134.5000        C\n",
      "47        1         1    male  42.00      0      0   26.2875        S\n",
      "...     ...       ...     ...    ...    ...    ...       ...      ...\n",
      "1082      3         1    male   9.00      0      1    3.1708        S\n",
      "1088      3         1    male  32.00      0      0    7.7750        S\n",
      "1094      3         1  female  31.00      0      0    8.6833        S\n",
      "1120      3         1    male  25.00      1      0    7.7750        S\n",
      "1131      3         1    male  32.00      0      0    8.0500        S\n",
      "1182      3         1  female  21.00      0      0    7.6500        S\n",
      "1187      3         1  female   1.00      1      1   16.7000        S\n",
      "1188      3         1  female  24.00      0      2   16.7000        S\n",
      "1189      3         1  female   4.00      1      1   16.7000        S\n",
      "1190      3         1    male  25.00      0      0    9.5000        S\n",
      "1196      3         1    male  29.00      0      0    9.5000        S\n",
      "1205      3         1  female  18.00      0      0    7.4958        S\n",
      "1223      3         1  female  23.00      0      0    7.5500        S\n",
      "1228      3         1    male  31.00      0      0    7.9250        S\n",
      "1232      3         1    male  16.00      0      0    8.0500        S\n",
      "1233      3         1    male  44.00      0      0    7.9250        S\n",
      "1236      3         1    male  14.00      0      0    9.2250        S\n",
      "1238      3         1    male  25.00      0      0    7.7958        S\n",
      "1240      3         1    male   0.42      0      1    8.5167        C\n",
      "1244      3         1  female  16.00      1      1    8.5167        C\n",
      "1254      3         1    male  25.00      0      0    0.0000        S\n",
      "1256      3         1    male   7.00      1      1   15.2458        C\n",
      "1257      3         1  female   9.00      1      1   15.2458        C\n",
      "1258      3         1  female  29.00      0      2   15.2458        C\n",
      "1260      3         1  female  18.00      0      0    9.8417        S\n",
      "1261      3         1  female  63.00      0      0    9.5875        S\n",
      "1277      3         1    male  22.00      0      0    7.2250        C\n",
      "1286      3         1  female  38.00      0      0    7.2292        C\n",
      "1290      3         1  female  47.00      1      0    7.0000        S\n",
      "1300      3         1  female  15.00      1      0   14.4542        C\n",
      "\n",
      "[425 rows x 8 columns]\n",
      "0       29.00\n",
      "1        0.92\n",
      "5       48.00\n",
      "6       63.00\n",
      "8       53.00\n",
      "11      18.00\n",
      "12      24.00\n",
      "13      26.00\n",
      "14      80.00\n",
      "17      50.00\n",
      "18      32.00\n",
      "20      37.00\n",
      "21      47.00\n",
      "22      26.00\n",
      "23      42.00\n",
      "24      29.00\n",
      "26      25.00\n",
      "27      19.00\n",
      "28      35.00\n",
      "29      28.00\n",
      "31      40.00\n",
      "32      30.00\n",
      "33      58.00\n",
      "35      45.00\n",
      "36      22.00\n",
      "41      44.00\n",
      "42      59.00\n",
      "43      60.00\n",
      "44      41.00\n",
      "47      42.00\n",
      "        ...  \n",
      "1082     9.00\n",
      "1088    32.00\n",
      "1094    31.00\n",
      "1120    25.00\n",
      "1131    32.00\n",
      "1182    21.00\n",
      "1187     1.00\n",
      "1188    24.00\n",
      "1189     4.00\n",
      "1190    25.00\n",
      "1196    29.00\n",
      "1205    18.00\n",
      "1223    23.00\n",
      "1228    31.00\n",
      "1232    16.00\n",
      "1233    44.00\n",
      "1236    14.00\n",
      "1238    25.00\n",
      "1240     0.42\n",
      "1244    16.00\n",
      "1254    25.00\n",
      "1256     7.00\n",
      "1257     9.00\n",
      "1258    29.00\n",
      "1260    18.00\n",
      "1261    63.00\n",
      "1277    22.00\n",
      "1286    38.00\n",
      "1290    47.00\n",
      "1300    15.00\n",
      "Name: age, Length: 425, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 5: Titanic Dataset, a)\n",
    "\n",
    "Step 3. Segment the data into survived and non-survived. \n",
    "\n",
    "Categorical data: pclass and embarked.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "grouped = df.groupby([df['survived']])\n",
    "\n",
    "# Print out the groups in a more readable fashion. \n",
    "for name, group in grouped: # df.groupby: similarly to dictionary.item(); returns a toople: key, value. \n",
    "    print(\"Survived or Not: {}\".format(name))\n",
    "    print(group)\n",
    "    print(group['age'])\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival list mean for age:\n",
      "28.819035294117647\n",
      "\n",
      "Survival list mean for sibsp:\n",
      "0.49176470588235294\n",
      "\n",
      "Survival list mean for parch:\n",
      "0.5388235294117647\n",
      "\n",
      "Survival list mean for fare:\n",
      "53.25888329411768\n",
      "\n",
      "Survival list mode for pclass:\n",
      "['1']\n",
      "\n",
      "Survival list mode for sex:\n",
      "['female']\n",
      "\n",
      "Survival list mode for embarked:\n",
      "['S']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 5: Titanic Dataset, a)\n",
    "\n",
    "Step 4. Calculate the means for numeric and modes for categorical. \n",
    "\n",
    "Categorical data: sex and embarked.\n",
    "\n",
    "Noted that the pclass should be a categorical variable by vocabulary definition, however, to simplify the problem, \n",
    "I think it is reasonable to treat it as a numeric, since when pclass increase, the chance for survival might be decrease. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "nonsurvival = grouped.get_group(0)\n",
    "survival = grouped.get_group(1)\n",
    "#survival.head(8)\n",
    "\n",
    "num_col = ['age', 'sibsp', 'parch', 'fare']\n",
    "cat_col = ['pclass', 'sex', 'embarked']\n",
    "\n",
    "for col in num_col:\n",
    "    mean = survival[col].mean()\n",
    "    print(\"Survival list mean for {}:\\n{}\\n\".format(col, mean))\n",
    "    \n",
    "for col in cat_col:\n",
    "    mode = survival[col].mode().values\n",
    "    print(\"Survival list mode for {}:\\n{}\\n\".format(col, mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female list mean for age:\n",
      "28.576658031088083\n",
      "\n",
      "Female list mean for sibsp:\n",
      "0.6191709844559585\n",
      "\n",
      "Female list mean for parch:\n",
      "0.6658031088082902\n",
      "\n",
      "Female list mean for fare:\n",
      "50.15493419689128\n",
      "\n",
      "Female list mode for pclass:\n",
      "['3']\n",
      "\n",
      "Female list mode for survived:\n",
      "[1]\n",
      "\n",
      "Female list mode for embarked:\n",
      "['S']\n",
      "\n",
      "Male list mean for age:\n",
      "30.539695585996952\n",
      "\n",
      "Male list mean for sibsp:\n",
      "0.4368340943683409\n",
      "\n",
      "Male list mean for parch:\n",
      "0.2785388127853881\n",
      "\n",
      "Male list mean for fare:\n",
      "28.641018721461116\n",
      "\n",
      "Male list mode for pclass:\n",
      "['3']\n",
      "\n",
      "Male list mode for survived:\n",
      "[0]\n",
      "\n",
      "Male list mode for embarked:\n",
      "['S']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 5: Titanic Dataset, b)\n",
    "\n",
    "Step 5. Group by sex. \n",
    "\n",
    "num_col = ['age', 'sibsp', 'parch', 'fare']\n",
    "cat_col = ['pclass', 'survived', 'embarked']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "grouped = df.groupby([df['sex']])\n",
    "\n",
    "# Print out the groups in a more readable fashion. \n",
    "\"\"\"for name, group in grouped: # df.groupby: similarly to dictionary.item(); returns a toople: key, value. \n",
    "    print(name)\n",
    "    print(group)\n",
    "    #print(group['age'])\n",
    "\n",
    "print()\"\"\"\n",
    "\n",
    "female = grouped.get_group('female')\n",
    "male = grouped.get_group('male')\n",
    "\n",
    "num_col = ['age', 'sibsp', 'parch', 'fare']\n",
    "cat_col = ['pclass', 'survived', 'embarked']\n",
    "\n",
    "for col in num_col:\n",
    "    mean = female[col].mean()\n",
    "    print(\"Female list mean for {}:\\n{}\\n\".format(col, mean))\n",
    "    \n",
    "for col in cat_col:\n",
    "    mode = female[col].mode().values\n",
    "    print(\"Female list mode for {}:\\n{}\\n\".format(col, mode))\n",
    "    \n",
    "for col in num_col:\n",
    "    mean = male[col].mean()\n",
    "    print(\"Male list mean for {}:\\n{}\\n\".format(col, mean))\n",
    "    \n",
    "for col in cat_col:\n",
    "    mode = male[col].mode().values\n",
    "    print(\"Male list mode for {}:\\n{}\\n\".format(col, mode))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total survival number from Female group: 290\n",
      "Survival rate for females: 0.7512953367875648\n",
      "***************************\n",
      "Total survival number from Male group: 135\n",
      "Survival rate for males: 0.2054794520547945\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 5: Titanic Dataset, b)\n",
    "\n",
    "Step 5. Survival rate for each sex. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "surv_num_fem = list(female['survived']).count(1) \n",
    "print(\"Total survival number from Female group: {}\".format(surv_num_fem))\n",
    "\n",
    "surv_rate_fem = surv_num_fem/len(female['survived'])\n",
    "print(\"Survival rate for females: {}\".format(surv_rate_fem))\n",
    "\n",
    "print(\"***************************\")\n",
    "\n",
    "surv_num_m = list(male['survived']).count(1) \n",
    "print(\"Total survival number from Male group: {}\".format(surv_num_m))\n",
    "\n",
    "surv_rate_m = surv_num_m/len(male['survived'])\n",
    "print(\"Survival rate for males: {}\".format(surv_rate_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Class list mean for age:\n",
      "39.08304964539007\n",
      "\n",
      "First Class list mean for sibsp:\n",
      "0.4787234042553192\n",
      "\n",
      "First Class list mean for parch:\n",
      "0.4148936170212766\n",
      "\n",
      "First Class list mean for fare:\n",
      "92.31609148936174\n",
      "\n",
      "First Class list mode for sex:\n",
      "['male']\n",
      "\n",
      "First Class list mode for survived:\n",
      "[1]\n",
      "\n",
      "First Class list mode for embarked:\n",
      "['S']\n",
      "\n",
      "Second Class list mean for age:\n",
      "29.506704980842912\n",
      "\n",
      "Second Class list mean for sibsp:\n",
      "0.41762452107279696\n",
      "\n",
      "Second Class list mean for parch:\n",
      "0.39080459770114945\n",
      "\n",
      "Second Class list mean for fare:\n",
      "21.85504444444445\n",
      "\n",
      "Second Class list mode for sex:\n",
      "['male']\n",
      "\n",
      "Second Class list mode for survived:\n",
      "[0]\n",
      "\n",
      "Second Class list mode for embarked:\n",
      "['S']\n",
      "\n",
      "Third Class list mean for age:\n",
      "24.745\n",
      "\n",
      "Third Class list mean for sibsp:\n",
      "0.564\n",
      "\n",
      "Third Class list mean for parch:\n",
      "0.442\n",
      "\n",
      "Third Class list mean for fare:\n",
      "12.879299000000007\n",
      "\n",
      "Third Class list mode for sex:\n",
      "['male']\n",
      "\n",
      "Third Class list mode for survived:\n",
      "[0]\n",
      "\n",
      "Third Class list mode for embarked:\n",
      "['S']\n",
      "\n",
      "Total survival number from First Class group: 179\n",
      "Survival rate for First Class passengers: 0.6347517730496454\n",
      "***************************\n",
      "Total survival number from Second Class group: 115\n",
      "Survival rate for Second Class passengers: 0.44061302681992337\n",
      "***************************\n",
      "Total survival number from Third Class group: 131\n",
      "Survival rate for Third Class passengers: 0.262\n",
      "***************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 5: Titanic Dataset, c)\n",
    "\n",
    "Step 6. Survival rate for each passenger class.\n",
    "\n",
    "1). Group by pclass;\n",
    "2). Survival rate.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "grouped = df.groupby([df['pclass']])\n",
    "\n",
    "# Print out the groups in a more readable fashion. \n",
    "\"\"\"for name, group in grouped: # df.groupby: similarly to dictionary.item(); returns a toople: key, value. \n",
    "    print(name)\n",
    "    print(group)\n",
    "    #print(group['age'])\n",
    "\n",
    "print()\n",
    "\"\"\"\n",
    "first = grouped.get_group('1')\n",
    "second = grouped.get_group('2')\n",
    "third = grouped.get_group('3')\n",
    "\n",
    "num_col = ['age', 'sibsp', 'parch', 'fare']\n",
    "cat_col = ['sex', 'survived', 'embarked']\n",
    "\n",
    "for col in num_col:\n",
    "    mean = first[col].mean()\n",
    "    print(\"First Class list mean for {}:\\n{}\\n\".format(col, mean))\n",
    "    \n",
    "for col in cat_col:\n",
    "    mode = first[col].mode().values\n",
    "    print(\"First Class list mode for {}:\\n{}\\n\".format(col, mode))\n",
    "    \n",
    "for col in num_col:\n",
    "    mean = second[col].mean()\n",
    "    print(\"Second Class list mean for {}:\\n{}\\n\".format(col, mean))\n",
    "    \n",
    "for col in cat_col:\n",
    "    mode = second[col].mode().values\n",
    "    print(\"Second Class list mode for {}:\\n{}\\n\".format(col, mode))\n",
    "    \n",
    "for col in num_col:\n",
    "    mean = third[col].mean()\n",
    "    print(\"Third Class list mean for {}:\\n{}\\n\".format(col, mean))\n",
    "    \n",
    "for col in cat_col:\n",
    "    mode = third[col].mode().values\n",
    "    print(\"Third Class list mode for {}:\\n{}\\n\".format(col, mode))\n",
    "\n",
    "surv_num_1 = list(first['survived']).count(1) \n",
    "print(\"Total survival number from First Class group: {}\".format(surv_num_1))\n",
    "\n",
    "surv_rate_1 = surv_num_1/len(first['survived'])\n",
    "print(\"Survival rate for First Class passengers: {}\".format(surv_rate_1))\n",
    "\n",
    "print(\"***************************\")\n",
    "\n",
    "surv_num_2 = list(second['survived']).count(1) \n",
    "print(\"Total survival number from Second Class group: {}\".format(surv_num_2))\n",
    "\n",
    "surv_rate_2 = surv_num_2/len(second['survived'])\n",
    "print(\"Survival rate for Second Class passengers: {}\".format(surv_rate_2))\n",
    "\n",
    "print(\"***************************\")\n",
    "\n",
    "surv_num_3 = list(third['survived']).count(1) \n",
    "print(\"Total survival number from Third Class group: {}\".format(surv_num_3))\n",
    "\n",
    "surv_rate_3 = surv_num_3/len(third['survived'])\n",
    "print(\"Survival rate for Third Class passengers: {}\".format(surv_rate_3))\n",
    "\n",
    "print(\"***************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Question 5: FYI, Recall the CrossValidation_Manually() class\n",
    "\n",
    "\"\"\"\n",
    "class CrossValidation_Manually():\n",
    "    \n",
    "    def __init__(self, model_list): \n",
    "        self.inst_attr = \"Selected Features for Cross Validation.\" \n",
    "        self.model_list = model_list\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return(print(\"Selected features: \\n{}\".format(self.model_list)))\n",
    "\n",
    "    \n",
    "    def K_fold(self):\n",
    "        \n",
    "        self.R2_Manually = []\n",
    "        self.MSE_Manually = []\n",
    "        \n",
    "        for i in range(10):\n",
    "            print(\"****************Fold {} as the test dataset*******************\".format(i))\n",
    "\n",
    "            # Noted that before execute the function, I need the data after the extra datapoints taken out. \n",
    "            # Namely, I have df_new and df_target_new. \n",
    "            n = len(df_target_new)\n",
    "            array = np.arange(n).reshape((n, 1))\n",
    "            #print(array)\n",
    "\n",
    "            split = np.vsplit(array, 10)\n",
    "\n",
    "            # Create linear regression object\n",
    "            regr = linear_model.LinearRegression()\n",
    "        \n",
    "            X_train = np.delete(np.array(df_new), (split[i].tolist()), axis=0)\n",
    "            \n",
    "            X_step_1 = np.array(df_new)[split[i].tolist(),]\n",
    "            X_test = np.squeeze(X_step_1)\n",
    "\n",
    "            y_train = np.delete(np.array(df_target_new), (split[i].tolist()), axis=0)\n",
    "        \n",
    "            y_step_1 = np.array(df_target_new)[split[i].tolist(),]  \n",
    "            y_test = np.squeeze(y_step_1)\n",
    "    \n",
    "            regr.fit(X_train, y_train)\n",
    "    \n",
    "            y_pred = regr.predict(X_test)\n",
    "    \n",
    "            print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "            # The mean squared error\n",
    "            print(\"Mean squared error: %.2f\"\n",
    "                  % mean_squared_error(y_test, y_pred))\n",
    "            self.MSE_Manually.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "            # Explained variance score: a result of 1 denotes a perfect prediction\n",
    "            print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "            self.R2_Manually.append(r2_score(y_test, y_pred))\n",
    "    \n",
    "            print()\n",
    "\n",
    "    def K_fold_logreg(self):\n",
    "        \n",
    "        #self.R2_logreg = []\n",
    "        self.MSE_logreg = []\n",
    "        self.score_logreg = []\n",
    "        self.accuracy_logreg = []\n",
    "        \n",
    "        for i in range(10):\n",
    "            print(\"****************Fold {} as the test dataset*******************\".format(i))\n",
    "\n",
    "            # Noted that before execute the function, I need the data after the extra datapoints taken out. \n",
    "            # Namely, I have df_new and df_target_new. \n",
    "            n = len(df_target_new)\n",
    "            array = np.arange(n).reshape((n, 1))\n",
    "            #print(array)\n",
    "\n",
    "            split = np.vsplit(array, 10)\n",
    "            \n",
    "            # Train and test data:\n",
    "            X_train = np.delete(np.array(df_new), (split[i].tolist()), axis=0)\n",
    "            \n",
    "            X_step_1 = np.array(df_new)[split[i].tolist(),]\n",
    "            X_test = np.squeeze(X_step_1)\n",
    "\n",
    "            y_train = np.delete(np.array(df_target_new), (split[i].tolist()), axis=0)\n",
    "        \n",
    "            y_step_1 = np.array(df_target_new)[split[i].tolist(),]  \n",
    "            y_test = np.squeeze(y_step_1)\n",
    "    \n",
    "            # Initiate logistic regression model:\n",
    "            logreg = linear_model.LogisticRegression(C=1e5)\n",
    "            \n",
    "            logreg.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = logreg.predict(X_test)\n",
    "\n",
    "            #print(\"Prediction: {}\".format(y_pred))\n",
    "\n",
    "            print(\"Logistic Regression Score using logreg.score(): {}\".format(logreg.score(X_test, y_test)))\n",
    "            self.score_logreg.append(logreg.score(X_test, y_test))\n",
    "            print()\n",
    "            \n",
    "            print(\"Logistic Regression Score using metric.accuracy_acore(): {}\".format(accuracy_score(y_test, y_pred)))\n",
    "            self.accuracy_logreg.append(accuracy_score(y_test, y_pred))\n",
    "            print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how many data points to be droped, I need the shape of vectorized data:\n",
      "(1043, 12)\n",
      "The shape of new data:\n",
      "(1040, 12)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 5: Titanic Dataset, d)\n",
    "\n",
    "Step 7: fit a logistic classifier.\n",
    "\n",
    "i. Dict_Vectorization.\n",
    "ii. Drop excessive datapoints in preparation of 10-fold cross validation data splitting process.\n",
    "\n",
    "Target: 'survived'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "col = ['pclass', 'age', 'sibsp', 'parch', 'fare', 'sex', 'embarked']\n",
    "\n",
    "# Create original X and y:\n",
    "Original_X = df[col]\n",
    "Original_y = df['survived']\n",
    "\n",
    "# Dict_Vectorize the X: \n",
    "dict_data = Original_X.T.to_dict().values()\n",
    "\n",
    "# Initiate vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform X and y: y does not need to transform. \n",
    "X = pd.DataFrame(vectorizer.fit_transform(dict_data))\n",
    "y = Original_y\n",
    "\n",
    "# Drop extra datapoints: \n",
    "print(\"To determine how many data points to be droped, I need the shape of vectorized data:\\n{}\".format(X.shape))\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# Set the number of samples to be removed.\n",
    "remove_n = 3\n",
    "\n",
    "# Randomly choose remove_n number of index from dataframe.\n",
    "drop = np.random.choice(df.index, remove_n, replace=False)\n",
    "\n",
    "# Drop rows that are randomly chosen based on indexes, for fearure data.\n",
    "df_new = X.drop(drop)\n",
    "\n",
    "# Drop the same rows in target data.\n",
    "# Write target into panda dataframe for easier furthur process.\n",
    "df_target_new = y.drop(drop)\n",
    "\n",
    "print(\"The shape of new data:\\n{}\".format(df_new.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************Fold 0 as the test dataset*******************\n",
      "Logistic Regression Score with LogisticRegression.scoore(): 0.75\n",
      "\n",
      "Logistic Regression Score with Metrics.accuracy_score(): 0.75\n",
      "\n",
      "****************Fold 1 as the test dataset*******************\n",
      "Logistic Regression Score with LogisticRegression.scoore(): 0.7115384615384616\n",
      "\n",
      "Logistic Regression Score with Metrics.accuracy_score(): 0.7115384615384616\n",
      "\n",
      "****************Fold 2 as the test dataset*******************\n",
      "Logistic Regression Score with LogisticRegression.scoore(): 0.6538461538461539\n",
      "\n",
      "Logistic Regression Score with Metrics.accuracy_score(): 0.6538461538461539\n",
      "\n",
      "****************Fold 3 as the test dataset*******************\n",
      "Logistic Regression Score with LogisticRegression.scoore(): 0.8653846153846154\n",
      "\n",
      "Logistic Regression Score with Metrics.accuracy_score(): 0.8653846153846154\n",
      "\n",
      "****************Fold 4 as the test dataset*******************\n",
      "Logistic Regression Score with LogisticRegression.scoore(): 0.7788461538461539\n",
      "\n",
      "Logistic Regression Score with Metrics.accuracy_score(): 0.7788461538461539\n",
      "\n",
      "****************Fold 5 as the test dataset*******************\n",
      "Logistic Regression Score with LogisticRegression.scoore(): 0.7692307692307693\n",
      "\n",
      "Logistic Regression Score with Metrics.accuracy_score(): 0.7692307692307693\n",
      "\n",
      "****************Fold 6 as the test dataset*******************\n",
      "Logistic Regression Score with LogisticRegression.scoore(): 0.7884615384615384\n",
      "\n",
      "Logistic Regression Score with Metrics.accuracy_score(): 0.7884615384615384\n",
      "\n",
      "****************Fold 7 as the test dataset*******************\n",
      "Logistic Regression Score with LogisticRegression.scoore(): 0.7403846153846154\n",
      "\n",
      "Logistic Regression Score with Metrics.accuracy_score(): 0.7403846153846154\n",
      "\n",
      "****************Fold 8 as the test dataset*******************\n",
      "Logistic Regression Score with LogisticRegression.scoore(): 0.7019230769230769\n",
      "\n",
      "Logistic Regression Score with Metrics.accuracy_score(): 0.7019230769230769\n",
      "\n",
      "****************Fold 9 as the test dataset*******************\n",
      "Logistic Regression Score with LogisticRegression.scoore(): 0.7307692307692307\n",
      "\n",
      "Logistic Regression Score with Metrics.accuracy_score(): 0.7307692307692307\n",
      "\n",
      "Mean Score for Manually generated 10-fold:\n",
      "0.7490384615384615\n",
      "\n",
      "Mean Accuracy Score for Manually generated 10-fold:\n",
      "0.7490384615384615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 5: Titanic Dataset, d)\n",
    "\n",
    "Step 7: fit a logistic classifier.\n",
    "\n",
    "iii. Call class method k_fold_logreg() to perform 10-fold cross validation and calculate the mean for the accuracy, etc..\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "CV = CrossValidation_Manually(col)\n",
    "CV.K_fold_logreg()\n",
    "#print(CV.R2_Manually)\n",
    "\n",
    "print(\"Mean Score for Manually generated 10-fold:\\n{}\".format(np.mean(CV.score_logreg)))\n",
    "print()\n",
    "#print(\"Mean Accuracy Score for Manually generated 10-fold:\\n{}\".format(np.mean(CV.accuracy_logreg)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      "[[ -3.63117890e-02   8.29330122e-01  -5.00333363e-01   1.90917427e-01\n",
      "    6.78715748e-04   5.19198433e-02   1.17073233e+00   1.33049502e-01\n",
      "   -7.83867647e-01   1.53428172e+00  -1.01436754e+00  -3.36398609e-01]]\n",
      "\n",
      "Predicted probabilities: [[ 0.07542176  0.92457824]\n",
      " [ 0.33090604  0.66909396]\n",
      " [ 0.03866     0.96134   ]\n",
      " ..., \n",
      " [ 0.80318954  0.19681046]\n",
      " [ 0.80604376  0.19395624]\n",
      " [ 0.89426825  0.10573175]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 5: Titanic Dataset, e) calculate the AIC and BIC  of this model on all of the data\n",
    "\n",
    "Step 8: i. This is statistical method, meaning it should be applied to every datapoint. \n",
    "           Thus, I need to fit all the data to a logistic model (different from what was done above, which is CV).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "col = ['pclass', 'age', 'sibsp', 'parch', 'fare', 'sex', 'embarked']\n",
    "\n",
    "# Create original X and y:\n",
    "Original_X = df[col]\n",
    "Original_y = df['survived']\n",
    "\n",
    "# Dict_Vectorize the X: \n",
    "dict_data = Original_X.T.to_dict().values()\n",
    "\n",
    "# Initiate vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform X and y: y does not need to transform. \n",
    "X = pd.DataFrame(vectorizer.fit_transform(dict_data))\n",
    "y = Original_y\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "# Fit the model with all samples, in preparation for the cross validation and AIC BIC.\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# Coefficients are needed for AIC and BIC's further calculation:\n",
    "print(\"Coefficients:\\n{}\".format(classifier.coef_))\n",
    "print()\n",
    "print(\"Predicted probabilities: {}\".format(classifier.predict_proba(X)))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Question 5: Titanic Dataset, e) calculate the AIC and BIC  of this model on all of the data\n",
    "\n",
    "Step 8: ii. Greate a class Logit_AIC_BIC() for logistic AIC and BIC.\n",
    "\n",
    "Equations:\n",
    "\n",
    "AIC = 2*k + 2*sum(LN(y(xi|ai))) = 2*k + 2*sum_on_sample(-sum_on_feature(log(1+exp(-ai.xi))))\n",
    "BIC = 2*sum(LN(y(xi|ai)/n)) = 2*sum_on_sample(-sum_on_feature(log(n+n*exp(-ai.xi))))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "k = len(col)+1\n",
    "\n",
    "class Logit_AIC_BIC():\n",
    "    def __init__(self): \n",
    "        self.inst_attr = \"AIC and BIC.\" \n",
    "        self.coef = coef\n",
    "        self.X = X\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return(print(\"Selected features: \\n{}\".format(self.model_list)))\n",
    "    \n",
    "    def aic_bic(self,coef,X):\n",
    "        n = len(coef)\n",
    "        a_dot_X = X.dot(pd.DataFrame(coef).T)\n",
    "\n",
    "        exp = []\n",
    "        for i in range(len(a_dot_X)):\n",
    "            exp.append(np.exp(-a_dot_X[0][i]))\n",
    "    \n",
    "        exp_df = pd.DataFrame(exp)\n",
    "        #print(exp_df)\n",
    "        log = []\n",
    "        for i in range(len(exp_df)):\n",
    "            log.append(np.log(exp_df[0][i]+1))\n",
    "    \n",
    "        log_df = -pd.DataFrame(log)\n",
    "        #print(log_df[0].sum())\n",
    "        total_log = log_df[0].sum()\n",
    "        self.AIC = 2*k + 2*total_log\n",
    "        print(\"AIC: {}\".format(self.AIC))\n",
    "        \n",
    "        log = []\n",
    "        for i in range(len(exp_df)):\n",
    "            log.append(np.log(exp_df[0][i]*n + n))\n",
    "    \n",
    "        log_df = -pd.DataFrame(log)\n",
    "\n",
    "        total_log = log_df[0].sum()\n",
    "        self.BIC = 2*total_log\n",
    "        print(\"BIC: {}\".format(self.BIC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Question 5: Titanic Dataset, F) Choose a better model based on AIC BIC\n",
    "\n",
    "Step9: Define models() for this question.\n",
    "\n",
    "'''\n",
    "def models(size):\n",
    "    # Define other independent variables for me to choose from. \n",
    "    Ind = ['pclass', 'age', 'sibsp', 'parch', 'fare', 'sex', 'embarked']\n",
    "    # Create the combinations for chosen number of variables. \n",
    "    lst_select = list(itertools.combinations(Ind,size))\n",
    "    print(\"Model Selected:\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "    print()\n",
    "    return lst_select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Selected:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Question 5: Titanic Dataset, F) Choose a better model based on AIC BIC\n",
    "\n",
    "Step9: \n",
    "\n",
    "Try size 2, failed; the smallest AIC/BIC is from:\n",
    "**************MODEL: ['pclass', 'age']**************\n",
    "AIC: -2991.574466059034\n",
    "BIC: -3007.574466059034\n",
    "\n",
    "\n",
    "Try size 3, failed; the smallest AIC/BIC is from: \n",
    "**************MODEL: ['pclass', 'age', 'sex']**************\n",
    "AIC: -3294.6710462244564\n",
    "BIC: -3310.6710462244564\n",
    "\n",
    "Try size 4,found:\n",
    "**************MODEL: ['pclass', 'age', 'sibsp', 'sex']**************\n",
    "AIC: -3555.07721364328\n",
    "BIC: -3571.07721364328\n",
    "'''\n",
    "lst_select = models(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      "[[-0.03797597  0.33971659  1.36310605  0.15182785 -0.83528831 -0.24135067]]\n",
      "\n",
      "Predicted probabilities: [[ 0.28060783  0.71939217]\n",
      " [ 0.07973917  0.92026083]\n",
      " [ 0.08280116  0.91719884]\n",
      " ..., \n",
      " [ 0.761697    0.238303  ]\n",
      " [ 0.76512645  0.23487355]\n",
      " [ 0.77849989  0.22150011]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.03833093  0.00497369  1.10785069  0.21585688 -0.70519945 -0.17374974]]\n",
      "\n",
      "Predicted probabilities: [[ 0.15897314  0.84102686]\n",
      " [ 0.09354432  0.90645568]\n",
      " [ 0.09711412  0.90288588]\n",
      " ..., \n",
      " [ 0.74393571  0.25606429]\n",
      " [ 0.74756955  0.25243045]\n",
      " [ 0.76117287  0.23882713]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.03760137  1.42546655  0.12537267 -0.82613073  1.62264472 -0.89793623\n",
      "  -0.30991194]]\n",
      "\n",
      "Predicted probabilities: [[ 0.06402111  0.93597889]\n",
      " [ 0.28746383  0.71253617]\n",
      " [ 0.03268193  0.96731807]\n",
      " ..., \n",
      " [ 0.88035484  0.11964516]\n",
      " [ 0.88232101  0.11767899]\n",
      " [ 0.88990765  0.11009235]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.03764961  0.66473191 -0.032299   -0.00843944  1.18728213  0.21896207\n",
      "  -0.78225072 -0.11003295]]\n",
      "\n",
      "Predicted probabilities: [[ 0.32937846  0.67062154]\n",
      " [ 0.1600084   0.8399916 ]\n",
      " [ 0.16554939  0.83445061]\n",
      " ..., \n",
      " [ 0.62039849  0.37960151]\n",
      " [ 0.62482165  0.37517835]\n",
      " [ 0.77877387  0.22122613]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.03387757  0.00273921  0.21396328  1.12072658  0.15055903 -0.82091707]]\n",
      "\n",
      "Predicted probabilities: [[ 0.23729936  0.76270064]\n",
      " [ 0.0844797   0.9155203 ]\n",
      " [ 0.08735286  0.91264714]\n",
      " ..., \n",
      " [ 0.77704239  0.22295761]\n",
      " [ 0.77996321  0.22003679]\n",
      " [ 0.79107652  0.20892348]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.03310371 -0.02606972  1.33254096  0.10650674 -0.87052058  1.51240614\n",
      "  -0.94387901]]\n",
      "\n",
      "Predicted probabilities: [[ 0.07918542  0.92081458]\n",
      " [ 0.29429924  0.70570076]\n",
      " [ 0.03573873  0.96426127]\n",
      " ..., \n",
      " [ 0.89313018  0.10686982]\n",
      " [ 0.89469978  0.10530022]\n",
      " [ 0.90077631  0.09922369]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.03335126  0.60962729 -0.06104325 -0.09019638  0.25614498  1.09922753\n",
      "   0.19553979 -0.83637966]]\n",
      "\n",
      "Predicted probabilities: [[ 0.3774861   0.6225139 ]\n",
      " [ 0.12465964  0.87534036]\n",
      " [ 0.12864349  0.87135651]\n",
      " ..., \n",
      " [ 0.65750001  0.34249999]\n",
      " [ 0.66124534  0.33875466]\n",
      " [ 0.80773742  0.19226258]]\n",
      "\n",
      "Coefficients:\n",
      "[[ -3.27497682e-02   7.43885192e-04   1.28682473e+00   1.12150527e-01\n",
      "   -8.59712297e-01   1.48910032e+00  -9.49837360e-01]]\n",
      "\n",
      "Predicted probabilities: [[ 0.07428512  0.92571488]\n",
      " [ 0.27710723  0.72289277]\n",
      " [ 0.03349066  0.96650934]\n",
      " ..., \n",
      " [ 0.89404873  0.10595127]\n",
      " [ 0.89558987  0.10441013]\n",
      " [ 0.90151495  0.09848505]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.03484977  0.59522583 -0.08661146 -0.05391292  0.00338428  0.95561295\n",
      "   0.23704341 -0.73795491]]\n",
      "\n",
      "Predicted probabilities: [[ 0.25712137  0.74287863]\n",
      " [ 0.13737946  0.86262054]\n",
      " [ 0.14190095  0.85809905]\n",
      " ..., \n",
      " [ 0.64269834  0.35730166]\n",
      " [ 0.64668971  0.35331029]\n",
      " [ 0.78937437  0.21062563]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.0319261   0.79818649 -0.48063089  0.10136169  1.11240702  0.13190925\n",
      "  -0.82539899  1.45178191 -1.03286463]]\n",
      "\n",
      "Predicted probabilities: [[ 0.10352929  0.89647071]\n",
      " [ 0.36113314  0.63886686]\n",
      " [ 0.04650317  0.95349683]\n",
      " ..., \n",
      " [ 0.81565786  0.18434214]\n",
      " [ 0.81804598  0.18195402]\n",
      " [ 0.90583781  0.09416219]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 0.00377084  0.31778653  0.50753524 -0.04423231 -0.80710574 -0.14013078]]\n",
      "\n",
      "Predicted probabilities: [[ 0.27674778  0.72325222]\n",
      " [ 0.22606858  0.77393142]\n",
      " [ 0.22606858  0.77393142]\n",
      " ..., \n",
      " [ 0.75466776  0.24533224]\n",
      " [ 0.75466776  0.24533224]\n",
      " [ 0.75421368  0.24578632]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 0.10147813  0.86324282 -0.01793729 -0.80078654  1.27366502 -1.22914602\n",
      "  -0.22339077]]\n",
      "\n",
      "Predicted probabilities: [[ 0.10143081  0.89856919]\n",
      " [ 0.58463224  0.41536776]\n",
      " [ 0.10330849  0.89669151]\n",
      " ..., \n",
      " [ 0.87925707  0.12074293]\n",
      " [ 0.87925707  0.12074293]\n",
      " [ 0.87925707  0.12074293]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 0.44172231 -0.30146229 -0.29624793  0.35478099  0.59021839  0.04665663\n",
      "  -0.79286292 -0.1003281 ]]\n",
      "\n",
      "Predicted probabilities: [[ 0.46555899  0.53444101]\n",
      " [ 0.32142813  0.67857187]\n",
      " [ 0.32142813  0.67857187]\n",
      " ..., \n",
      " [ 0.62413309  0.37586691]\n",
      " [ 0.62413309  0.37586691]\n",
      " [ 0.77645028  0.22354972]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 0.00237275  0.74931283  0.00934324 -0.74226046  1.26779687 -1.25140127\n",
      "  -0.21499341]]\n",
      "\n",
      "Predicted probabilities: [[ 0.07344372  0.92655628]\n",
      " [ 0.5844569   0.4155431 ]\n",
      " [ 0.10173429  0.89826571]\n",
      " ..., \n",
      " [ 0.87654627  0.12345373]\n",
      " [ 0.87654627  0.12345373]\n",
      " [ 0.87637928  0.12362072]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 0.40862655 -0.34693754 -0.27422907  0.00455999  0.3632781   0.09660136\n",
      "  -0.67241951 -0.02715345]]\n",
      "\n",
      "Predicted probabilities: [[ 0.30149152  0.69850848]\n",
      " [ 0.36808949  0.63191051]\n",
      " [ 0.36808949  0.63191051]\n",
      " ..., \n",
      " [ 0.60906581  0.39093419]\n",
      " [ 0.60906581  0.39093419]\n",
      " [ 0.75459638  0.24540362]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 0.71438504 -0.70093035 -0.01187469  0.68554198  0.03757766 -0.72153965\n",
      "   1.29160805 -1.29002806 -0.19904862]]\n",
      "\n",
      "Predicted probabilities: [[ 0.12272706  0.87727294]\n",
      " [ 0.69292492  0.30707508]\n",
      " [ 0.1458153   0.8541847 ]\n",
      " ..., \n",
      " [ 0.78509399  0.21490601]\n",
      " [ 0.78509399  0.21490601]\n",
      " [ 0.88307345  0.11692655]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 0.00145324  0.0066831   0.7747247  -0.00530439 -0.79479915  1.22108825\n",
      "  -1.2464671 ]]\n",
      "\n",
      "Predicted probabilities: [[ 0.09299869  0.90700131]\n",
      " [ 0.56550049  0.43449951]\n",
      " [ 0.09938837  0.90061163]\n",
      " ..., \n",
      " [ 0.88656711  0.11343289]\n",
      " [ 0.88656711  0.11343289]\n",
      " [ 0.88647208  0.11352792]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 0.40627335 -0.32526681 -0.31101595  0.00244358  0.28485236  0.4574593\n",
      "   0.06928481 -0.75675352]]\n",
      "\n",
      "Predicted probabilities: [[ 0.39344518  0.60655482]\n",
      " [ 0.29807891  0.70192109]\n",
      " [ 0.29807891  0.70192109]\n",
      " ..., \n",
      " [ 0.63710819  0.36289181]\n",
      " [ 0.63710819  0.36289181]\n",
      " [ 0.78220042  0.21779958]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 0.6858147  -0.66125812 -0.05954484  0.01000707  0.66852443  0.04394557\n",
      "  -0.74745827  1.24076677 -1.27575505]]\n",
      "\n",
      "Predicted probabilities: [[ 0.14006405  0.85993595]\n",
      " [ 0.66412907  0.33587093]\n",
      " [ 0.13767074  0.86232926]\n",
      " ..., \n",
      " [ 0.79776552  0.20223448]\n",
      " [ 0.79776552  0.20223448]\n",
      " [ 0.89261611  0.10738389]]\n",
      "\n",
      "Coefficients:\n",
      "[[  6.79535417e-01  -6.62533489e-01  -5.87877298e-02   4.45218722e-04\n",
      "    6.47524794e-01   4.81429389e-02  -7.37453534e-01   1.23714450e+00\n",
      "   -1.27893030e+00]]\n",
      "\n",
      "Predicted probabilities: [[ 0.13259724  0.86740276]\n",
      " [ 0.6602719   0.3397281 ]\n",
      " [ 0.13568882  0.86431118]\n",
      " ..., \n",
      " [ 0.79825402  0.20174598]\n",
      " [ 0.79825402  0.20174598]\n",
      " [ 0.89220371  0.10779629]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.02082397  0.01417316  0.17798573 -0.29221992]]\n",
      "\n",
      "Predicted probabilities: [[ 0.09831032  0.90168968]\n",
      " [ 0.11740342  0.88259658]\n",
      " [ 0.11975394  0.88024606]\n",
      " ..., \n",
      " [ 0.65129089  0.34870911]\n",
      " [ 0.65365183  0.34634817]\n",
      " [ 0.66095786  0.33904214]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.00738067  0.07106367  1.29567584 -1.165644   -0.2614456 ]]\n",
      "\n",
      "Predicted probabilities: [[ 0.22940602  0.77059398]\n",
      " [ 0.76164523  0.23835477]\n",
      " [ 0.21557737  0.78442263]\n",
      " ..., \n",
      " [ 0.77402853  0.22597147]\n",
      " [ 0.77467335  0.22532665]\n",
      " [ 0.77723955  0.22276045]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.0114978   0.84130459 -0.6173622  -0.26636114  0.31606425 -0.15694232]]\n",
      "\n",
      "Predicted probabilities: [[ 0.65525421  0.34474579]\n",
      " [ 0.46112156  0.53887844]\n",
      " [ 0.46420865  0.53579135]\n",
      " ..., \n",
      " [ 0.37890601  0.62109399]\n",
      " [ 0.38025987  0.61974013]\n",
      " [ 0.65525421  0.34474579]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.01745699  0.01202092  1.24959533 -1.14278193 -0.38217038]]\n",
      "\n",
      "Predicted probabilities: [[ 0.03258967  0.96741033]\n",
      " [ 0.40430216  0.59569784]\n",
      " [ 0.05946311  0.94053689]\n",
      " ..., \n",
      " [ 0.80403827  0.19596173]\n",
      " [ 0.80540989  0.19459011]\n",
      " [ 0.80962207  0.19037793]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.02235108  0.58768925 -0.44970438 -0.16151493  0.01251004 -0.20871131]]\n",
      "\n",
      "Predicted probabilities: [[ 0.14056399  0.85943601]\n",
      " [ 0.18518939  0.81481061]\n",
      " [ 0.18885956  0.81114044]\n",
      " ..., \n",
      " [ 0.48444484  0.51555516]\n",
      " [ 0.48723648  0.51276352]\n",
      " [ 0.67583792  0.32416208]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.01134798  1.12436224 -1.03760607  0.01135164  1.32771798 -1.22961017\n",
      "  -0.24771722]]\n",
      "\n",
      "Predicted probabilities: [[ 0.24822788  0.75177212]\n",
      " [ 0.79871913  0.20128087]\n",
      " [ 0.23744036  0.76255964]\n",
      " ..., \n",
      " [ 0.5763632   0.4236368 ]\n",
      " [ 0.57774801  0.42225199]\n",
      " [ 0.80988176  0.19011824]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.0133728   0.01098134 -0.17420795  1.1750341  -1.20430316]]\n",
      "\n",
      "Predicted probabilities: [[ 0.04399351  0.95600649]\n",
      " [ 0.4825244   0.5174756 ]\n",
      " [ 0.08055534  0.91944466]\n",
      " ..., \n",
      " [ 0.8188553   0.1811447 ]\n",
      " [ 0.81984499  0.18015501]\n",
      " [ 0.82272289  0.17727711]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.01698057  0.56464038 -0.52148972 -0.24317621  0.01047808  0.11306353]]\n",
      "\n",
      "Predicted probabilities: [[ 0.21775921  0.78224079]\n",
      " [ 0.20501242  0.79498758]\n",
      " [ 0.20801753  0.79198247]\n",
      " ..., \n",
      " [ 0.50241649  0.49758351]\n",
      " [ 0.50453896  0.49546104]\n",
      " [ 0.70122191  0.29877809]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.008724    1.07914001 -1.01819708 -0.05073901 -0.03935759  1.25726301\n",
      "  -1.24705909]]\n",
      "\n",
      "Predicted probabilities: [[ 0.27613162  0.72386838]\n",
      " [ 0.7980797   0.2019203 ]\n",
      " [ 0.24590654  0.75409346]\n",
      " ..., \n",
      " [ 0.59601814  0.40398186]\n",
      " [ 0.59706799  0.40293201]\n",
      " [ 0.8235505   0.1764495 ]]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.01287159  0.85888402 -0.89656972 -0.03937732  0.00730221  1.16361325\n",
      "  -1.24067627]]\n",
      "\n",
      "Predicted probabilities: [[ 0.09822286  0.90177714]\n",
      " [ 0.56520357  0.43479643]\n",
      " [ 0.10639739  0.89360261]\n",
      " ..., \n",
      " [ 0.67856693  0.32143307]\n",
      " [ 0.67996905  0.32003095]\n",
      " [ 0.84196044  0.15803956]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      "[[ 0.01059122 -0.03997459  1.1077232  -1.32512097 -0.30191881]]\n",
      "\n",
      "Predicted probabilities: [[ 0.0419409   0.9580591 ]\n",
      " [ 0.57915046  0.42084954]\n",
      " [ 0.10778609  0.89221391]\n",
      " ..., \n",
      " [ 0.81244821  0.18755179]\n",
      " [ 0.81244821  0.18755179]\n",
      " [ 0.81139695  0.18860305]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 0.41837514 -0.59200035 -0.34601922  0.00957311  0.23819939 -0.18153954]]\n",
      "\n",
      "Predicted probabilities: [[ 0.23912285  0.76087715]\n",
      " [ 0.29317754  0.70682246]\n",
      " [ 0.29317754  0.70682246]\n",
      " ..., \n",
      " [ 0.50802519  0.49197481]\n",
      " [ 0.50802519  0.49197481]\n",
      " [ 0.68789045  0.31210955]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 1.0199848  -1.05954995 -0.05844892  0.0567971   1.22346436 -1.32147843\n",
      "  -0.23260251]]\n",
      "\n",
      "Predicted probabilities: [[ 0.25597376  0.74402624]\n",
      " [ 0.83159164  0.16840836]\n",
      " [ 0.27928862  0.72071138]\n",
      " ..., \n",
      " [ 0.59856938  0.40143062]\n",
      " [ 0.59856938  0.40143062]\n",
      " [ 0.81426144  0.18573856]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 0.78039629 -0.96750711 -0.03540237  0.0077411   1.14226162 -1.36477481\n",
      "  -0.29685747]]\n",
      "\n",
      "Predicted probabilities: [[ 0.07444513  0.92555487]\n",
      " [ 0.67839025  0.32160975]\n",
      " [ 0.14670863  0.85329137]\n",
      " ..., \n",
      " [ 0.67938833  0.32061167]\n",
      " [ 0.67938833  0.32061167]\n",
      " [ 0.82660134  0.17339866]]\n",
      "\n",
      "Coefficients:\n",
      "[[ 0.78028193 -0.95237931 -0.07983677  0.00705113 -0.11381293  1.11063085\n",
      "  -1.362565  ]]\n",
      "\n",
      "Predicted probabilities: [[ 0.09372077  0.90627923]\n",
      " [ 0.70127222  0.29872778]\n",
      " [ 0.16522798  0.83477202]\n",
      " ..., \n",
      " [ 0.68638505  0.31361495]\n",
      " [ 0.68638505  0.31361495]\n",
      " [ 0.83737271  0.16262729]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coef_list = []\n",
    "X_list = []\n",
    "model_list = []\n",
    "for i in range(len(lst_select)):\n",
    "    data_col = list(lst_select[i])\n",
    "    Original_X = df[data_col]\n",
    "    y=df['survived']\n",
    "    \n",
    "    model_list.append(data_col)\n",
    "\n",
    "    # Dict_Vectorize the X: \n",
    "    dict_data = pd.DataFrame(Original_X).T.to_dict().values()\n",
    "\n",
    "    # Initiate vectorizer\n",
    "    vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "    # Transform X and y: y does not need to transform. \n",
    "    X = pd.DataFrame(vectorizer.fit_transform(dict_data))\n",
    "\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    # Fit the model with all samples, in preparation for the cross validation and AIC BIC.\n",
    "    classifier.fit(X, y)\n",
    "\n",
    "    # Coefficients are needed for AIC and BIC's further calculation:\n",
    "    print(\"Coefficients:\\n{}\".format(classifier.coef_))\n",
    "    print()\n",
    "    print(\"Predicted probabilities: {}\".format(classifier.predict_proba(X)))\n",
    "    print()\n",
    "    \n",
    "    coef_list.append(classifier.coef_)\n",
    "    X_list.append(X)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************MODEL: ['pclass', 'age', 'sibsp', 'parch']**************\n",
      "AIC: -3045.8485523436466\n",
      "BIC: -3061.8485523436466\n",
      "**************MODEL: ['pclass', 'age', 'sibsp', 'fare']**************\n",
      "AIC: -2936.004610815609\n",
      "BIC: -2952.004610815609\n",
      "**************MODEL: ['pclass', 'age', 'sibsp', 'sex']**************\n",
      "AIC: -3555.07721364328\n",
      "BIC: -3571.07721364328\n",
      "**************MODEL: ['pclass', 'age', 'sibsp', 'embarked']**************\n",
      "AIC: -2962.1990506545653\n",
      "BIC: -2978.1990506545653\n",
      "**************MODEL: ['pclass', 'age', 'parch', 'fare']**************\n",
      "AIC: -2700.6446934500364\n",
      "BIC: -2716.6446934500364\n",
      "**************MODEL: ['pclass', 'age', 'parch', 'sex']**************\n",
      "AIC: -3307.6910244725336\n",
      "BIC: -3323.6910244725336\n",
      "**************MODEL: ['pclass', 'age', 'parch', 'embarked']**************\n",
      "AIC: -2735.79173897531\n",
      "BIC: -2751.79173897531\n",
      "**************MODEL: ['pclass', 'age', 'fare', 'sex']**************\n",
      "AIC: -3265.72627899386\n",
      "BIC: -3281.72627899386\n",
      "**************MODEL: ['pclass', 'age', 'fare', 'embarked']**************\n",
      "AIC: -2713.814588960437\n",
      "BIC: -2729.814588960437\n",
      "**************MODEL: ['pclass', 'age', 'sex', 'embarked']**************\n",
      "AIC: -3128.68871260268\n",
      "BIC: -3144.68871260268\n",
      "**************MODEL: ['pclass', 'sibsp', 'parch', 'fare']**************\n",
      "AIC: -1632.8702338589292\n",
      "BIC: -1648.8702338589292\n",
      "**************MODEL: ['pclass', 'sibsp', 'parch', 'sex']**************\n",
      "AIC: -2548.0561221183684\n",
      "BIC: -2564.0561221183684\n",
      "**************MODEL: ['pclass', 'sibsp', 'parch', 'embarked']**************\n",
      "AIC: -1868.4646694364515\n",
      "BIC: -1884.4646694364515\n",
      "**************MODEL: ['pclass', 'sibsp', 'fare', 'sex']**************\n",
      "AIC: -2507.8588663334767\n",
      "BIC: -2523.8588663334767\n",
      "**************MODEL: ['pclass', 'sibsp', 'fare', 'embarked']**************\n",
      "AIC: -1776.6746231299478\n",
      "BIC: -1792.6746231299478\n",
      "**************MODEL: ['pclass', 'sibsp', 'sex', 'embarked']**************\n",
      "AIC: -2528.734451603663\n",
      "BIC: -2544.734451603663\n",
      "**************MODEL: ['pclass', 'parch', 'fare', 'sex']**************\n",
      "AIC: -2444.189047280243\n",
      "BIC: -2460.189047280243\n",
      "**************MODEL: ['pclass', 'parch', 'fare', 'embarked']**************\n",
      "AIC: -1780.93167267331\n",
      "BIC: -1796.93167267331\n",
      "**************MODEL: ['pclass', 'parch', 'sex', 'embarked']**************\n",
      "AIC: -2468.175283917088\n",
      "BIC: -2484.175283917088\n",
      "**************MODEL: ['pclass', 'fare', 'sex', 'embarked']**************\n",
      "AIC: -2458.713623619796\n",
      "BIC: -2474.713623619796\n",
      "**************MODEL: ['age', 'sibsp', 'parch', 'fare']**************\n",
      "AIC: -1755.5685903243016\n",
      "BIC: -1771.5685903243016\n",
      "**************MODEL: ['age', 'sibsp', 'parch', 'sex']**************\n",
      "AIC: -2460.817884014271\n",
      "BIC: -2476.817884014271\n",
      "**************MODEL: ['age', 'sibsp', 'parch', 'embarked']**************\n",
      "AIC: -1897.6020585985013\n",
      "BIC: -1913.6020585985013\n",
      "**************MODEL: ['age', 'sibsp', 'fare', 'sex']**************\n",
      "AIC: -2509.074261871679\n",
      "BIC: -2525.074261871679\n",
      "**************MODEL: ['age', 'sibsp', 'fare', 'embarked']**************\n",
      "AIC: -1963.7280393923124\n",
      "BIC: -1979.7280393923124\n",
      "**************MODEL: ['age', 'sibsp', 'sex', 'embarked']**************\n",
      "AIC: -2524.9739133637127\n",
      "BIC: -2540.9739133637127\n",
      "**************MODEL: ['age', 'parch', 'fare', 'sex']**************\n",
      "AIC: -2309.3530378985774\n",
      "BIC: -2325.3530378985774\n",
      "**************MODEL: ['age', 'parch', 'fare', 'embarked']**************\n",
      "AIC: -1749.4998139149839\n",
      "BIC: -1765.4998139149839\n",
      "**************MODEL: ['age', 'parch', 'sex', 'embarked']**************\n",
      "AIC: -2391.2227049434664\n",
      "BIC: -2407.2227049434664\n",
      "**************MODEL: ['age', 'fare', 'sex', 'embarked']**************\n",
      "AIC: -2313.1375623601507\n",
      "BIC: -2329.1375623601507\n",
      "**************MODEL: ['sibsp', 'parch', 'fare', 'sex']**************\n",
      "AIC: -2094.9378010941123\n",
      "BIC: -2110.9378010941123\n",
      "**************MODEL: ['sibsp', 'parch', 'fare', 'embarked']**************\n",
      "AIC: -1392.7330794178215\n",
      "BIC: -1408.7330794178215\n",
      "**************MODEL: ['sibsp', 'parch', 'sex', 'embarked']**************\n",
      "AIC: -2271.9758018767075\n",
      "BIC: -2287.9758018767075\n",
      "**************MODEL: ['sibsp', 'fare', 'sex', 'embarked']**************\n",
      "AIC: -2152.5521280572493\n",
      "BIC: -2168.5521280572493\n",
      "**************MODEL: ['parch', 'fare', 'sex', 'embarked']**************\n",
      "AIC: -2092.177909506381\n",
      "BIC: -2108.177909506381\n"
     ]
    }
   ],
   "source": [
    "for model, coef, X in zip(model_list, coef_list, X_list):\n",
    "    print(\"**************MODEL: {}**************\".format(model))\n",
    "    titanic = Logit_AIC_BIC()\n",
    "    titanic.aic_bic(coef, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:\n",
      "0.7813998082454459\n",
      "\n",
      "Intercept:\n",
      "[ 0.7247085]\n",
      "\n",
      "Coefficients:\n",
      "[[-0.03760137  1.42546655  0.12537267 -0.82613073  1.62264472 -0.89793623\n",
      "  -0.30991194]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 5: g). The accuracy of the selected model:\n",
    "\n",
    "['pclass', 'age', 'sibsp', 'sex']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "col = ['pclass', 'age', 'sibsp', 'sex']\n",
    "\n",
    "# Create original X and y:\n",
    "Original_X = df[col]\n",
    "Original_y = df['survived']\n",
    "\n",
    "# Dict_Vectorize the X: \n",
    "dict_data = Original_X.T.to_dict().values()\n",
    "\n",
    "# Initiate vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform X and y: y does not need to transform. \n",
    "X = pd.DataFrame(vectorizer.fit_transform(dict_data))\n",
    "y = Original_y\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "# Fit the model with all samples, in preparation for the cross validation and AIC BIC.\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# Coefficients are needed for AIC and BIC's further calculation:\n",
    "print(\"Score:\\n{}\".format(classifier.score(X,y)))\n",
    "print()\n",
    "print(\"Intercept:\\n{}\".format(classifier.intercept_))\n",
    "print()\n",
    "print(\"Coefficients:\\n{}\".format(classifier.coef_))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**************************************QUESTION 6: DIAGNOSIS CANCER**************************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>radius_3</th>\n",
       "      <th>texture_4</th>\n",
       "      <th>perimeter_5</th>\n",
       "      <th>area_6</th>\n",
       "      <th>smooth_7</th>\n",
       "      <th>compact_8</th>\n",
       "      <th>concavity_9</th>\n",
       "      <th>con_pont_10</th>\n",
       "      <th>...</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>843786</td>\n",
       "      <td>M</td>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>...</td>\n",
       "      <td>15.47</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>844359</td>\n",
       "      <td>M</td>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>84458202</td>\n",
       "      <td>M</td>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>...</td>\n",
       "      <td>17.06</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Diagnosis  radius_3  texture_4  perimeter_5  area_6  smooth_7  \\\n",
       "0    842302         M     17.99      10.38       122.80  1001.0   0.11840   \n",
       "1    842517         M     20.57      17.77       132.90  1326.0   0.08474   \n",
       "2  84300903         M     19.69      21.25       130.00  1203.0   0.10960   \n",
       "3  84348301         M     11.42      20.38        77.58   386.1   0.14250   \n",
       "4  84358402         M     20.29      14.34       135.10  1297.0   0.10030   \n",
       "5    843786         M     12.45      15.70        82.57   477.1   0.12780   \n",
       "6    844359         M     18.25      19.98       119.60  1040.0   0.09463   \n",
       "7  84458202         M     13.71      20.83        90.20   577.9   0.11890   \n",
       "\n",
       "   compact_8  concavity_9  con_pont_10   ...        23     24      25      26  \\\n",
       "0    0.27760      0.30010      0.14710   ...     25.38  17.33  184.60  2019.0   \n",
       "1    0.07864      0.08690      0.07017   ...     24.99  23.41  158.80  1956.0   \n",
       "2    0.15990      0.19740      0.12790   ...     23.57  25.53  152.50  1709.0   \n",
       "3    0.28390      0.24140      0.10520   ...     14.91  26.50   98.87   567.7   \n",
       "4    0.13280      0.19800      0.10430   ...     22.54  16.67  152.20  1575.0   \n",
       "5    0.17000      0.15780      0.08089   ...     15.47  23.75  103.40   741.6   \n",
       "6    0.10900      0.11270      0.07400   ...     22.88  27.66  153.20  1606.0   \n",
       "7    0.16450      0.09366      0.05985   ...     17.06  28.14  110.60   897.0   \n",
       "\n",
       "       27      28      29      30      31       32  \n",
       "0  0.1622  0.6656  0.7119  0.2654  0.4601  0.11890  \n",
       "1  0.1238  0.1866  0.2416  0.1860  0.2750  0.08902  \n",
       "2  0.1444  0.4245  0.4504  0.2430  0.3613  0.08758  \n",
       "3  0.2098  0.8663  0.6869  0.2575  0.6638  0.17300  \n",
       "4  0.1374  0.2050  0.4000  0.1625  0.2364  0.07678  \n",
       "5  0.1791  0.5249  0.5355  0.1741  0.3985  0.12440  \n",
       "6  0.1442  0.2576  0.3784  0.1932  0.3063  0.08368  \n",
       "7  0.1654  0.3682  0.2678  0.1556  0.3196  0.11510  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Question 6.\n",
    "\n",
    "Step 1: Read in the dataset. \n",
    "\n",
    "\n",
    "1) ID number\n",
    "2) Diagnosis (M = malignant, B = benign)\n",
    "3-32)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "\ta) radius (mean of distances from center to points on the perimeter)\n",
    "\tb) texture (standard deviation of gray-scale values)\n",
    "\tc) perimeter\n",
    "\td) area\n",
    "\te) smoothness (local variation in radius lengths)\n",
    "\tf) compactness (perimeter^2 / area - 1.0)\n",
    "\tg) concavity (severity of concave portions of the contour)\n",
    "\th) concave points (number of concave portions of the contour)\n",
    "\ti) symmetry \n",
    "\tj) fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "Several of the papers listed above contain detailed descriptions of\n",
    "how these features are computed. \n",
    "\n",
    "The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "largest values) of these features were computed for each image,\n",
    "resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
    "13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "\"\"\"\n",
    "read=pd.read_csv('wdbc.data', sep=',',\n",
    "                names = [\"ID\", \"Diagnosis\", \n",
    "                         \"radius_3\", \"texture_4\", \"perimeter_5\", \"area_6\", \"smooth_7\", \"compact_8\", \"concavity_9\",\"con_pont_10\", \"sym_11\", \"fract_dim_12\",\n",
    "                        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\",\n",
    "                        \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\"])\n",
    "\n",
    "read.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type for column ID: <class 'numpy.int64'>\n",
      "Type for column Diagnosis: <class 'str'>\n",
      "Type for column radius_3: <class 'numpy.float64'>\n",
      "Type for column texture_4: <class 'numpy.float64'>\n",
      "Type for column perimeter_5: <class 'numpy.float64'>\n",
      "Type for column area_6: <class 'numpy.float64'>\n",
      "Type for column smooth_7: <class 'numpy.float64'>\n",
      "Type for column compact_8: <class 'numpy.float64'>\n",
      "Type for column concavity_9: <class 'numpy.float64'>\n",
      "Type for column con_pont_10: <class 'numpy.float64'>\n",
      "Type for column sym_11: <class 'numpy.float64'>\n",
      "Type for column fract_dim_12: <class 'numpy.float64'>\n",
      "Type for column 13: <class 'numpy.float64'>\n",
      "Type for column 14: <class 'numpy.float64'>\n",
      "Type for column 15: <class 'numpy.float64'>\n",
      "Type for column 16: <class 'numpy.float64'>\n",
      "Type for column 17: <class 'numpy.float64'>\n",
      "Type for column 18: <class 'numpy.float64'>\n",
      "Type for column 19: <class 'numpy.float64'>\n",
      "Type for column 20: <class 'numpy.float64'>\n",
      "Type for column 21: <class 'numpy.float64'>\n",
      "Type for column 22: <class 'numpy.float64'>\n",
      "Type for column 23: <class 'numpy.float64'>\n",
      "Type for column 24: <class 'numpy.float64'>\n",
      "Type for column 25: <class 'numpy.float64'>\n",
      "Type for column 26: <class 'numpy.float64'>\n",
      "Type for column 27: <class 'numpy.float64'>\n",
      "Type for column 28: <class 'numpy.float64'>\n",
      "Type for column 29: <class 'numpy.float64'>\n",
      "Type for column 30: <class 'numpy.float64'>\n",
      "Type for column 31: <class 'numpy.float64'>\n",
      "Type for column 32: <class 'numpy.float64'>\n",
      "ID              False\n",
      "Diagnosis       False\n",
      "radius_3        False\n",
      "texture_4       False\n",
      "perimeter_5     False\n",
      "area_6          False\n",
      "smooth_7        False\n",
      "compact_8       False\n",
      "concavity_9     False\n",
      "con_pont_10     False\n",
      "sym_11          False\n",
      "fract_dim_12    False\n",
      "13              False\n",
      "14              False\n",
      "15              False\n",
      "16              False\n",
      "17              False\n",
      "18              False\n",
      "19              False\n",
      "20              False\n",
      "21              False\n",
      "22              False\n",
      "23              False\n",
      "24              False\n",
      "25              False\n",
      "26              False\n",
      "27              False\n",
      "28              False\n",
      "29              False\n",
      "30              False\n",
      "31              False\n",
      "32              False\n",
      "dtype: bool\n",
      "\n",
      "Level for Diagnosis is: \n",
      "{'B', 'M'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 6. \n",
    "\n",
    "step 2: Inspect the data. \n",
    "\n",
    "i. Type;\n",
    "ii. NaNs.\n",
    "iii. Categorical Levels.\n",
    "\n",
    "Noted that only Diagnosis is a categorical data. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "col = [\"ID\", \"Diagnosis\", \n",
    "                         \"radius_3\", \"texture_4\", \"perimeter_5\", \"area_6\", \"smooth_7\", \"compact_8\", \"concavity_9\",\"con_pont_10\", \"sym_11\", \"fract_dim_12\",\n",
    "                        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\",\n",
    "                        \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\"]\n",
    "# Check type:\n",
    "for column in col:\n",
    "    print(\"Type for column {}: {}\".format(column, type(read[column][0])))\n",
    "    \n",
    "# Check NaNs.\n",
    "print(read.isnull().any()) # No NAN. If there is any, comment out the following. \n",
    "print()\n",
    "\n",
    "# Inspect categorical data:\n",
    "lst = list(read['Diagnosis'])\n",
    "levels = set(lst)\n",
    "print(\"Level for Diagnosis is: \\n{}\".format(levels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level for Diagnosis is: \n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 6. a). Fit a univariate OLS linear regression model to predict whether a given sample is benign or malignant,\n",
    "using the Standard Error of the area as the independent variable.  \n",
    "Specifically, using 0.5 as your cutoff criterion, classify samples in the test set by whether they are predicted to be malignant. \n",
    "\n",
    "Step 3: Convert Diagnosis into binary integers: B-0, M-1.\n",
    "\n",
    "Noted that according to my own definition of columns, Standard error of areas would be column '16'.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for ele in range(len(read['Diagnosis'])):\n",
    "    if read['Diagnosis'][ele] == 'B':\n",
    "        read['Diagnosis'][ele] = 0\n",
    "    else:\n",
    "        read['Diagnosis'][ele] = 1\n",
    "\n",
    "lst = list(read['Diagnosis'])\n",
    "levels = set(lst)\n",
    "print(\"Level for Diagnosis is: \\n{}\".format(levels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 0.00562916]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 6. a). Fit a univariate OLS linear regression model to predict whether a given sample is benign or malignant,\n",
    "using the Standard Error of the area as the independent variable. Specifically, using 0.5 as your cutoff criterion, \n",
    "classify samples in the test set by whether they are predicted to be malignant. \n",
    "\n",
    "Noted that according to my own definition of columns, Standard error of areas would be column '16'.\n",
    "\n",
    "Step 4: Fit an OLS regression model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X = read['16']\n",
    "y = read['Diagnosis']\n",
    "\n",
    "# Initiate regression.\n",
    "OLS = linear_model.LinearRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)\n",
    "\n",
    "OLS.fit(X_train.reshape(-1, 1), y_train)\n",
    "\n",
    "y_pred_OLS = OLS.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "print('Coefficients: \\n', OLS.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for OLS-based Binary Classification, threshold of 0.5:\n",
      "0.22807017543859648\n",
      "\n",
      "R2 for OLS-based Binary Classification, threshold of 0.5:\n",
      "0.03766233766233751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 6. a). Fit a univariate OLS linear regression model to predict whether a given sample is benign or malignant,\n",
    "using the Standard Error of the area as the independent variable.  \n",
    "\n",
    "Specifically, using 0.5 as your cutoff criterion, classify samples in the test set by whether they are predicted to be malignant. \n",
    "\n",
    "Noted that according to my own definition of columns, Standard error of areas would be column '16'.\n",
    "\n",
    "Step 4: Use 0.5 as the cutoff, calculate the accuracy of the model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "y_binary = []\n",
    "for y in y_pred_OLS:\n",
    "    if y < 0.5:\n",
    "        y_binary.append(0)\n",
    "    else:\n",
    "        y_binary.append(1)\n",
    "        \n",
    "#print(y_binary)\n",
    "\n",
    "\n",
    "# The accuracy: I used MSE to measure since this is an OLS model. accuracy_score() cannot handle mix of continuous and binary variables. \n",
    "print(\"MSE for OLS-based Binary Classification, threshold of 0.5:\\n{}\".format(mean_squared_error(y_test, y_binary)))\n",
    "print()\n",
    "print(\"R2 for OLS-based Binary Classification, threshold of 0.5:\\n{}\".format(r2_score(y_test, y_binary)))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode of the Diagnosis: 0    0\n",
      "dtype: object\n",
      "MSE for OLS-based Binary Classification, threshold of 0.3:\n",
      "0.15789473684210525\n",
      "\n",
      "R2 for OLS-based Binary Classification, threshold of 0.3:\n",
      "0.3337662337662337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 6. a). Fit a univariate OLS linear regression model to predict whether a given sample is benign or malignant,\n",
    "using the Standard Error of the area as the independent variable.  \n",
    "\n",
    "Even though the OLS classifier is clearly not a good idea, I still suspect that if I changed the cutoff threshold, \n",
    "the accuracy could be improved since the diagnosis class is biased in itself. \n",
    "\n",
    "\"\"\"\n",
    "print(\"Mode of the Diagnosis: {}\".format((read['Diagnosis']).mode()))\n",
    "# Its biased with a mode of non-cancer.\n",
    "\n",
    "\n",
    "# Set y_binary to store binary classifier, with a cutoff at 0.3, since it is biased towards 0, \n",
    "# which means 0 has more power to pull results towards itself.\n",
    "y_binary = []\n",
    "for y in y_pred_OLS:\n",
    "    if y < 0.3:\n",
    "        y_binary.append(0)\n",
    "    else:\n",
    "        y_binary.append(1)\n",
    "        \n",
    "#print(y_binary)\n",
    "\n",
    "\n",
    "# The accuracy: I used MSE to measure since this is an OLS model. accuracy_score() cannot handle mix of continuous and binary variables. \n",
    "print(\"MSE for OLS-based Binary Classification, threshold of 0.3:\\n{}\".format(mean_squared_error(y_test, y_binary)))\n",
    "print()\n",
    "print(\"R2 for OLS-based Binary Classification, threshold of 0.3:\\n{}\".format(r2_score(y_test, y_binary)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how many data points to be droped, I need the shape of data:\n",
      "(569,)\n",
      "The shape of new data:\n",
      "(560,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 6. a). Using 10-fold  cross validation, what is the accuracy  of this approach?\n",
    "\n",
    "Step 5: Perform 10-fold Cross Validation and Use 0.5 as the cutoff, calculate the accuracy of the model.\n",
    "\n",
    "i. Randomly remove extra datapoints.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X = read['16']\n",
    "y = read['Diagnosis']\n",
    "\n",
    "print(\"To determine how many data points to be droped, I need the shape of data:\\n{}\".format(X.shape))\n",
    "\n",
    "# Remove random 9 data:\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the number of samples to be removed.\n",
    "remove_n = 9\n",
    "\n",
    "# Randomly choose remove_n number of index from dataframe.\n",
    "drop = np.random.choice(read.index, remove_n, replace=False)\n",
    "\n",
    "# Drop rows that are randomly chosen based on indexes, for fearure data.\n",
    "df_new = X.drop(drop)\n",
    "\n",
    "# Drop the same rows in target data.\n",
    "# Write target into panda dataframe for easier furthur process.\n",
    "df_target_new = y.drop(drop)\n",
    "\n",
    "print(\"The shape of new data:\\n{}\".format(df_new.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************Fold 0 as the test dataset*******************\n",
      "MSE for OLS-based Binary Classification:\n",
      "0.5357142857142857\n",
      "\n",
      "\n",
      "****************Fold 1 as the test dataset*******************\n",
      "MSE for OLS-based Binary Classification:\n",
      "0.21428571428571427\n",
      "\n",
      "\n",
      "****************Fold 2 as the test dataset*******************\n",
      "MSE for OLS-based Binary Classification:\n",
      "0.14285714285714285\n",
      "\n",
      "\n",
      "****************Fold 3 as the test dataset*******************\n",
      "MSE for OLS-based Binary Classification:\n",
      "0.3392857142857143\n",
      "\n",
      "\n",
      "****************Fold 4 as the test dataset*******************\n",
      "MSE for OLS-based Binary Classification:\n",
      "0.25\n",
      "\n",
      "\n",
      "****************Fold 5 as the test dataset*******************\n",
      "MSE for OLS-based Binary Classification:\n",
      "0.14285714285714285\n",
      "\n",
      "\n",
      "****************Fold 6 as the test dataset*******************\n",
      "MSE for OLS-based Binary Classification:\n",
      "0.08928571428571429\n",
      "\n",
      "\n",
      "****************Fold 7 as the test dataset*******************\n",
      "MSE for OLS-based Binary Classification:\n",
      "0.17857142857142858\n",
      "\n",
      "\n",
      "****************Fold 8 as the test dataset*******************\n",
      "MSE for OLS-based Binary Classification:\n",
      "0.07142857142857142\n",
      "\n",
      "\n",
      "****************Fold 9 as the test dataset*******************\n",
      "MSE for OLS-based Binary Classification:\n",
      "0.08928571428571429\n",
      "\n",
      "\n",
      "Mean for MES: 0.20535714285714285\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 6. a). Using 10-fold  cross validation, what is the accuracy  of this approach?\n",
    "\n",
    "Step 5: Perform 10-fold Cross Validation and Use 0.5 as the cutoff, calculate the accuracy of the model.\n",
    "\n",
    "ii. 10-fold cross validation with 0.5 as the cutoff. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "n = len(df_target_new)\n",
    "array = np.arange(n).reshape((n, 1))\n",
    "#print(array)\n",
    "\n",
    "split = np.vsplit(array, 10)\n",
    "\n",
    "MSE = []\n",
    "for i in range(10):\n",
    "    print(\"****************Fold {} as the test dataset*******************\".format(i))\n",
    "\n",
    "    X_train = np.delete(np.array(df_new), (split[i].tolist()), axis=0)\n",
    "    X_step_1 = np.array(df_new)[split[i].tolist(),]\n",
    "    \n",
    "    X_test = np.squeeze(X_step_1)\n",
    "\n",
    "    y_train = np.delete(np.array(df_target_new), (split[i].tolist()), axis=0)\n",
    "    y_step_1 = np.array(df_target_new)[split[i].tolist(),]\n",
    "    \n",
    "    y_test = np.squeeze(y_step_1)\n",
    "    \n",
    "    OLS.fit(X_train.reshape(-1, 1), y_train)\n",
    "    \n",
    "    y_pred = OLS.predict(X_test.reshape(-1, 1))\n",
    "    \n",
    "    # Use 0.5 as the cuttoff:\n",
    "    y_binary = []\n",
    "    for y in y_pred:\n",
    "        if y < 0.5:\n",
    "            y_binary.append(0)\n",
    "        else:\n",
    "            y_binary.append(1)\n",
    "        \n",
    "    # The accuracy: I used MSE to measure since this is an OLS model. accuracy_score() cannot handle mix of continuous and binary variables. \n",
    "    print(\"MSE for OLS-based Binary Classification:\\n{}\".format(mean_squared_error(y_test, y_binary)))\n",
    "    print()\n",
    "\n",
    "    # The mean squared error list:\n",
    "    MSE.append(mean_squared_error(y_test, y_binary))\n",
    "    print()\n",
    "\n",
    "print(\"Mean for MES: {}\".format(np.mean(MSE)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8681898066783831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 6: b. How does it compare to the logistic regression model calculated in #1?\n",
    "\n",
    "Step 6: Fit the model with a logistic regression and compare the two. \n",
    "\n",
    "i. Fit the model with a logistic regression. Since I wanted to plot the data to compare, I did not use the 10-fold here,\n",
    "instead, I fit the data with train_test_split() and a test size of 10% of the total samples. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Since I have already convert 'diagnosis' into integers, I need to convert them back, so I reaload the dataset:\n",
    "read=pd.read_csv('wdbc.data', sep=',',\n",
    "                names = [\"ID\", \"Diagnosis\", \n",
    "                         \"radius_3\", \"texture_4\", \"perimeter_5\", \"area_6\", \"smooth_7\", \"compact_8\", \"concavity_9\",\"con_pont_10\", \"sym_11\", \"fract_dim_12\",\n",
    "                        \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\",\n",
    "                        \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\"])\n",
    "\n",
    "\n",
    "for ele in range(len(read['Diagnosis'])):\n",
    "    if read['Diagnosis'][ele] == 'B':\n",
    "        read['Diagnosis'][ele] = '0'\n",
    "    else:\n",
    "        read['Diagnosis'][ele] = '1'\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X_log = read['16']\n",
    "y_log = read['Diagnosis']\n",
    "\n",
    "# Initiate regression.\n",
    "logreg = linear_model.LogisticRegression()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)\n",
    "\n",
    "logreg.fit(X_log.reshape(-1, 1), y_log)\n",
    "\n",
    "y_pred_logreg = logreg.predict(X_log.reshape(-1, 1))\n",
    "\n",
    "X_sample = X_log.reshape(-1,1)\n",
    "\n",
    "print(\"Accuracy score: {}\".format(logreg.score(X_sample, y_log)))\n",
    "#print(logreg.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11412163]\n",
      "-4.2555549842\n"
     ]
    }
   ],
   "source": [
    "print(logreg.coef_[0])\n",
    "print(logreg.intercept_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJIAAAHNCAYAAABFDVGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niCREiSQREREREREREUnI/wABkBDyCBHtpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x177e9564550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Question 6: b. How does it compare to the logistic regression model calculated in #1?\n",
    "\n",
    "Step 6: In order to compare two methods, I plotted as the following: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import pylab\n",
    "\n",
    "\n",
    "# Plot Rating Vs Rank, Number of actors Vs Rank.\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "\n",
    "#X = np.linspace(-5, 10, 569)\n",
    "\n",
    "def model(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "loss = model((X_sample.ravel()) * logreg.coef_[0] + logreg.intercept_[0]).ravel()\n",
    "\n",
    "\"\"\"plt.clf()\n",
    "plt.scatter(X.ravel(), y, color='black', zorder=20)\n",
    "\n",
    "plt.plot(X, loss, color='red', linewidth=3)\"\"\"\n",
    "\n",
    "plt1 = ax1.scatter(X_test.ravel(), y_test, color='black', alpha=0.8, zorder=20)\n",
    "plt2 = ax1.scatter(X_test.ravel(), y_binary, color='red', alpha=0.5, marker = \"x\", zorder=20)\n",
    "ax1.plot(X_test, OLS.coef_ * X_test + OLS.intercept_, linewidth=1)\n",
    "ax1.legend((plt1, plt2),(\"True\", \"OLS Classifier\"), \n",
    "           scatterpoints = 2, loc = \"lower right\", fontsize = 20)\n",
    "ax1.set_title(\"True and Predicted Diagnosis OLS\", fontsize = 20)\n",
    "ax1.set_ylabel(\"Diagnosis Based on OLS Regression\")\n",
    "ax1.set_xlabel('Area')\n",
    "\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "plt4 = ax2.scatter(X_sample.ravel(), y_log, color='black', alpha=0.8, zorder=20)\n",
    "plt3 = ax2.scatter(X_sample.ravel(), y_pred_logreg, color='red', alpha=0.5, marker = \"x\", zorder=20)\n",
    "#ax2.plot(X_sample.ravel(), loss.ravel(), color='red', linewidth=3)\n",
    "ax2.legend((plt4, plt3),(\"True\", \"Logit Classifier\"), \n",
    "           scatterpoints = 2, loc = \"lower right\", fontsize = 20)\n",
    "ax2.set_title(\"True and Predicted Diagnosis Logistic\", fontsize = 20)\n",
    "ax2.set_ylabel(\"Diagnosis Based on Logistic Regression\")\n",
    "ax2.set_xlabel('Area')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n*************************************************QUESTION 7*******************************************************************\\n'"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "*************************************************QUESTION 7*******************************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>760.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2.98</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit    gre   gpa  prestige\n",
       "0      0  380.0  3.61       3.0\n",
       "1      1  660.0  3.67       3.0\n",
       "2      1  800.0  4.00       1.0\n",
       "3      1  640.0  3.19       4.0\n",
       "4      0  520.0  2.93       4.0\n",
       "5      1  760.0  3.00       2.0\n",
       "6      1  560.0  2.98       1.0\n",
       "7      0  400.0  3.08       2.0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Question 7:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('admissions.csv')\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admission success: 0\n",
      "GRE Mean: 573.5793357933579\n",
      "GRE Median: 580.0\n",
      "\n",
      "GPA Mean: 3.345404411764704\n",
      "GPA Median: 3.34\n",
      "\n",
      "************************\n",
      "Admission success: 1\n",
      "GRE Mean: 618.8976377952756\n",
      "GRE Median: 620.0\n",
      "\n",
      "GPA Mean: 3.4892063492063485\n",
      "GPA Median: 3.545\n",
      "\n",
      "************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 7: a. what are the mean and median GRE, GPA of students who were admitted? \n",
    "                a. What about students who were not admitted? \n",
    "                \n",
    "Groupby() method by 'admit'.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Group by admit:\n",
    "grouped = df.groupby([df['admit']])\n",
    "\n",
    "# Print out the groups in a more readable fashion. \n",
    "for name, group in grouped: \n",
    "    print(\"Admission success: {}\".format(name))\n",
    "    print(\"GRE Mean: {}\".format(group['gre'].mean()))\n",
    "    print(\"GRE Median: {}\".format(group['gre'].median()))\n",
    "    print()\n",
    "    print(\"GPA Mean: {}\".format((group['gpa'].mean())))\n",
    "    print(\"GPA Median: {}\".format(group['gpa'].median()))\n",
    "    print()\n",
    "    print(\"************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admission success: 0\n",
      "Prestige Rank: 1.0 - 4.0\n",
      "Pretige Mode: [ 2.]\n",
      "\n",
      "************************\n",
      "Admission success: 1\n",
      "Prestige Rank: 1.0 - 4.0\n",
      "Pretige Mode: [ 2.]\n",
      "\n",
      "************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 7: a. What is the modal academic prestige rank among students who were admitted? \n",
    "                a. What about students who were not admitted?\n",
    "                \n",
    "Groupby() method by 'admit'.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Group by admit:\n",
    "grouped = df.groupby([df['admit']])\n",
    "\n",
    "# Print out the groups in a more readable fashion. \n",
    "for name, group in grouped: \n",
    "    print(\"Admission success: {}\".format(name))\n",
    "    print(\"Prestige Rank: {} - {}\".format(group['prestige'].min(), group['prestige'].max()))\n",
    "    print(\"Pretige Mode: {}\".format(group['prestige'].mode().values))\n",
    "    print()\n",
    "    print(\"************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic Rank: 1.0\n",
      "GRE Mean: 611.8032786885246\n",
      "GRE Median: 600.0\n",
      "\n",
      "GPA Mean: 3.453114754098361\n",
      "GPA Median: 3.53\n",
      "\n",
      "************************\n",
      "Academic Rank: 2.0\n",
      "GRE Mean: 596.6216216216217\n",
      "GRE Median: 600.0\n",
      "\n",
      "GPA Mean: 3.364026845637583\n",
      "GPA Median: 3.38\n",
      "\n",
      "************************\n",
      "Academic Rank: 3.0\n",
      "GRE Mean: 574.8760330578513\n",
      "GRE Median: 580.0\n",
      "\n",
      "GPA Mean: 3.4328925619834694\n",
      "GPA Median: 3.43\n",
      "\n",
      "************************\n",
      "Academic Rank: 4.0\n",
      "GRE Mean: 570.1492537313433\n",
      "GRE Median: 560.0\n",
      "\n",
      "GPA Mean: 3.318358208955224\n",
      "GPA Median: 3.33\n",
      "\n",
      "************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 7: b. What is the mean and median GRE score and GPA for each academic rank? \n",
    "\n",
    "Groupby() prestige.\n",
    "\n",
    "\"\"\"\n",
    "# Group by prestige:\n",
    "grouped = df.groupby([df['prestige']])\n",
    "\n",
    "# Print out the groups in a more readable fashion. \n",
    "for name, group in grouped: \n",
    "    print(\"Academic Rank: {}\".format(name))\n",
    "    print(\"GRE Mean: {}\".format(group['gre'].mean()))\n",
    "    print(\"GRE Median: {}\".format(group['gre'].median()))\n",
    "    print()\n",
    "    print(\"GPA Mean: {}\".format((group['gpa'].mean())))\n",
    "    print(\"GPA Median: {}\".format(group['gpa'].median()))\n",
    "    print()\n",
    "    print(\"************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic Rank: 1.0\n",
      "Admission Rate: 0.5409836065573771\n",
      "************************\n",
      "Academic Rank: 2.0\n",
      "Admission Rate: 0.35333333333333333\n",
      "************************\n",
      "Academic Rank: 3.0\n",
      "Admission Rate: 0.23140495867768596\n",
      "************************\n",
      "Academic Rank: 4.0\n",
      "Admission Rate: 0.1791044776119403\n",
      "************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 7: b. What is the observed probability of admission for each academic rank?\n",
    "\n",
    "There is a shortcut here:\n",
    "Normally I need to count the admitted and divide it by total count since this should be treated as categorical.\n",
    "\n",
    "However, for the original dataset, the admit is stored as numeric, with 1 represents admission and 0 means rejection. \n",
    "Thus, I can simply sum up the column and divide it by the length of the column to calculate the probability.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Group by prestige:\n",
    "grouped = df.groupby([df['prestige']])\n",
    "\n",
    "# Print out the groups in a more readable fashion. \n",
    "for name, group in grouped: \n",
    "    print(\"Academic Rank: {}\".format(name))\n",
    "    \n",
    "    rate = (group['admit'].sum())/len(group['admit'])\n",
    "    print(\"Admission Rate: {}\".format(rate))\n",
    "\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type for column admit: <class 'numpy.int64'>\n",
      "Type for column gre: <class 'numpy.float64'>\n",
      "Type for column gpa: <class 'numpy.float64'>\n",
      "Type for column prestige: <class 'str'>\n",
      "Index that containing special characters:\n",
      "[187, 212]\n",
      "(398, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"data = df.dropna(how='all')\\nprint(data.shape)\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Question 7: c. Fit logistic regression\n",
    "\n",
    "Step 1: i.  Treat 'prestige' as categorical, so I input it as string.\n",
    "        ii. Drop NaNs. The reason I did it in such complicated way is that the usuall df.dropnan() function did not work here.\n",
    "        Thus, I have to obtain the NaN index first and drop the NaNs by index for one column, then reset the index,\n",
    "        and drop the NaNs for another column based on the new NaN index. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "col = ['admit', 'gre', 'gpa', 'prestige']\n",
    "# Convert 'prestige' column into string:\n",
    "df['prestige'] = df['prestige'].astype(str)\n",
    "for column in col:\n",
    "    print(\"Type for column {}: {}\".format(column, type(df[column][0])))\n",
    "    \n",
    "index_list = df['gre'].index[df['gre'].isnull() == True].tolist()\n",
    "print(\"Index that containing special characters:\\n{}\".format(index_list))\n",
    "\n",
    "df=df.drop(np.array(index_list))\n",
    "\n",
    "# Need to reset the index, otherwise the index are gone after the drop. \n",
    "df = df.reset_index(drop=True)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\"\"\"data = df.dropna(how='all')\n",
    "print(data.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index that containing special characters:\n",
      "[234]\n",
      "(397, 4)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 7: c. Fit logistic regression\n",
    "\n",
    "Step 1: ii. Drop NaNs, drop the NaNs for another column based on the new NaN index. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "index_list = df['gpa'].index[df['gpa'].isnull() == True].tolist()\n",
    "print(\"Index that containing special characters:\\n{}\".format(index_list))\n",
    "\n",
    "df=df.drop(np.array(index_list))\n",
    "\n",
    "# Need to reset the index, otherwise the index are gone after the drop. \n",
    "df = df.reset_index(drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      "[[ 0.19596439  0.00172658  0.35943456 -0.31929464 -0.88821548 -1.10459123]]\n",
      "\n",
      "\n",
      "Intercept:\n",
      "[-1.95266679]\n",
      "\n",
      "Accuracy score for all:\n",
      "0.7128463476070529\n",
      "\n",
      "10 fold scores are\n",
      "[ 0.82926829  0.575       0.725       0.725       0.725       0.7\n",
      "  0.71794872  0.64102564  0.74358974  0.66666667]\n",
      "\n",
      "Average Accuracy = 0.70\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 7: c. Fit logistic regression and 10-fold cross validation. \n",
    "\n",
    "Step 2: Fit a logistic regression. Since there is a categorical variable, I need to vectorize the independent.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Data columns\n",
    "data_col = ['gre', 'gpa', 'prestige']\n",
    "\n",
    "Original_X = df[data_col]\n",
    "Original_y = df['admit']\n",
    "\n",
    "# Dict_Vectorize the X: \n",
    "dict_data = Original_X.T.to_dict().values()\n",
    "\n",
    "# Initiate vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform X and y: y does not need to transform. \n",
    "X = pd.DataFrame(vectorizer.fit_transform(dict_data))\n",
    "y = Original_y\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "# Fit the model with all samples, in preparation for the cross validation and AIC BIC.\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# Coefficients are needed for AIC and BIC's further calculation:\n",
    "print(\"Coefficients:\\n{}\".format(classifier.coef_))\n",
    "print()\n",
    "#print(\"Predicted probabilities: {}\".format(classifier.predict_proba(X)))\n",
    "print()\n",
    "print(\"Intercept:\\n{}\".format(classifier.intercept_))\n",
    "print()\n",
    "print(\"Accuracy score for all:\\n{}\".format(classifier.score(X, y)))\n",
    "print()\n",
    "\n",
    "# 10-fold Cross Validation\n",
    "scores = cross_val_score(classifier, X, y, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print()\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC: -230.768054418152\n",
      "BIC: -238.768054418152\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 7: d. AIC and BIC on all data. \n",
    "\n",
    "Use the class Logit_AIC_BIC(), and its method aic_bic(coef, X)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Set up needed input for class:\n",
    "coef = classifier.coef_\n",
    "k = 3+1\n",
    "\n",
    "# Initiate class:\n",
    "admit = Logit_AIC_BIC()\n",
    "\n",
    "# Call method\n",
    "admit.aic_bic(coef, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.573854\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.082</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>       <td>admit</td>            <td>AIC:</td>         <td>467.6399</td> \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2018-02-15 15:29</td>       <td>BIC:</td>         <td>491.5435</td> \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>397</td>        <td>Log-Likelihood:</td>    <td>-227.82</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>5</td>            <td>LL-Null:</td>        <td>-248.08</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>391</td>         <td>LLR p-value:</td>    <td>1.1761e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <td></td>   <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0</th> <td>0.7793</td>   <td>0.3325</td>  <td>2.3438</td>  <td>0.0191</td> <td>0.1276</td>  <td>1.4311</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1</th> <td>0.0022</td>   <td>0.0011</td>  <td>2.0280</td>  <td>0.0426</td> <td>0.0001</td>  <td>0.0044</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th> <td>-3.8769</td>  <td>1.1425</td>  <td>-3.3934</td> <td>0.0007</td> <td>-6.1161</td> <td>-1.6376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th> <td>-4.5570</td>  <td>1.1134</td>  <td>-4.0927</td> <td>0.0000</td> <td>-6.7393</td> <td>-2.3747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th> <td>-5.2155</td>  <td>1.1514</td>  <td>-4.5297</td> <td>0.0000</td> <td>-7.4722</td> <td>-2.9588</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th> <td>-5.4303</td>  <td>1.1399</td>  <td>-4.7639</td> <td>0.0000</td> <td>-7.6644</td> <td>-3.1962</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                         Results: Logit\n",
       "=================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.082     \n",
       "Dependent Variable: admit            AIC:              467.6399  \n",
       "Date:               2018-02-15 15:29 BIC:              491.5435  \n",
       "No. Observations:   397              Log-Likelihood:   -227.82   \n",
       "Df Model:           5                LL-Null:          -248.08   \n",
       "Df Residuals:       391              LLR p-value:      1.1761e-07\n",
       "Converged:          1.0000           Scale:            1.0000    \n",
       "No. Iterations:     6.0000                                       \n",
       "--------------------------------------------------------------------\n",
       "       Coef.     Std.Err.       z       P>|z|      [0.025     0.975]\n",
       "--------------------------------------------------------------------\n",
       "0      0.7793      0.3325     2.3438    0.0191     0.1276     1.4311\n",
       "1      0.0022      0.0011     2.0280    0.0426     0.0001     0.0044\n",
       "2     -3.8769      1.1425    -3.3934    0.0007    -6.1161    -1.6376\n",
       "3     -4.5570      1.1134    -4.0927    0.0000    -6.7393    -2.3747\n",
       "4     -5.2155      1.1514    -4.5297    0.0000    -7.4722    -2.9588\n",
       "5     -5.4303      1.1399    -4.7639    0.0000    -7.6644    -3.1962\n",
       "=================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Question 7: d. AIC and BIC on all data - ANOTHER WAY\n",
    "\n",
    "I found another way to do the logistic regression with statsmodels.\n",
    "\n",
    "However, the results are completely different, from the coefficients to AIC and BIC.\n",
    "\n",
    "I did some research, and found that if I set the \"Inverse of regularization strength\", aka C, in sklearn LogisticRegression() to \n",
    "a larger number, which indicates less regularization strength, the coefficients obtained from sklearn will be closer to that from \n",
    "statsmodels. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import LogitResults\n",
    "\n",
    "logr = sm.Logit(pd.DataFrame(y), X)\n",
    "result = logr.fit()\n",
    "result.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ('gre', 'gpa'):\n",
      "\n",
      "\n",
      "Acuracy scores: 0.6826196473551638\n",
      "\n",
      "AIC: -191.83355574860718\n",
      "BIC: -197.83355574860718\n",
      "Coefficients: [[ 0.01773589  0.00208469]]\n",
      "\n",
      "Intercept:[-2.0331085]\n",
      "*********************************\n",
      "\n",
      "Model ('gre', 'prestige'):\n",
      "\n",
      "\n",
      "Acuracy scores: 0.7027707808564232\n",
      "\n",
      "AIC: -311.1832324261594\n",
      "BIC: -317.1832324261594\n",
      "Coefficients: [[ 0.00212494  0.44936252 -0.24094497 -0.78062754 -1.04065305]]\n",
      "\n",
      "Intercept:[-1.61286304]\n",
      "*********************************\n",
      "\n",
      "Model ('gpa', 'prestige'):\n",
      "\n",
      "\n",
      "Acuracy scores: 0.7002518891687658\n",
      "\n",
      "AIC: -269.50203683452713\n",
      "BIC: -275.50203683452713\n",
      "Coefficients: [[ 0.42745908  0.42786986 -0.25195679 -0.86794489 -1.07512822]]\n",
      "\n",
      "Intercept:[-1.76716004]\n",
      "*********************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 7: d. Use AIC and BIC to find the best fit for the data.  \n",
    "\n",
    "Try evry combination of independent variables. \n",
    "\n",
    "Similar to Question 3, f. find all combination. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# set the size at 2:\n",
    "size = 2\n",
    "Ind = [\"gre\", \"gpa\", \"prestige\"]\n",
    "# Create the combinations for chosen number of variables. \n",
    "lst_select = list(itertools.combinations(Ind,size))\n",
    "\n",
    "#print(lst_select)\n",
    "\n",
    "for lst in lst_select:\n",
    "    data_col = list(lst)\n",
    "    Original_X = df[data_col]\n",
    "    Original_y = df['admit']\n",
    "\n",
    "    # Dict_Vectorize the X: \n",
    "    dict_data = Original_X.T.to_dict().values()\n",
    "\n",
    "    # Initiate vectorizer\n",
    "    vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "    # Transform X and y: y does not need to transform. \n",
    "    X = pd.DataFrame(vectorizer.fit_transform(dict_data))\n",
    "    y = Original_y\n",
    "\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    # Fit the model with all samples, in preparation for the cross validation and AIC BIC.\n",
    "    classifier.fit(X, y)\n",
    "    \n",
    "    # Print the accuracy scores:\n",
    "    print(\"Model {}:\\n\".format(lst))\n",
    "    print()\n",
    "    print(\"Acuracy scores: {}\".format(classifier.score(X,y)))\n",
    "    print()\n",
    "    \n",
    "    coef = classifier.coef_\n",
    "    k = len(lst)+1\n",
    "\n",
    "    # Initiate class:\n",
    "    admit = Logit_AIC_BIC()\n",
    "\n",
    "    # Call method\n",
    "    \n",
    "    admit.aic_bic(coef, X)\n",
    "    print(\"Coefficients: {}\".format(coef))\n",
    "    print()\n",
    "    print(\"Intercept:{}\".format(classifier.intercept_))\n",
    "    print(\"*********************************\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Question 7: d. Use AIC and BIC to find the best fit for the data.  \n",
    "\n",
    "According to above results, I chose gre and prestige as the dependent variables.   \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "***********************************QUESTION 8: FLAG AND RELIGION********************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>648</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>black</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>red</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2388</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>green</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American-Samoa</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>blue</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>blue</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Angola</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1247</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>red</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Anguilla</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Antigua-Barbuda</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>black</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1   2   3     4   5   6   7   8   9   10  ...    21  22  23  \\\n",
       "0      Afghanistan   5   1   648  16  10   2   0   3   5  ...     0   0   1   \n",
       "1          Albania   3   1    29   3   6   6   0   0   3  ...     0   0   1   \n",
       "2          Algeria   4   1  2388  20   8   2   2   0   3  ...     0   0   1   \n",
       "3   American-Samoa   6   3     0   0   1   1   0   0   5  ...     0   0   0   \n",
       "4          Andorra   3   1     0   0   6   0   3   0   3  ...     0   0   0   \n",
       "5           Angola   4   2  1247   7  10   5   0   2   3  ...     0   0   1   \n",
       "6         Anguilla   1   4     0   0   1   1   0   1   3  ...     0   0   0   \n",
       "7  Antigua-Barbuda   1   4     0   0   1   1   0   1   5  ...     0   0   1   \n",
       "\n",
       "   24  25  26  27 28     29     30  \n",
       "0   0   0   1   0  0  black  green  \n",
       "1   0   0   0   1  0    red    red  \n",
       "2   1   0   0   0  0  green  white  \n",
       "3   0   1   1   1  0   blue    red  \n",
       "4   0   0   0   0  0   blue    red  \n",
       "5   0   0   1   0  0    red  black  \n",
       "6   0   0   0   1  0  white   blue  \n",
       "7   0   1   0   0  0  black    red  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Question 8: Step 1. Reading data\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/machine-learning-databases/flags/\n",
    "\n",
    "Noted: for convenience, I did not input the full name for each column. They are simply represented by numbers in order:\n",
    "\n",
    "1. name:\tName of the country concerned \n",
    "2. landmass:\t1=N.America, 2=S.America, 3=Europe, 4=Africa, 4=Asia, 6=Oceania \n",
    "3. zone:\tGeographic quadrant, based on Greenwich and the Equator; 1=NE, 2=SE, 3=SW, 4=NW \n",
    "4. area:\tin thousands of square km \n",
    "5. population:\tin round millions \n",
    "6. language: 1=English, 2=Spanish, 3=French, 4=German, 5=Slavic, 6=Other Indo-European, 7=Chinese, 8=Arabic, 9=Japanese/Turkish/Finnish/Magyar, 10=Others \n",
    "7. religion: 0=Catholic, 1=Other Christian, 2=Muslim, 3=Buddhist, 4=Hindu, 5=Ethnic, 6=Marxist, 7=Others \n",
    "8. bars: Number of vertical bars in the flag \n",
    "9. stripes: Number of horizontal stripes in the flag \n",
    "10. colours: Number of different colours in the flag \n",
    "11. red: 0 if red absent, 1 if red present in the flag \n",
    "12. green: same for green \n",
    "13. blue: same for blue \n",
    "14. gold: same for gold (also yellow) \n",
    "15. white: same for white \n",
    "16. black: same for black \n",
    "17. orange: same for orange (also brown) \n",
    "18. mainhue: predominant colour in the flag (tie-breaks decided by taking the topmost hue, if that fails then the most central hue, and if that fails the leftmost hue) \n",
    "19. circles: Number of circles in the flag \n",
    "20. crosses: Number of (upright) crosses \n",
    "21. saltires: Number of diagonal crosses \n",
    "22. quarters: Number of quartered sections \n",
    "23. sunstars: Number of sun or star symbols \n",
    "24. crescent: 1 if a crescent moon symbol present, else 0 \n",
    "25. triangle: 1 if any triangles present, 0 otherwise \n",
    "26. icon: 1 if an inanimate image present (e.g., a boat), otherwise 0 \n",
    "27. animate: 1 if an animate image (e.g., an eagle, a tree, a human hand) present, 0 otherwise \n",
    "28. text: 1 if any letters or writing on the flag (e.g., a motto or slogan), 0 otherwise \n",
    "29. topleft: colour in the top-left corner (moving right to decide tie-breaks) \n",
    "30. botright: Colour in the bottom-left corner (moving left to decide tie-breaks)\n",
    "\n",
    "\"\"\"\n",
    "# Create the column names. \n",
    "col = list(range(1,31))\n",
    "\n",
    "# Read data file.\n",
    "df = pd.read_csv('flag.data', names = col)\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape:(194, 30)\n",
      "(194, 29)\n",
      "24    False\n",
      "28    False\n",
      "20    False\n",
      "21    False\n",
      "23    False\n",
      "8     False\n",
      "9     False\n",
      "dtype: bool\n",
      "\n",
      "Type for column 24: <class 'numpy.int64'>\n",
      "Type for column 28: <class 'numpy.int64'>\n",
      "Type for column 20: <class 'numpy.int64'>\n",
      "Type for column 21: <class 'numpy.int64'>\n",
      "Type for column 23: <class 'numpy.int64'>\n",
      "Type for column 8: <class 'numpy.int64'>\n",
      "Type for column 9: <class 'numpy.int64'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 8: Step 2: Clean data\n",
    "\n",
    "i. Drop name.\n",
    "ii. Check the define to determine feature selection, otherwise, its going to be overfitted.\n",
    "iii. Check the type of data and NaNs.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Drop the name column:\n",
    "print(\"Original Shape:{}\".format(df.shape))\n",
    "df.drop([1], axis=1, inplace=True)\n",
    "\n",
    "\"\"\" ii. Feature Selection by definition: Since the purpose of this question is to classify the religion based on flags, I took out irrelavant features.\n",
    "\n",
    "Categorical:\n",
    "2. landmass:\t1=N.America, 2=S.America, 3=Europe, 4=Africa, 4=Asia, 6=Oceania \n",
    "24. crescent: 1 if a crescent moon symbol present, else 0 \n",
    "28. text: 1 if any letters or writing on the flag (e.g., a motto or slogan), 0 otherwise \n",
    "\n",
    "\n",
    "Numerical:\n",
    "20. crosses: Number of (upright) crosses \n",
    "21. saltires: Number of diagonal crosses \n",
    "23. sunstars: Number of sun or star symbols \n",
    "8. bars: Number of vertical bars in the flag \n",
    "9. stripes: Number of horizontal stripes in the flag \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Column names for independent variable: \n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "data_col = [24,28,20,21,23,8,9]\n",
    "\n",
    "data = df[data_col]\n",
    "religion = df[7]\n",
    "\n",
    "# Clean data: check first if there is any NANs.\n",
    "print(data.isnull().any()) # No NAN. If there is any, comment out the following. \n",
    "#df = data.dropna(how='all')\n",
    "print()\n",
    "\n",
    "#data[2] = data[2].astype(str)\n",
    "\n",
    "# Check each column's type: noted that horsepower is str. \n",
    "for column in data_col:\n",
    "    print(\"Type for column {}: {}\".format(column, type(data[column][0])))\n",
    "    \n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24, 28), (24, 20), (24, 21), (24, 23), (24, 8), (24, 9), (28, 20), (28, 21), (28, 23), (28, 8), (28, 9), (20, 21), (20, 23), (20, 8), (20, 9), (21, 23), (21, 8), (21, 9), (23, 8), (23, 9), (8, 9)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 8: Step 4: Binary logistic regressions.\n",
    "\n",
    "i. combinations of pairs of binary results.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# set the size at 2:\n",
    "size = 2\n",
    "Ind = [24,28,20,21,23,8,9]\n",
    "# Create the combinations for chosen number of variables. \n",
    "lst_select = list(itertools.combinations(Ind,size))\n",
    "\n",
    "print(lst_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model (24, 28):\n",
      "\n",
      "Acuracy scores: 0.35051546391752575\n",
      "10 fold scores are\n",
      "[ 0.31818182  0.31818182  0.27272727  0.27272727  0.45        0.31578947\n",
      "  0.38888889  0.35294118  0.375       0.5       ]\n",
      "\n",
      "Average Accuracy = 0.36\n",
      "*********************************\n",
      "Model (24, 20):\n",
      "\n",
      "Acuracy scores: 0.35051546391752575\n",
      "10 fold scores are\n",
      "[ 0.31818182  0.31818182  0.27272727  0.27272727  0.45        0.31578947\n",
      "  0.38888889  0.35294118  0.375       0.5       ]\n",
      "\n",
      "Average Accuracy = 0.36\n",
      "*********************************\n",
      "Model (24, 21):\n",
      "\n",
      "Acuracy scores: 0.35051546391752575\n",
      "10 fold scores are\n",
      "[ 0.31818182  0.31818182  0.27272727  0.27272727  0.45        0.31578947\n",
      "  0.38888889  0.35294118  0.375       0.5       ]\n",
      "\n",
      "Average Accuracy = 0.36\n",
      "*********************************\n",
      "Model (24, 23):\n",
      "\n",
      "Acuracy scores: 0.35051546391752575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold scores are\n",
      "[ 0.31818182  0.31818182  0.27272727  0.27272727  0.45        0.31578947\n",
      "  0.38888889  0.35294118  0.375       0.5       ]\n",
      "\n",
      "Average Accuracy = 0.36\n",
      "*********************************\n",
      "Model (24, 8):\n",
      "\n",
      "Acuracy scores: 0.3917525773195876\n",
      "10 fold scores are\n",
      "[ 0.31818182  0.27272727  0.27272727  0.36363636  0.55        0.42105263\n",
      "  0.5         0.35294118  0.375       0.5625    ]\n",
      "\n",
      "Average Accuracy = 0.40\n",
      "*********************************\n",
      "Model (24, 9):\n",
      "\n",
      "Acuracy scores: 0.34536082474226804\n",
      "10 fold scores are\n",
      "[ 0.31818182  0.31818182  0.31818182  0.27272727  0.45        0.31578947\n",
      "  0.38888889  0.29411765  0.375       0.625     ]\n",
      "\n",
      "Average Accuracy = 0.37\n",
      "*********************************\n",
      "Model (28, 20):\n",
      "\n",
      "Acuracy scores: 0.32989690721649484\n",
      "10 fold scores are\n",
      "[ 0.22727273  0.27272727  0.27272727  0.27272727  0.25        0.26315789\n",
      "  0.33333333  0.35294118  0.375       0.3125    ]\n",
      "\n",
      "Average Accuracy = 0.29\n",
      "*********************************\n",
      "Model (28, 21):\n",
      "\n",
      "Acuracy scores: 0.30927835051546393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold scores are\n",
      "[ 0.27272727  0.27272727  0.27272727  0.27272727  0.25        0.31578947\n",
      "  0.33333333  0.35294118  0.375       0.3125    ]\n",
      "\n",
      "Average Accuracy = 0.30\n",
      "*********************************\n",
      "Model (28, 23):\n",
      "\n",
      "Acuracy scores: 0.30927835051546393\n",
      "10 fold scores are\n",
      "[ 0.27272727  0.27272727  0.27272727  0.27272727  0.3         0.31578947\n",
      "  0.33333333  0.35294118  0.375       0.375     ]\n",
      "\n",
      "Average Accuracy = 0.31\n",
      "*********************************\n",
      "Model (28, 8):\n",
      "\n",
      "Acuracy scores: 0.35051546391752575\n",
      "10 fold scores are\n",
      "[ 0.27272727  0.27272727  0.27272727  0.36363636  0.4         0.42105263\n",
      "  0.44444444  0.35294118  0.375       0.4375    ]\n",
      "\n",
      "Average Accuracy = 0.36\n",
      "*********************************\n",
      "Model (28, 9):\n",
      "\n",
      "Acuracy scores: 0.30412371134020616\n",
      "10 fold scores are\n",
      "[ 0.27272727  0.27272727  0.31818182  0.27272727  0.3         0.26315789\n",
      "  0.33333333  0.23529412  0.375       0.5       ]\n",
      "\n",
      "Average Accuracy = 0.31\n",
      "*********************************\n",
      "Model (20, 21):\n",
      "\n",
      "Acuracy scores: 0.3247422680412371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold scores are\n",
      "[ 0.22727273  0.27272727  0.27272727  0.27272727  0.3         0.26315789\n",
      "  0.33333333  0.35294118  0.375       0.375     ]\n",
      "\n",
      "Average Accuracy = 0.30\n",
      "*********************************\n",
      "Model (20, 23):\n",
      "\n",
      "Acuracy scores: 0.29381443298969073\n",
      "10 fold scores are\n",
      "[ 0.18181818  0.27272727  0.31818182  0.31818182  0.25        0.21052632\n",
      "  0.38888889  0.29411765  0.3125      0.25      ]\n",
      "\n",
      "Average Accuracy = 0.28\n",
      "*********************************\n",
      "Model (20, 8):\n",
      "\n",
      "Acuracy scores: 0.34536082474226804\n",
      "10 fold scores are\n",
      "[ 0.27272727  0.27272727  0.27272727  0.36363636  0.4         0.36842105\n",
      "  0.38888889  0.35294118  0.375       0.4375    ]\n",
      "\n",
      "Average Accuracy = 0.35\n",
      "*********************************\n",
      "Model (20, 9):\n",
      "\n",
      "Acuracy scores: 0.37628865979381443\n",
      "10 fold scores are\n",
      "[ 0.36363636  0.31818182  0.36363636  0.31818182  0.4         0.26315789\n",
      "  0.44444444  0.35294118  0.4375      0.5       ]\n",
      "\n",
      "Average Accuracy = 0.38\n",
      "*********************************\n",
      "Model (21, 23):\n",
      "\n",
      "Acuracy scores: 0.30927835051546393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold scores are\n",
      "[ 0.27272727  0.27272727  0.27272727  0.27272727  0.3         0.31578947\n",
      "  0.33333333  0.35294118  0.375       0.3125    ]\n",
      "\n",
      "Average Accuracy = 0.31\n",
      "*********************************\n",
      "Model (21, 8):\n",
      "\n",
      "Acuracy scores: 0.35051546391752575\n",
      "10 fold scores are\n",
      "[ 0.27272727  0.27272727  0.27272727  0.36363636  0.4         0.36842105\n",
      "  0.44444444  0.35294118  0.375       0.4375    ]\n",
      "\n",
      "Average Accuracy = 0.36\n",
      "*********************************\n",
      "Model (21, 9):\n",
      "\n",
      "Acuracy scores: 0.3556701030927835\n",
      "10 fold scores are\n",
      "[ 0.36363636  0.27272727  0.31818182  0.27272727  0.4         0.26315789\n",
      "  0.33333333  0.35294118  0.4375      0.5       ]\n",
      "\n",
      "Average Accuracy = 0.35\n",
      "*********************************\n",
      "Model (23, 8):\n",
      "\n",
      "Acuracy scores: 0.35051546391752575\n",
      "10 fold scores are\n",
      "[ 0.27272727  0.22727273  0.27272727  0.36363636  0.4         0.42105263\n",
      "  0.44444444  0.35294118  0.375       0.4375    ]\n",
      "\n",
      "Average Accuracy = 0.36\n",
      "*********************************\n",
      "Model (23, 9):\n",
      "\n",
      "Acuracy scores: 0.34536082474226804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold scores are\n",
      "[ 0.36363636  0.27272727  0.27272727  0.27272727  0.35        0.26315789\n",
      "  0.38888889  0.29411765  0.4375      0.4375    ]\n",
      "\n",
      "Average Accuracy = 0.34\n",
      "*********************************\n",
      "Model (8, 9):\n",
      "\n",
      "Acuracy scores: 0.35051546391752575\n",
      "10 fold scores are\n",
      "[ 0.27272727  0.27272727  0.31818182  0.31818182  0.4         0.36842105\n",
      "  0.44444444  0.29411765  0.375       0.4375    ]\n",
      "\n",
      "Average Accuracy = 0.35\n",
      "*********************************\n",
      "For average accuracy on all data for all combinations:\n",
      "0.34020618556701027\n",
      "For average accuracy from 10-fold CV for all combinations:\n",
      "0.34032757919754825\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 8: Step 4: Binary logistic regressions.\n",
    "\n",
    "ii. Iterate through each pair of columns for logistic regression.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "kfold_accuracy_logregr = []\n",
    "all_accuracy_logregr = []\n",
    "for lst in lst_select:\n",
    "    data_col = list(lst)\n",
    "    Original_X = data[data_col]\n",
    "    y=religion\n",
    "\n",
    "    # Dict_Vectorize the X: \n",
    "    dict_data = pd.DataFrame(Original_X).T.to_dict().values()\n",
    "\n",
    "    # Initiate vectorizer\n",
    "    vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "    # Transform X and y: y does not need to transform. \n",
    "    X = pd.DataFrame(vectorizer.fit_transform(dict_data))\n",
    "\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    # Fit the model with all samples, in preparation for the cross validation and AIC BIC.\n",
    "    classifier.fit(X, y)\n",
    "    \n",
    "    # Print the accuracy scores:\n",
    "    print(\"Model {}:\\n\".format(lst))\n",
    "    print(\"Acuracy scores: {}\".format(classifier.score(X,y)))\n",
    "    all_accuracy_logregr.append(classifier.score(X,y))\n",
    "    \n",
    "    scores = cross_val_score(classifier, X, y, cv=10)\n",
    "    print(\"10 fold scores are\")\n",
    "    print(scores)\n",
    "    print()\n",
    "    print(\"Average Accuracy = %.2f\" % np.mean(scores))\n",
    "    kfold_accuracy_logregr.append(np.mean(scores))\n",
    "    \n",
    "    print(\"*********************************\")\n",
    "    \n",
    "print(\"For average accuracy on all data for all combinations:\\n{}\".format(np.mean((all_accuracy_logregr))))\n",
    "print(\"For average accuracy from 10-fold CV for all combinations:\\n{}\".format(np.mean(kfold_accuracy_logregr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.4536082474226804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 8: Step 9: Multinomial logistic regression.\n",
    "\n",
    "Multinomial logistic. Set parameter \"multi_class\" to \"multinomial\". I also need to set the solver to lbfgs.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data_col = [24,28,20,21,23,8,9]\n",
    "\n",
    "data = df[data_col]\n",
    "religion = df[7]\n",
    "\n",
    "# Dict_Vectorize the X: \n",
    "dict_data = data.T.to_dict().values()\n",
    "\n",
    "# Initiate vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform X and y: y does not need to transform. \n",
    "X = pd.DataFrame(vectorizer.fit_transform(dict_data))\n",
    "y = religion\n",
    "\n",
    "classifier = linear_model.LogisticRegression(multi_class = \"multinomial\", \n",
    "                                            solver = 'lbfgs')\n",
    "# Fit the model with all samples, in preparation for the cross validation and AIC BIC.\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# Accuracy score for multinomial:\n",
    "print(\"Accuracy score: {}\".format(classifier.score(X,y)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "******************************************QUESTION 9: FINAL PROJECT***********************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371528, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dateCrawled</th>\n",
       "      <th>name</th>\n",
       "      <th>seller</th>\n",
       "      <th>offerType</th>\n",
       "      <th>price</th>\n",
       "      <th>abtest</th>\n",
       "      <th>vehicleType</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>powerPS</th>\n",
       "      <th>model</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>monthOfRegistration</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>brand</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>dateCreated</th>\n",
       "      <th>nrOfPictures</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>lastSeen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-03-24 11:52:17</td>\n",
       "      <td>Golf_3_1.6</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>480</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>manuell</td>\n",
       "      <td>0</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-24 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>70435</td>\n",
       "      <td>2016-04-07 03:16:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-03-24 10:58:45</td>\n",
       "      <td>A5_Sportback_2.7_Tdi</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>18300</td>\n",
       "      <td>test</td>\n",
       "      <td>coupe</td>\n",
       "      <td>2011</td>\n",
       "      <td>manuell</td>\n",
       "      <td>190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125000</td>\n",
       "      <td>5</td>\n",
       "      <td>diesel</td>\n",
       "      <td>audi</td>\n",
       "      <td>ja</td>\n",
       "      <td>2016-03-24 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>66954</td>\n",
       "      <td>2016-04-07 01:46:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-03-14 12:52:21</td>\n",
       "      <td>Jeep_Grand_Cherokee_\"Overland\"</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>9800</td>\n",
       "      <td>test</td>\n",
       "      <td>suv</td>\n",
       "      <td>2004</td>\n",
       "      <td>automatik</td>\n",
       "      <td>163</td>\n",
       "      <td>grand</td>\n",
       "      <td>125000</td>\n",
       "      <td>8</td>\n",
       "      <td>diesel</td>\n",
       "      <td>jeep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-14 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>90480</td>\n",
       "      <td>2016-04-05 12:47:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-03-17 16:54:04</td>\n",
       "      <td>GOLF_4_1_4__3TRER</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>1500</td>\n",
       "      <td>test</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2001</td>\n",
       "      <td>manuell</td>\n",
       "      <td>75</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>6</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>nein</td>\n",
       "      <td>2016-03-17 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>91074</td>\n",
       "      <td>2016-03-17 17:40:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-03-31 17:25:20</td>\n",
       "      <td>Skoda_Fabia_1.4_TDI_PD_Classic</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>3600</td>\n",
       "      <td>test</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2008</td>\n",
       "      <td>manuell</td>\n",
       "      <td>69</td>\n",
       "      <td>fabia</td>\n",
       "      <td>90000</td>\n",
       "      <td>7</td>\n",
       "      <td>diesel</td>\n",
       "      <td>skoda</td>\n",
       "      <td>nein</td>\n",
       "      <td>2016-03-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>60437</td>\n",
       "      <td>2016-04-06 10:17:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-04-04 17:36:23</td>\n",
       "      <td>BMW_316i___e36_Limousine___Bastlerfahrzeug__Ex...</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>650</td>\n",
       "      <td>test</td>\n",
       "      <td>limousine</td>\n",
       "      <td>1995</td>\n",
       "      <td>manuell</td>\n",
       "      <td>102</td>\n",
       "      <td>3er</td>\n",
       "      <td>150000</td>\n",
       "      <td>10</td>\n",
       "      <td>benzin</td>\n",
       "      <td>bmw</td>\n",
       "      <td>ja</td>\n",
       "      <td>2016-04-04 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>33775</td>\n",
       "      <td>2016-04-06 19:17:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-04-01 20:48:51</td>\n",
       "      <td>Peugeot_206_CC_110_Platinum</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>2200</td>\n",
       "      <td>test</td>\n",
       "      <td>cabrio</td>\n",
       "      <td>2004</td>\n",
       "      <td>manuell</td>\n",
       "      <td>109</td>\n",
       "      <td>2_reihe</td>\n",
       "      <td>150000</td>\n",
       "      <td>8</td>\n",
       "      <td>benzin</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>nein</td>\n",
       "      <td>2016-04-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>67112</td>\n",
       "      <td>2016-04-05 18:18:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-03-21 18:54:38</td>\n",
       "      <td>VW_Derby_Bj_80__Scheunenfund</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>limousine</td>\n",
       "      <td>1980</td>\n",
       "      <td>manuell</td>\n",
       "      <td>50</td>\n",
       "      <td>andere</td>\n",
       "      <td>40000</td>\n",
       "      <td>7</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>nein</td>\n",
       "      <td>2016-03-21 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>19348</td>\n",
       "      <td>2016-03-25 16:47:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dateCrawled                                               name  \\\n",
       "0  2016-03-24 11:52:17                                         Golf_3_1.6   \n",
       "1  2016-03-24 10:58:45                               A5_Sportback_2.7_Tdi   \n",
       "2  2016-03-14 12:52:21                     Jeep_Grand_Cherokee_\"Overland\"   \n",
       "3  2016-03-17 16:54:04                                 GOLF_4_1_4__3TRER   \n",
       "4  2016-03-31 17:25:20                     Skoda_Fabia_1.4_TDI_PD_Classic   \n",
       "5  2016-04-04 17:36:23  BMW_316i___e36_Limousine___Bastlerfahrzeug__Ex...   \n",
       "6  2016-04-01 20:48:51                        Peugeot_206_CC_110_Platinum   \n",
       "7  2016-03-21 18:54:38                       VW_Derby_Bj_80__Scheunenfund   \n",
       "\n",
       "   seller offerType  price abtest vehicleType  yearOfRegistration    gearbox  \\\n",
       "0  privat   Angebot    480   test         NaN                1993    manuell   \n",
       "1  privat   Angebot  18300   test       coupe                2011    manuell   \n",
       "2  privat   Angebot   9800   test         suv                2004  automatik   \n",
       "3  privat   Angebot   1500   test  kleinwagen                2001    manuell   \n",
       "4  privat   Angebot   3600   test  kleinwagen                2008    manuell   \n",
       "5  privat   Angebot    650   test   limousine                1995    manuell   \n",
       "6  privat   Angebot   2200   test      cabrio                2004    manuell   \n",
       "7  privat   Angebot      0   test   limousine                1980    manuell   \n",
       "\n",
       "   powerPS    model  kilometer  monthOfRegistration fuelType       brand  \\\n",
       "0        0     golf     150000                    0   benzin  volkswagen   \n",
       "1      190      NaN     125000                    5   diesel        audi   \n",
       "2      163    grand     125000                    8   diesel        jeep   \n",
       "3       75     golf     150000                    6   benzin  volkswagen   \n",
       "4       69    fabia      90000                    7   diesel       skoda   \n",
       "5      102      3er     150000                   10   benzin         bmw   \n",
       "6      109  2_reihe     150000                    8   benzin     peugeot   \n",
       "7       50   andere      40000                    7   benzin  volkswagen   \n",
       "\n",
       "  notRepairedDamage          dateCreated  nrOfPictures  postalCode  \\\n",
       "0               NaN  2016-03-24 00:00:00             0       70435   \n",
       "1                ja  2016-03-24 00:00:00             0       66954   \n",
       "2               NaN  2016-03-14 00:00:00             0       90480   \n",
       "3              nein  2016-03-17 00:00:00             0       91074   \n",
       "4              nein  2016-03-31 00:00:00             0       60437   \n",
       "5                ja  2016-04-04 00:00:00             0       33775   \n",
       "6              nein  2016-04-01 00:00:00             0       67112   \n",
       "7              nein  2016-03-21 00:00:00             0       19348   \n",
       "\n",
       "              lastSeen  \n",
       "0  2016-04-07 03:16:57  \n",
       "1  2016-04-07 01:46:50  \n",
       "2  2016-04-05 12:47:46  \n",
       "3  2016-03-17 17:40:17  \n",
       "4  2016-04-06 10:17:21  \n",
       "5  2016-04-06 19:17:07  \n",
       "6  2016-04-05 18:18:39  \n",
       "7  2016-03-25 16:47:58  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Question 9: Step 1: read in data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('autos.csv', sep=',', header=0, encoding='cp1252')\n",
    "print(df.shape)\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371528, 9)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 9: Step 2: Clean data\n",
    "\n",
    "i. drop data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df.drop(['dateCrawled', 'name', 'seller', 'offerType', 'abtest', 'model', 'notRepairedDamage', 'monthOfRegistration', 'dateCreated', 'nrOfPictures', 'lastSeen'], axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicleType            False\n",
      "yearOfRegistration     False\n",
      "gearbox                False\n",
      "powerPS                False\n",
      "kilometer              False\n",
      "monthOfRegistration    False\n",
      "fuelType               False\n",
      "brand                  False\n",
      "dtype: bool\n",
      "\n",
      "(371528, 9)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Question 9: Step 2: Clean data\n",
    "\n",
    "ii. Check NaNs.drop NaNs.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(df.isnull().any()) # No NAN. If there is any, comment out the following. \n",
    "print()\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>vehicleType</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>powerPS</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>brand</th>\n",
       "      <th>postalCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18300</td>\n",
       "      <td>coupe</td>\n",
       "      <td>2011</td>\n",
       "      <td>manuell</td>\n",
       "      <td>190</td>\n",
       "      <td>125000</td>\n",
       "      <td>diesel</td>\n",
       "      <td>audi</td>\n",
       "      <td>66954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9800</td>\n",
       "      <td>suv</td>\n",
       "      <td>2004</td>\n",
       "      <td>automatik</td>\n",
       "      <td>163</td>\n",
       "      <td>125000</td>\n",
       "      <td>diesel</td>\n",
       "      <td>jeep</td>\n",
       "      <td>90480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1500</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2001</td>\n",
       "      <td>manuell</td>\n",
       "      <td>75</td>\n",
       "      <td>150000</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>91074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3600</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2008</td>\n",
       "      <td>manuell</td>\n",
       "      <td>69</td>\n",
       "      <td>90000</td>\n",
       "      <td>diesel</td>\n",
       "      <td>skoda</td>\n",
       "      <td>60437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>650</td>\n",
       "      <td>limousine</td>\n",
       "      <td>1995</td>\n",
       "      <td>manuell</td>\n",
       "      <td>102</td>\n",
       "      <td>150000</td>\n",
       "      <td>benzin</td>\n",
       "      <td>bmw</td>\n",
       "      <td>33775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2200</td>\n",
       "      <td>cabrio</td>\n",
       "      <td>2004</td>\n",
       "      <td>manuell</td>\n",
       "      <td>109</td>\n",
       "      <td>150000</td>\n",
       "      <td>benzin</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>67112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>limousine</td>\n",
       "      <td>1980</td>\n",
       "      <td>manuell</td>\n",
       "      <td>50</td>\n",
       "      <td>40000</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>19348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14500</td>\n",
       "      <td>bus</td>\n",
       "      <td>2014</td>\n",
       "      <td>manuell</td>\n",
       "      <td>125</td>\n",
       "      <td>30000</td>\n",
       "      <td>benzin</td>\n",
       "      <td>ford</td>\n",
       "      <td>94505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price vehicleType  yearOfRegistration    gearbox  powerPS  kilometer  \\\n",
       "0  18300       coupe                2011    manuell      190     125000   \n",
       "1   9800         suv                2004  automatik      163     125000   \n",
       "2   1500  kleinwagen                2001    manuell       75     150000   \n",
       "3   3600  kleinwagen                2008    manuell       69      90000   \n",
       "4    650   limousine                1995    manuell      102     150000   \n",
       "5   2200      cabrio                2004    manuell      109     150000   \n",
       "6      0   limousine                1980    manuell       50      40000   \n",
       "7  14500         bus                2014    manuell      125      30000   \n",
       "\n",
       "  fuelType       brand  postalCode  \n",
       "0   diesel        audi       66954  \n",
       "1   diesel        jeep       90480  \n",
       "2   benzin  volkswagen       91074  \n",
       "3   diesel       skoda       60437  \n",
       "4   benzin         bmw       33775  \n",
       "5   benzin     peugeot       67112  \n",
       "6   benzin  volkswagen       19348  \n",
       "7   benzin        ford       94505  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape after dropping the NaNs in vehicleType:\n",
      "(333659, 9)\n",
      "Dataframe shape after dropping the NaNs in fuelType:\n",
      "(317768, 9)\n",
      "Dataframe shape after dropping the NaNs in fuelType:\n",
      "(309900, 9)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Question 9: Step 2: Clean data\n",
    "\n",
    "iii. Drop NaNs.\n",
    "\n",
    "\"\"\"\n",
    "# Nans in vehivle type:\n",
    "index_list = df['vehicleType'].index[df['vehicleType'].isnull() == True].tolist()\n",
    "# print(\"Index that containing special characters:\\n{}\".format(index_list))\n",
    "\n",
    "df=df.drop(np.array(index_list))\n",
    "\n",
    "# Need to reset the index, otherwise the index are gone after the drop. \n",
    "df = df.reset_index(drop=True)\n",
    "print(\"Dataframe shape after dropping the NaNs in vehicleType:\\n{}\".format(df.shape))\n",
    "\n",
    "\n",
    "# Nans in fuel type\n",
    "index_list = df['fuelType'].index[df['fuelType'].isnull() == True].tolist()\n",
    "# print(\"Index that containing special characters:\\n{}\".format(index_list))\n",
    "\n",
    "df=df.drop(np.array(index_list))\n",
    "\n",
    "# Need to reset the index, otherwise the index are gone after the drop. \n",
    "df = df.reset_index(drop=True)\n",
    "print(\"Dataframe shape after dropping the NaNs in fuelType:\\n{}\".format(df.shape))\n",
    "\n",
    "\n",
    "# Nans in gearbox\n",
    "index_list = df['gearbox'].index[df['gearbox'].isnull() == True].tolist()\n",
    "# print(\"Index that containing special characters:\\n{}\".format(index_list))\n",
    "\n",
    "df=df.drop(np.array(index_list))\n",
    "\n",
    "# Need to reset the index, otherwise the index are gone after the drop. \n",
    "df = df.reset_index(drop=True)\n",
    "print(\"Dataframe shape after dropping the NaNs in fuelType:\\n{}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>vehicleType</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>powerPS</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>brand</th>\n",
       "      <th>postalCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000625</td>\n",
       "      <td>coupe</td>\n",
       "      <td>2011</td>\n",
       "      <td>manuell</td>\n",
       "      <td>0.371500</td>\n",
       "      <td>-0.005624</td>\n",
       "      <td>diesel</td>\n",
       "      <td>audi</td>\n",
       "      <td>66954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001564</td>\n",
       "      <td>suv</td>\n",
       "      <td>2004</td>\n",
       "      <td>automatik</td>\n",
       "      <td>0.220482</td>\n",
       "      <td>-0.005624</td>\n",
       "      <td>diesel</td>\n",
       "      <td>jeep</td>\n",
       "      <td>90480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003701</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2001</td>\n",
       "      <td>manuell</td>\n",
       "      <td>-0.271725</td>\n",
       "      <td>0.626713</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>91074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.003160</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2008</td>\n",
       "      <td>manuell</td>\n",
       "      <td>-0.305284</td>\n",
       "      <td>-0.890896</td>\n",
       "      <td>diesel</td>\n",
       "      <td>skoda</td>\n",
       "      <td>60437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003919</td>\n",
       "      <td>limousine</td>\n",
       "      <td>1995</td>\n",
       "      <td>manuell</td>\n",
       "      <td>-0.120707</td>\n",
       "      <td>0.626713</td>\n",
       "      <td>benzin</td>\n",
       "      <td>bmw</td>\n",
       "      <td>33775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.003520</td>\n",
       "      <td>cabrio</td>\n",
       "      <td>2004</td>\n",
       "      <td>manuell</td>\n",
       "      <td>-0.081554</td>\n",
       "      <td>0.626713</td>\n",
       "      <td>benzin</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>67112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.004087</td>\n",
       "      <td>limousine</td>\n",
       "      <td>1980</td>\n",
       "      <td>manuell</td>\n",
       "      <td>-0.411556</td>\n",
       "      <td>-2.155570</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>19348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.000354</td>\n",
       "      <td>bus</td>\n",
       "      <td>2014</td>\n",
       "      <td>manuell</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>-2.408504</td>\n",
       "      <td>benzin</td>\n",
       "      <td>ford</td>\n",
       "      <td>94505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price vehicleType  yearOfRegistration    gearbox   powerPS  kilometer  \\\n",
       "0  0.000625       coupe                2011    manuell  0.371500  -0.005624   \n",
       "1 -0.001564         suv                2004  automatik  0.220482  -0.005624   \n",
       "2 -0.003701  kleinwagen                2001    manuell -0.271725   0.626713   \n",
       "3 -0.003160  kleinwagen                2008    manuell -0.305284  -0.890896   \n",
       "4 -0.003919   limousine                1995    manuell -0.120707   0.626713   \n",
       "5 -0.003520      cabrio                2004    manuell -0.081554   0.626713   \n",
       "6 -0.004087   limousine                1980    manuell -0.411556  -2.155570   \n",
       "7 -0.000354         bus                2014    manuell  0.007938  -2.408504   \n",
       "\n",
       "  fuelType       brand  postalCode  \n",
       "0   diesel        audi       66954  \n",
       "1   diesel        jeep       90480  \n",
       "2   benzin  volkswagen       91074  \n",
       "3   diesel       skoda       60437  \n",
       "4   benzin         bmw       33775  \n",
       "5   benzin     peugeot       67112  \n",
       "6   benzin  volkswagen       19348  \n",
       "7   benzin        ford       94505  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Question 9: Step 3: Standardize numerical data:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_col = ['price', 'powerPS', 'kilometer']\n",
    "cat_col = ['yearOfRegistration']\n",
    "\n",
    "for column in num_col:\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    df[column] = (df[column]-mean)/std\n",
    "\n",
    "\n",
    "data[cat_col] = data[cat_col].astype(str)\n",
    "\n",
    "df.head(8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Data: Sum of squared error, aka residual sum of squares.: 307425.42\n",
      "\n",
      "AIC:-2468.510066291117 \n",
      "BIC:-2383.35802674091 \n",
      "*******************************************\n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage: -10.106986505414435\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Question 9: Step 3: Linear regression.\n",
    "\n",
    "i. Use AIC/BIC to choose for all data.\n",
    "ii. Use 10-fold cross validation, and get the mean scores. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data_col = ['vehicleType', 'yearOfRegistration', 'gearbox', 'powerPS', 'kilometer', 'fuelType', 'brand']\n",
    "\n",
    "\n",
    "data = df[data_col]\n",
    "price = df['price']\n",
    "\n",
    "# Dict_Vectorize the X: \n",
    "dict_data = data.T.to_dict().values()\n",
    "\n",
    "# Initiate vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform X and y: y does not need to transform. \n",
    "X = pd.DataFrame(vectorizer.fit_transform(dict_data))\n",
    "y = price\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X, y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X)\n",
    "\n",
    "# Sum of squared error, aka residual sum of squares. \n",
    "print(\"All Data: Sum of squared error, aka residual sum of squares.: %.2f\"\n",
    "      % ((mean_squared_error(y, y_pred))*len(y)))\n",
    "print()\n",
    "\n",
    "\n",
    "# AIC/BIC\n",
    "k = len(data_col)+1\n",
    "n = len(y)\n",
    "\n",
    "mean_sq_er_total = mean_squared_error(y, y_pred)\n",
    " \n",
    "# AIC and BIC for All coefficients\n",
    "aic_total = (2*k) + n * np.log(mean_sq_er_total)\n",
    "print(\"AIC:{} \".format(aic_total))\n",
    "bic_total = k * np.log(n) + n * np.log(mean_sq_er_total)\n",
    "print(\"BIC:{} \".format(bic_total))\n",
    "print(\"*******************************************\")\n",
    "print()  \n",
    "\n",
    "\n",
    "\n",
    "# 10-fold Cross Validation\n",
    "\n",
    "# Inditiate KFold.\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "# Split data into 10 folds.\n",
    "s = kf.split(X)\n",
    "n = kf.get_n_splits(X)\n",
    "print(\"Splitted into {} folds\".format(n))\n",
    "\n",
    "# Initiate regression.\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "R2_KFold = []\n",
    "# Cross-validation.\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train, y_train)\n",
    "    # Predict.\n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    R2_KFold.append(r2_score(y_test, y_pred))\n",
    "\n",
    "print('Average coverage: {}'.format(np.mean(R2_KFold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    # Create the final model selection list. \\n    lst_models = []\\n    for ele in lst_select:\\n        lst_main = ['vehicleType']\\n        lst_main.extend(ele)\\n        lst_models.append(lst_main)\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Question 9: Step 4: Choose the feature combinations for better model.\n",
    "\n",
    "i. Create a class for model combinations. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def models(size):\n",
    "    # Define other independent variables for me to choose from. \n",
    "    Ind = ['vehicleType', 'yearOfRegistration', 'gearbox', 'powerPS', 'kilometer', 'fuelType', 'brand']\n",
    "    # Create the combinations for chosen number of variables. \n",
    "    lst_select = list(itertools.combinations(Ind,size))\n",
    "    print(\"Model Selected:\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "    print()\n",
    "    return lst_select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Selected:\n",
      "\n",
      "\n",
      "[('vehicleType', 'yearOfRegistration'), ('vehicleType', 'gearbox'), ('vehicleType', 'powerPS'), ('vehicleType', 'kilometer'), ('vehicleType', 'fuelType'), ('vehicleType', 'brand'), ('yearOfRegistration', 'gearbox'), ('yearOfRegistration', 'powerPS'), ('yearOfRegistration', 'kilometer'), ('yearOfRegistration', 'fuelType'), ('yearOfRegistration', 'brand'), ('gearbox', 'powerPS'), ('gearbox', 'kilometer'), ('gearbox', 'fuelType'), ('gearbox', 'brand'), ('powerPS', 'kilometer'), ('powerPS', 'fuelType'), ('powerPS', 'brand'), ('kilometer', 'fuelType'), ('kilometer', 'brand'), ('fuelType', 'brand')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Question 9: Step 3: Find a better fit.\n",
    "\n",
    "ii. Set the number of features of interest to 2, and generate the list of feature combinations. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model_lst = models(2)\n",
    "print(model_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************Model ('vehicleType', 'yearOfRegistration')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309783.29\n",
      "\n",
      "Coefficients:[ -4.58014496e+07  -4.58014498e+07  -4.58014498e+07  -4.58014498e+07\n",
      "  -4.58014498e+07  -4.58014498e+07  -4.58014498e+07  -4.58014498e+07\n",
      "  -1.42104842e-04] \n",
      "AIC:-110.73357821734983 \n",
      "BIC:-78.80156338602231 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.5718700087537816\n",
      "*******************Model ('vehicleType', 'gearbox')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309783.53\n",
      "\n",
      "Coefficients:[  3.85804587e+03   3.85804507e+03   1.17617683e+07   1.17617681e+07\n",
      "   1.17617681e+07   1.17617681e+07   1.17617681e+07   1.17617681e+07\n",
      "   1.17617681e+07   1.17617681e+07] \n",
      "AIC:-110.48696779439244 \n",
      "BIC:-78.55495296306492 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.5681833296763298\n",
      "*******************Model ('vehicleType', 'powerPS')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309761.57\n",
      "\n",
      "Coefficients:[  8.55218721e-03  -4.45374749e+10  -4.45374749e+10  -4.45374749e+10\n",
      "  -4.45374749e+10  -4.45374749e+10  -4.45374749e+10  -4.45374749e+10\n",
      "  -4.45374749e+10] \n",
      "AIC:-132.462433818955 \n",
      "BIC:-100.53041898762747 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.6540659942814541\n",
      "*******************Model ('vehicleType', 'kilometer')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309783.57\n",
      "\n",
      "Coefficients:[  3.14390452e-05   2.42807615e+07   2.42807613e+07   2.42807613e+07\n",
      "   2.42807613e+07   2.42807613e+07   2.42807613e+07   2.42807613e+07\n",
      "   2.42807613e+07] \n",
      "AIC:-110.4541128854749 \n",
      "BIC:-78.52209805414738 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.568819996232829\n",
      "*******************Model ('vehicleType', 'fuelType')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 307608.39\n",
      "\n",
      "Coefficients:[-41037061.01752216 -41037064.99176687 -41037064.99592851\n",
      " -41037064.99220009 -41037065.00754034 -41037064.99005769\n",
      " -41037064.99097094 -29900800.31641958 -29900800.48209369\n",
      " -29900800.48134423 -29900800.47861247 -29900800.48327056\n",
      " -29900800.48239265 -29900800.48265243 -29900800.4815191 ] \n",
      "AIC:-2294.1252119415562 \n",
      "BIC:-2262.1931971102285 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -9.429143945216751\n",
      "*******************Model ('vehicleType', 'brand')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309656.56\n",
      "\n",
      "Coefficients:[  1.58064532e+08   1.58064532e+08   1.58064532e+08   1.58064532e+08\n",
      "   1.58064532e+08   1.58064532e+08   1.58064532e+08   1.58064532e+08\n",
      "   1.58064532e+08   1.58064532e+08   1.58064532e+08   1.58064532e+08\n",
      "   1.58064532e+08   1.58064532e+08   1.58064532e+08   1.58064532e+08\n",
      "   1.58064532e+08   1.58064532e+08   1.58064532e+08   1.58064532e+08\n",
      "   1.58064532e+08   1.58064532e+08   1.58064532e+08   1.58064532e+08\n",
      "   1.58064532e+08   1.58064532e+08   1.58064532e+08   1.58064532e+08\n",
      "   1.58064532e+08   1.58064532e+08   1.58064532e+08   1.58064532e+08\n",
      "   1.58064532e+08   1.58064532e+08   1.58064532e+08   1.58064532e+08\n",
      "   1.58064532e+08   1.58064532e+08   1.58064532e+08   1.58064532e+08\n",
      "  -1.43776153e+09  -1.43776153e+09  -1.43776153e+09  -1.43776153e+09\n",
      "  -1.43776153e+09  -1.43776153e+09  -1.43776153e+09  -1.43776153e+09] \n",
      "AIC:-237.532383870774 \n",
      "BIC:-205.6003690394465 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -1.1594759182333645\n",
      "*******************Model ('yearOfRegistration', 'gearbox')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309897.83\n",
      "\n",
      "Coefficients:[ 0.00025746 -0.00025746 -0.00028759] \n",
      "AIC:3.8338574618974013 \n",
      "BIC:35.76587229322492 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.017737592682450197\n",
      "*******************Model ('yearOfRegistration', 'powerPS')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309876.14\n",
      "\n",
      "Coefficients:[ 0.00839927 -0.00038789] \n",
      "AIC:-17.86162526015763 \n",
      "BIC:14.07038957116989 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.10201740247043985\n",
      "*******************Model ('yearOfRegistration', 'kilometer')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309897.69\n",
      "\n",
      "Coefficients:[-0.00075024 -0.00032151] \n",
      "AIC:3.6937139289597685 \n",
      "BIC:35.62572876028729 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.01718005504222079\n",
      "*******************Model ('yearOfRegistration', 'fuelType')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 307686.08\n",
      "\n",
      "Coefficients:[  1.57781518e+08   1.57781514e+08   1.57781514e+08   1.57781514e+08\n",
      "   1.57781514e+08   1.57781514e+08   1.57781514e+08   6.69871538e-05] \n",
      "AIC:-2215.865385639109 \n",
      "BIC:-2183.933370807781 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -9.067118996522257\n",
      "*******************Model ('yearOfRegistration', 'brand')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309759.40\n",
      "\n",
      "Coefficients:[  2.17416070e+07   2.17416070e+07   2.17416070e+07   2.17416070e+07\n",
      "   2.17416070e+07   2.17416070e+07   2.17416070e+07   2.17416070e+07\n",
      "   2.17416070e+07   2.17416070e+07   2.17416070e+07   2.17416070e+07\n",
      "   2.17416070e+07   2.17416070e+07   2.17416070e+07   2.17416070e+07\n",
      "   2.17416070e+07   2.17416070e+07   2.17416070e+07   2.17416070e+07\n",
      "   2.17416070e+07   2.17416070e+07   2.17416070e+07   2.17416070e+07\n",
      "   2.17416070e+07   2.17416070e+07   2.17416071e+07   2.17416070e+07\n",
      "   2.17416070e+07   2.17416070e+07   2.17416070e+07   2.17416070e+07\n",
      "   2.17416070e+07   2.17416073e+07   2.17416070e+07   2.17416070e+07\n",
      "   2.17416070e+07   2.17416070e+07   2.17416070e+07   2.17416070e+07\n",
      "   8.57943669e-05] \n",
      "AIC:-134.63344710087028 \n",
      "BIC:-102.70143226954276 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.6626390873107242\n",
      "*******************Model ('gearbox', 'powerPS')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309877.74\n",
      "\n",
      "Coefficients:[-0.00158388  0.00158388  0.00838924] \n",
      "AIC:-16.262927251179647 \n",
      "BIC:15.669087580147874 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.09158629208074323\n",
      "*******************Model ('gearbox', 'kilometer')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309899.00\n",
      "\n",
      "Coefficients:[  6.83906879e-07  -6.83906879e-07  -1.14477928e-05] \n",
      "AIC:4.999957702715217 \n",
      "BIC:36.931972534042735 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.008506761945413666\n",
      "*******************Model ('gearbox', 'fuelType')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 307686.14\n",
      "\n",
      "Coefficients:[  1.35950749e+08   1.35950745e+08   1.35950745e+08   1.35950745e+08\n",
      "   1.35950745e+08   1.35950745e+08   1.35950745e+08  -2.88348381e+09\n",
      "  -2.88348381e+09] \n",
      "AIC:-2215.807594320988 \n",
      "BIC:-2183.87557948966 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -9.06742861191017\n",
      "*******************Model ('gearbox', 'brand')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309758.61\n",
      "\n",
      "Coefficients:[  3.68912420e+06   3.68912420e+06   3.68912420e+06   3.68912420e+06\n",
      "   3.68912420e+06   3.68912420e+06   3.68912420e+06   3.68912420e+06\n",
      "   3.68912420e+06   3.68912420e+06   3.68912420e+06   3.68912420e+06\n",
      "   3.68912420e+06   3.68912420e+06   3.68912420e+06   3.68912420e+06\n",
      "   3.68912420e+06   3.68912420e+06   3.68912420e+06   3.68912420e+06\n",
      "   3.68912420e+06   3.68912420e+06   3.68912420e+06   3.68912420e+06\n",
      "   3.68912420e+06   3.68912420e+06   3.68912421e+06   3.68912420e+06\n",
      "   3.68912420e+06   3.68912420e+06   3.68912420e+06   3.68912420e+06\n",
      "   3.68912420e+06   3.68912444e+06   3.68912420e+06   3.68912420e+06\n",
      "   3.68912420e+06   3.68912420e+06   3.68912420e+06   3.68912420e+06\n",
      "  -2.23615690e-03   2.23615358e-03] \n",
      "AIC:-135.42408041547938 \n",
      "BIC:-103.49206558415186 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.6698394889050341\n",
      "*******************Model ('powerPS', 'kilometer')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309878.26\n",
      "\n",
      "Coefficients:[ 0.00014731  0.00818134] \n",
      "AIC:-15.735881937483615 \n",
      "BIC:16.196132893843906 \n",
      "\n",
      "Splitted into 10 folds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average coverage from 10-fold: -0.08816822668837827\n",
      "*******************Model ('powerPS', 'fuelType')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 307663.27\n",
      "\n",
      "Coefficients:[ -1.38400420e+08  -1.38400424e+08  -1.38400424e+08  -1.38400424e+08\n",
      "  -1.38400424e+08  -1.38400424e+08  -1.38400424e+08   8.60831636e-03] \n",
      "AIC:-2238.838396386464 \n",
      "BIC:-2206.9063815551362 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -9.150065002200513\n",
      "*******************Model ('powerPS', 'brand')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309741.94\n",
      "\n",
      "Coefficients:[  3.01974413e+07   3.01974413e+07   3.01974413e+07   3.01974413e+07\n",
      "   3.01974413e+07   3.01974413e+07   3.01974413e+07   3.01974413e+07\n",
      "   3.01974413e+07   3.01974413e+07   3.01974413e+07   3.01974413e+07\n",
      "   3.01974413e+07   3.01974413e+07   3.01974413e+07   3.01974413e+07\n",
      "   3.01974413e+07   3.01974413e+07   3.01974413e+07   3.01974413e+07\n",
      "   3.01974413e+07   3.01974413e+07   3.01974413e+07   3.01974413e+07\n",
      "   3.01974413e+07   3.01974413e+07   3.01974414e+07   3.01974413e+07\n",
      "   3.01974413e+07   3.01974413e+07   3.01974413e+07   3.01974413e+07\n",
      "   3.01974413e+07   3.01974416e+07   3.01974413e+07   3.01974413e+07\n",
      "   3.01974413e+07   3.01974413e+07   3.01974413e+07   3.01974413e+07\n",
      "   7.67950714e-03] \n",
      "AIC:-152.09757300413546 \n",
      "BIC:-120.16555817280793 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.7313098607607594\n",
      "*******************Model ('kilometer', 'fuelType')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 307685.40\n",
      "\n",
      "Coefficients:[  7.53877233e+07   7.53877193e+07   7.53877193e+07   7.53877193e+07\n",
      "   7.53877193e+07   7.53877193e+07   7.53877193e+07   1.55385723e-03] \n",
      "AIC:-2216.54862781414 \n",
      "BIC:-2184.6166129828125 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -9.075409360111419\n",
      "*******************Model ('kilometer', 'brand')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 309758.67\n",
      "\n",
      "Coefficients:[  5.74205022e+07   5.74205022e+07   5.74205022e+07   5.74205022e+07\n",
      "   5.74205022e+07   5.74205022e+07   5.74205022e+07   5.74205022e+07\n",
      "   5.74205022e+07   5.74205022e+07   5.74205022e+07   5.74205022e+07\n",
      "   5.74205022e+07   5.74205022e+07   5.74205022e+07   5.74205022e+07\n",
      "   5.74205022e+07   5.74205022e+07   5.74205022e+07   5.74205022e+07\n",
      "   5.74205022e+07   5.74205022e+07   5.74205022e+07   5.74205022e+07\n",
      "   5.74205022e+07   5.74205022e+07   5.74205023e+07   5.74205022e+07\n",
      "   5.74205022e+07   5.74205022e+07   5.74205022e+07   5.74205022e+07\n",
      "   5.74205022e+07   5.74205025e+07   5.74205022e+07   5.74205022e+07\n",
      "   5.74205022e+07   5.74205022e+07   5.74205022e+07   5.74205022e+07\n",
      "   1.67588983e-03] \n",
      "AIC:-135.36085494387848 \n",
      "BIC:-103.42884011255096 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -0.6717468738762851\n",
      "*******************Model ('fuelType', 'brand')************************\n",
      "All Data: Sum of squared error, aka residual sum of squares.: 307519.57\n",
      "\n",
      "Coefficients:[  3.24834578e+07   3.24834578e+07   3.24834578e+07   3.24834578e+07\n",
      "   3.24834578e+07   3.24834578e+07   3.24834578e+07   3.24834577e+07\n",
      "   3.24834577e+07   3.24834578e+07   3.24834578e+07   3.24834578e+07\n",
      "   3.24834578e+07   3.24834578e+07   3.24834578e+07   3.24834578e+07\n",
      "   3.24834578e+07   3.24834578e+07   3.24834578e+07   3.24834578e+07\n",
      "   3.24834578e+07   3.24834578e+07   3.24834578e+07   3.24834578e+07\n",
      "   3.24834578e+07   3.24834578e+07   3.24834578e+07   3.24834578e+07\n",
      "   3.24834578e+07   3.24834577e+07   3.24834578e+07   3.24834578e+07\n",
      "   3.24834578e+07   3.24834580e+07   3.24834578e+07   3.24834578e+07\n",
      "   3.24834578e+07   3.24834572e+07   3.24834578e+07   3.24834578e+07\n",
      "   1.79036956e+08   1.79036951e+08   1.79036951e+08   1.79036951e+08\n",
      "   1.79036951e+08   1.79036951e+08   1.79036951e+08] \n",
      "AIC:-2383.6189821251937 \n",
      "BIC:-2351.686967293866 \n",
      "\n",
      "Splitted into 10 folds\n",
      "Average coverage from 10-fold: -9.676170215413382\n"
     ]
    }
   ],
   "source": [
    "for col in model_lst:\n",
    "    print(\"*******************Model {}************************\".format(col))\n",
    "    data = df[list(col)]\n",
    "    price = df['price']\n",
    "\n",
    "    # Dict_Vectorize the X: \n",
    "    dict_data = data.T.to_dict().values()\n",
    "\n",
    "    # Initiate vectorizer\n",
    "    vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "    # Transform X and y: y does not need to transform. \n",
    "    X = pd.DataFrame(vectorizer.fit_transform(dict_data))\n",
    "    y = price\n",
    "\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X, y)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = regr.predict(X)\n",
    "\n",
    "    # Sum of squared error, aka residual sum of squares. \n",
    "    print(\"All Data: Sum of squared error, aka residual sum of squares.: %.2f\"\n",
    "          % ((mean_squared_error(y, y_pred))*len(y)))\n",
    "    print()\n",
    "    print(\"Coefficients:{} \".format(regr.coef_))\n",
    "\n",
    "\n",
    "    # AIC/BIC\n",
    "    k = len(col)+1\n",
    "    n = len(y)\n",
    "\n",
    "    mean_sq_er_total = mean_squared_error(y, y_pred)\n",
    " \n",
    "    # AIC and BIC for All coefficients\n",
    "    aic_total = (2*k) + n * np.log(mean_sq_er_total)\n",
    "    print(\"AIC:{} \".format(aic_total))\n",
    "    bic_total = k * np.log(n) + n * np.log(mean_sq_er_total)\n",
    "    print(\"BIC:{} \".format(bic_total))\n",
    "    print()  \n",
    "\n",
    "\n",
    "\n",
    "    # 10-fold Cross Validation\n",
    "\n",
    "    # Inditiate KFold.\n",
    "    kf = KFold(n_splits=10)\n",
    "\n",
    "    # Split data into 10 folds.\n",
    "    s = kf.split(X)\n",
    "    n = kf.get_n_splits(X)\n",
    "    print(\"Splitted into {} folds\".format(n))\n",
    "\n",
    "    # Initiate regression.\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    R2_KFold = []\n",
    "    # Cross-validation.\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        # Train the model using the training sets\n",
    "        regr.fit(X_train, y_train)\n",
    "        # Predict.\n",
    "        y_pred = regr.predict(X_test)\n",
    "    \n",
    "        R2_KFold.append(r2_score(y_test, y_pred))\n",
    "\n",
    "    print('Average coverage from 10-fold: {}'.format(np.mean(R2_KFold)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
